{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a7dda0d",
   "metadata": {},
   "source": [
    "# Spike sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cdd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reAnalyse = False\n",
    "engine = \"dask\" #\"dask\", \"submitit\", or None\n",
    "GPU_available = False\n",
    "preprocessed_folder = 'preprocessing'\n",
    "sorter_folder='kilosort4_output_MB'\n",
    "training_folder = 'sorting_analyzer_training_MB'\n",
    "fullAnalyzer_folder = 'sorting_analyzer_full'\n",
    "whitenFirst = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd66156",
   "metadata": {},
   "source": [
    "## Set up everything\n",
    "You shouldn't have to change anything from here so you can keep that part folded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b006d50",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c2661-565d-4949-aa45-d44200c20f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "from spikeinterface.sortingcomponents.motion import estimate_motion, interpolate_motion\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import submitit\n",
    "from memory_profiler import memory_usage\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import asyncio\n",
    "import gc\n",
    "\n",
    "#import HayLabAnalysis as hla\n",
    "from ipyfilechooser import FileChooser\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython import get_ipython\n",
    "import IPython\n",
    "from IPython.display import Javascript\n",
    "import pickleshare\n",
    "\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e190a",
   "metadata": {},
   "source": [
    "### Define a few variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_extract = 1 #min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac2f4e",
   "metadata": {},
   "source": [
    "#### Structural (important for the process but no need to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e310982",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_job=None\n",
    "sorter='kilosort4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ed711",
   "metadata": {},
   "source": [
    "### Define a few functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magicretrieve(stored_var):\n",
    "   # myvar will contain the variable previously stored with \"%store test\"\n",
    "   myvar_filename = get_ipython().ipython_dir + '/profile_default/db/autorestore/' + stored_var\n",
    "   with open(myvar_filename, 'rb') as f:\n",
    "      return pickle.load(f)\n",
    "\n",
    "def magicstore(stored_var, value):\n",
    "   # myvar will contain the variable previously stored with \"%store test\"\n",
    "   myvar_filename = get_ipython().ipython_dir + '/profile_default/db/autorestore/' + stored_var\n",
    "   with open(myvar_filename, 'wb') as f:\n",
    "      pickle.dump(value,f)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_my_expe_choice(chooser):\n",
    "    currentFile_data = str(chooser.selected)\n",
    "    currentFile_mnt = os.path.join('/mnt/data/ahay',os.path.split(currentFile_data)[1])\n",
    "    if not currentFile_data.startswith('/crnldata/'):\n",
    "        print('please make sure to select the file on crnldata')\n",
    "        return\n",
    "    # check if the file already exists on /mnt/\n",
    "    if os.path.isfile(currentFile_mnt):\n",
    "        print(f\"{currentFile_mnt} already exists\")\n",
    "    else:\n",
    "        print(f\"there is no version of {currentFile_data} on /mnt/\")\n",
    "        shouldCopy = False # it took a while (20 min for a file of about 150Gb)\n",
    "        if shouldCopy:\n",
    "            print(f'it will be copied to {currentFile_mnt}')\n",
    "            startTime = time.time()\n",
    "            shutil.copyfile(currentFile_data, currentFile_mnt)\n",
    "            print(f'the transfer is complete, it took {time.time()-startTime} seconds')\n",
    "        else:\n",
    "            print('it can be transfered from here by changing the shouldCopy parameter but probably best not to because it takes a while and uses massive ressources')\n",
    "\n",
    "    magicstore('currentFile_data', currentFile_data)\n",
    "    magicstore('currentFile_mnt', currentFile_mnt)\n",
    "\n",
    "\n",
    "def selectData(currentFile):\n",
    "    print(currentFile)\n",
    "    if currentFile is not None:\n",
    "        pathName, fileName = os.path.split(currentFile)\n",
    "    else:\n",
    "        pathName = '/crnldata/waking/audrey_hay/'\n",
    "        fileName = ''\n",
    "    fc = FileChooser(path=pathName, filename=fileName, filter_pattern='NP_spikes_*.raw', select_default=True, show_only_dirs = False, title = \"<b>Select file on crnldata</b>\")\n",
    "    display(fc)\n",
    "\n",
    "    # Register callback function\n",
    "    fc.register_callback(update_my_expe_choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b95e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runFunction(engine,funcName,*params,**engineParams):\n",
    "    start_time = time.time()\n",
    "    match engine:\n",
    "        case \"dask\":\n",
    "            print(\"dask\")\n",
    "            cluster = SLURMCluster(\n",
    "                                **engineParams,\n",
    "                                nanny=False,\n",
    "                                log_directory=\"si_dask_logs\",\n",
    "                                scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                                )\n",
    "            cluster.scale(1)\n",
    "            client = Client(cluster)\n",
    "\n",
    "            print(client.get_versions(check=True))\n",
    "    \n",
    "            \"\"\"\n",
    "            from dask.distributed import PipInstall\n",
    "            plugin = PipInstall(packages=[\"git+https://github.com/SpikeInterface/spikeinterface.git\"], pip_options=[\"--upgrade\"])\n",
    "            client.register_plugin(plugin)\n",
    "            plugin = PipInstall(packages=[\"git+https://github.com/SpikeInterface/probeinterface.git\"], pip_options=[\"--upgrade\"])\n",
    "            client.register_plugin(plugin)\n",
    "            plugin = PipInstall(packages=[\"git+https://github.com/SpikeInterface/spikeinterface-gui.git\"], pip_options=[\"--upgrade\"])\n",
    "            client.register_plugin(plugin)\n",
    "            plugin = PipInstall(packages=[\"kilosort\"], pip_options=[\"--upgrade\"])\n",
    "            client.register_plugin(plugin)\n",
    "\n",
    "            plugin = PipInstall(packages=[\"numpy\"], pip_options=[\"--upgrade\"])\n",
    "            client.register_plugin(plugin)\n",
    "            \"\"\"\n",
    "            \n",
    "            print(cluster.job_script()) \n",
    "\n",
    "            future = client.submit(funcName, *params)\n",
    "            res = future.result()\n",
    "                        \n",
    "            # Close cluster\n",
    "            client.close()\n",
    "            cluster.close()\n",
    "\n",
    "        \n",
    "        case \"submitit\":\n",
    "            print(\"submitit\")\n",
    "\n",
    "            executor = submitit.AutoExecutor(folder=os.getcwd()+'/si_logs/')\n",
    "            executor.update_parameters(**engineParams)\n",
    "\n",
    "            job = executor.submit(funcName, *params)\n",
    "\n",
    "            # print the ID of your job\n",
    "            print(\"submit job\" + str(job.job_id))  \n",
    "\n",
    "            # await a single result\n",
    "            await job.awaitable().results()\n",
    "            res = job.result()\n",
    "\n",
    "        case _:\n",
    "            print(\"no engine\")\n",
    "            res = funcName(*params) \n",
    "            \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b539b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateDict(rec, folder, sorter_params):\n",
    "    print(sorter_params)\n",
    "    print(type(sorter_params))\n",
    "    si.set_global_job_kwargs(n_jobs=40, progress_bar=True, chunk_duration=\"1s\")\n",
    "    sorting = si.run_sorter(sorter, rec, verbose=True, folder=folder, remove_existing_folder=True, **sorter_params)\n",
    "    return sorting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_analyzer(sorting,rec,folder,append=False):\n",
    "    si.set_global_job_kwargs(n_jobs=40, progress_bar=True, chunk_duration=\"1s\")\n",
    "    \n",
    "    if append:\n",
    "        sorting_analyzer = si.load_sorting_analyzer(folder)\n",
    "    else:\n",
    "        sorting_analyzer = si.create_sorting_analyzer(sorting, rec, sparse=True, folder=folder,overwrite=True)\n",
    "\n",
    "    sorting_analyzer.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)\n",
    "    sorting_analyzer.compute(\"waveforms\")\n",
    "    sorting_analyzer.compute(\"templates\")\n",
    "    sorting_analyzer.compute(\"noise_levels\")\n",
    "    sorting_analyzer.compute(\"unit_locations\", method=\"monopolar_triangulation\")\n",
    "    sorting_analyzer.compute(\"isi_histograms\")\n",
    "    sorting_analyzer.compute(\"correlograms\") #, window_ms=100, bin_ms=5.\n",
    "    sorting_analyzer.compute(\"principal_components\", n_components=3, mode='by_channel_global', whiten=True)\n",
    "    sorting_analyzer.compute(\"quality_metrics\", metric_names=[\"snr\", \"firing_rate\"])\n",
    "    sorting_analyzer.compute(\"template_similarity\")\n",
    "    sorting_analyzer.compute(\"spike_amplitudes\")\n",
    "\n",
    "    print(folder)\n",
    "    print(sorting_analyzer)\n",
    "    sorting_analyzer.save_as(folder=folder, format='binary_folder')\n",
    "    print(sorting_analyzer)\n",
    "\n",
    "    #return sorting_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017793bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_traces(rec):\n",
    "    # filter traces\n",
    "    rec = rec.astype('float32')\n",
    "    print(\"filtering trace, please wait\")\n",
    "    rec_filt = si.bandpass_filter(rec)\n",
    "    display(rec_filt)\n",
    "\n",
    "    # Account for the slight delays in recordings due to the fact the 384 channels are only digitilized with 32 ADCs\n",
    "    print(\"aligning trace, please wait\")\n",
    "    isi=0.076923077\n",
    "    inter_sample_shift = np.arange(12)*isi\n",
    "    inter_sample_shift = np.expand_dims(np.repeat(inter_sample_shift,2),0)\n",
    "    inter_sample_shift = np.repeat(inter_sample_shift,16,0).flatten()\n",
    "    rec_filt_shifted = si.phase_shift(rec_filt, inter_sample_shift=inter_sample_shift)\n",
    "    \n",
    "    # Remove bad channels\n",
    "    print(\"removing bad channels, please wait\")\n",
    "    try:\n",
    "        bad_channel_ids, channel_labels = si.detect_bad_channels(rec_filt_shifted)\n",
    "        print('bad_channel_ids', bad_channel_ids)\n",
    "        rec = rec.remove_channels(bad_channel_ids)\n",
    "        rec_filt = rec_filt.remove_channels(bad_channel_ids)\n",
    "        rec_filt_shifted = rec_filt_shifted.remove_channels(bad_channel_ids)\n",
    "    except Exception as error:\n",
    "        print(\"could not remove bad channels because there was an error:\")\n",
    "        print(error)\n",
    "\n",
    "\n",
    "    # Remove the common noisy events (artefacts)\n",
    "    print(\"remove commonn ref, please wait\")\n",
    "    rec_filt_ref = si.common_reference(rec_filt_shifted)\n",
    "    display(rec_filt_ref)\n",
    "\n",
    "    recording_layers = dict(\n",
    "        raw = rec,\n",
    "        filter = rec_filt,\n",
    "        realigned = rec_filt_shifted,\n",
    "        cmr = rec_filt_ref,\n",
    "    )\n",
    "\n",
    "    return recording_layers, bad_channel_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_drift(rec):      \n",
    "    \n",
    "    job_kwargs_global = dict(n_jobs=40, progress_bar=True, chunk_duration=\"1s\")\n",
    "    si.set_global_job_kwargs(**job_kwargs_global)\n",
    "    \n",
    "    recording_corrected, motion, motion_info = si.correct_motion(\n",
    "            rec, preset=\"dredge_fast\", folder=None, output_motion=True, output_motion_info=True, estimate_motion_kwargs=dict(rigid=True)#,\n",
    "        )\n",
    "    return recording_corrected, motion, motion_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(rec,recording_layers,layerName):\n",
    "\n",
    "    rec_whitened = si.WhitenRecording(rec)\n",
    "\n",
    "    recording_layers[layerName] = rec_whitened\n",
    "\n",
    "    return recording_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05baf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessing(rec,foldername):\n",
    "    return rec.save(folder=foldername, n_jobs=20, chunk_duration='1s', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339081a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkRessources():\n",
    "    # check node and CPU information\n",
    "    print(\"### Node counts: \\nA: currently in use \\B available\")\n",
    "    !sinfo -o%A\n",
    "    print(\"### CPU counts: \\nA: core currently in use \\nI: available \\nO: unavailable (maintenance, down, etc) \\nT: total\")\n",
    "    !sinfo -o%C\n",
    "    !sinfo\n",
    "\n",
    "    # check some stats of our last job\n",
    "    if last_job is not None:\n",
    "        print('### CPU time and MaxRSS of our last job (about 1000Mb should be added to your MaxRSS (Mb) in order to cover safely the memory needs of the python runtime)###')\n",
    "        os.system(f'sacct -j {last_job.job_id} --format=\"CPUTime,MaxRSS\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a24596",
   "metadata": {},
   "source": [
    "## Choose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6dc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#currentFile_data = '/crnldata/waking/audrey_hay/NPX/NPX1/VB/Expe_2024-07-22_17-55-16/NP_spikes_2024-07-22T17_55_16.raw'\n",
    "#hla.tools.magicstore('currentFile_data', currentFile_data)\n",
    "currentFile_data = magicretrieve('currentFile_data')\n",
    "selectData(currentFile_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8669ae",
   "metadata": {},
   "source": [
    "### Move data to /mnt/ if appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shouldCopy = False # it took a while (20 min for a file of about 150Gb)\n",
    "if shouldCopy:\n",
    "    currentFile_data = magicretrieve('currentFile_data')\n",
    "    currentFile_mnt = magicretrieve('currentFile_mnt')\n",
    "    print(f'The file {currentFile_data} will be copied to {currentFile_mnt}')\n",
    "    startTime = time.time()\n",
    "    shutil.copyfile(currentFile_data, currentFile_mnt)\n",
    "    print(f'the transfer is complete, it took {time.time()-startTime} seconds')\n",
    "else:\n",
    "    print('it can be transfered from here by changing the shouldCopy parameter but probably best not to because it takes a while and uses massive ressources')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532274e",
   "metadata": {},
   "source": [
    "### Lazy load data and probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8606c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "currentFile_data = magicretrieve('currentFile_data')\n",
    "currentFile_mnt = magicretrieve('currentFile_mnt')\n",
    "print(currentFile_mnt)\n",
    "\n",
    "raw_rec = si.read_binary(currentFile_mnt, dtype='uint16', num_channels=384, sampling_frequency=30_000.,gain_to_uV=1, offset_to_uV=0)\n",
    "raw_rec.annotate(raw_path = currentFile_data)\n",
    "raw_rec = si.ScaleRecording(raw_rec, gain=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/crnldata/waking/audrey_hay/NPX/NPXprobe.pkl', 'rb') as outp: \n",
    "    probe = pickle.load(outp)\n",
    "probe.set_device_channel_indices(np.arange(384))\n",
    "\n",
    "raw_rec = raw_rec.set_probe(probe)\n",
    "display(si.plot_probe_map(raw_rec, with_channel_ids=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a43680",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c250e0",
   "metadata": {},
   "source": [
    "### Filter and apply common ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf29d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse:\n",
    "    match engine,GPU_available:\n",
    "        case \"dask\", _: # no real interest in using gpu\n",
    "            engineParam=dict(\n",
    "                                queue='CPU',\n",
    "                                cores=1,\n",
    "                                memory=\"3GB\",\n",
    "                                worker_extra_args=[\"--lifetime\", \"2m\"],\n",
    "            )\n",
    "        case \"submitit\", _:\n",
    "            engineParam=dict(\n",
    "                                mem_gb=3,\n",
    "                                timeout_min=2,\n",
    "                                slurm_partition=\"CPU\",\n",
    "                                cpus_per_task=4\n",
    "            )\n",
    "        case _:\n",
    "            engineParam = dict()\n",
    "\n",
    "    recording_layers, bad_channel_ids  = await runFunction(engine, preprocess_traces, raw_rec, **engineParam)\n",
    "    \n",
    "    print(\"computing done, please wait for display\")\n",
    "    si.plot_traces(recording_layers, backend='ipywidgets') #, mode='line'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637e35e",
   "metadata": {},
   "source": [
    "### Correct for motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348fd935",
   "metadata": {},
   "outputs": [],
   "source": [
    "whitenFirst = True\n",
    "canal_lim=300 #limitation of canals to keep for drift correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse:\n",
    "    probe_good = probe.get_slice(recording_layers[\"raw\"].get_channel_ids())\n",
    "    probe_2_correct = probe.get_slice([i for i in recording_layers[\"raw\"].get_channel_ids() if i < canal_lim])\n",
    "\n",
    "    if whitenFirst:\n",
    "        recording_layers = whiten(recording_layers[\"cmr\"],recording_layers,\"whitenFirst\")\n",
    "        rec_2_correct = recording_layers[\"whitenFirst\"]\n",
    "    else:\n",
    "        rec_2_correct = recording_layers[\"cmr\"]\n",
    "\n",
    "    rec_2_correct_short = rec_2_correct.channel_slice(probe_2_correct.device_channel_indices)\n",
    "    display(rec_2_correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef37904",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse:\n",
    "    match engine,GPU_available:\n",
    "        case \"dask\", _:\n",
    "            engineParam=dict(\n",
    "                                queue='CPU',\n",
    "                                cores=1,\n",
    "                                memory=\"50GB\",\n",
    "                                job_cpu=55,\n",
    "                                worker_extra_args=[\"--lifetime\", \"20m\"],\n",
    "            )\n",
    "        case \"submitit\", _:\n",
    "            engineParam=dict(\n",
    "                                slurm_array_parallelism=4,\n",
    "                                mem_gb=60,\n",
    "                                timeout_min=20,\n",
    "                                slurm_partition=\"CPU\",\n",
    "                                cpus_per_task=40\n",
    "            )\n",
    "        case _:\n",
    "            engineParam = dict()\n",
    "\n",
    "    recording_corrected_short, motion, motion_info  = await runFunction(engine,check_drift,rec_2_correct_short, **engineParam)\n",
    "\n",
    "    recording_corrected = interpolate_motion(\n",
    "                recording=rec_2_correct,\n",
    "                motion=motion_info['motion'],\n",
    "                **motion_info['parameters']['interpolate_motion_kwargs'])\n",
    "    \n",
    "    recording_layers[\"corrected\"] = recording_corrected\n",
    "    display(motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (72) 16 min (slurm_array_parallelism=4, mem_gb=60, timeout_min=20, slurm_partition=\"CPU\", cpus_per_task=40)\n",
    "print(\"\"\"rq: you should avoid submitting multiple small tasks with submitit, which would create many independent jobs\n",
    "      and possibly overload the cluster, while you can do it without any problem through dask.distributed.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7225843",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    si.plot_motion_info(motion_info, recording_corrected,\n",
    "                   color_amplitude=True,\n",
    "        amplitude_cmap=\"inferno\",)\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot motion: {e}. Possibly because you have to redo the analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse and not whitenFirst:\n",
    "    recording_layers = whiten(recording_corrected,recording_layers,\"whitenLast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28144436",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    si.plot_traces(recording_layers, backend='ipywidgets') #, mode='line'\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot traces: {e}. Possibly because you have to redo the analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse:\n",
    "    if whitenFirst:\n",
    "        rec = recording_layers[\"whitenFirst\"] #if whitened and not corrected\n",
    "        #rec = recording_layers[\"corrected\"] #if whitened then corrected\n",
    "    else:\n",
    "        rec = recording_layers[\"whitenLast\"] #if corrected the whiten\n",
    "        \n",
    "    match engine,GPU_available:\n",
    "        case \"dask\", _: # real interest in using gpu ?\n",
    "            engineParam=dict(\n",
    "                                queue='CPU',\n",
    "                                cores=1,\n",
    "                                memory=\"15GB\",\n",
    "                                job_cpu=20,\n",
    "                                worker_extra_args=[\"--lifetime\", \"1h\"],\n",
    "            )\n",
    "        case \"submitit\", _:\n",
    "            engineParam=dict(\n",
    "                                mem_gb=3,\n",
    "                                timeout_min=60,\n",
    "                                slurm_partition=\"CPU\",\n",
    "                                cpus_per_task=4\n",
    "            )\n",
    "        case _:\n",
    "            engineParam = dict()\n",
    "\n",
    "    recording_saved  = await runFunction(engine, save_preprocessing, rec, preprocessed_folder, **engineParam)    \n",
    "    display(recording_saved)\n",
    "else:\n",
    "    print(f\"Trying to load preprocessed data\")\n",
    "    recording_saved = si.load_extractor(preprocessed_folder)\n",
    "    display(recording_saved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9470dc2",
   "metadata": {},
   "source": [
    "## Identify spike clusters for the first few minutes of recording"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df08855",
   "metadata": {},
   "source": [
    "It is good practice to have a look at available ressources and current use of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f7a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkRessources()\n",
    "\n",
    "#!sinfo --nodes=node15 -o \"%50N  %10c  %20m  %30G \"\n",
    "!squeue --partition=\"GPU\"\n",
    "\n",
    "#torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 712.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 187.19 MiB is free. Process 1619368 has 77.78 GiB memory in use. Including non-PyTorch memory, this process has 1.28 GiB memory in use. Of the allocated memory 529.55 MiB is allocated by PyTorch, and 276.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for information, display a list of all parameters that can be modified for the sorter\n",
    "params = si.get_default_sorter_params(sorter_name_or_class=sorter)\n",
    "print(f\"For information, parameters that are available for the sorter {sorter} are:\\n\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c4d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse:\n",
    "    rec_training = recording_saved.frame_slice(0, 30_000 * 60 * duration_extract)\n",
    "    match engine,GPU_available:\n",
    "        case \"dask\", False:\n",
    "            engineParam=dict(\n",
    "                                queue='CPU',\n",
    "                                cores=1,\n",
    "                                memory=\"6GB\",\n",
    "                                job_cpu=55,\n",
    "                                worker_extra_args=[\"--lifetime\", \"2h\"],\n",
    "            )\n",
    "        case \"dask\", True:\n",
    "            engineParam=dict(\n",
    "                                queue='GPU',\n",
    "                                cores=1,\n",
    "                                memory=\"4GB\",\n",
    "                                worker_extra_args=[\"--lifetime\", \"2h\"],\n",
    "                                job_extra_directives=['--gres=gpu:1g.20gb:1'],\n",
    "\n",
    "            )                        \n",
    "        case \"submitit\", False:\n",
    "            engineParam=dict(\n",
    "                                slurm_array_parallelism=4,\n",
    "                                mem_gb=5,\n",
    "                                timeout_min=120,\n",
    "                                slurm_partition=\"CPU\",\n",
    "                                cpus_per_task=60\n",
    "            )\n",
    "        case \"submitit\", True:\n",
    "            engineParam=dict(\n",
    "                                mem_gb=5,\n",
    "                                timeout_min=10,\n",
    "                                slurm_partition=\"GPU\",\n",
    "                                cpus_per_task=2,\n",
    "            )\n",
    "        case _:\n",
    "            engineParam = dict()\n",
    "\n",
    "    sorter_params=dict(\n",
    "        do_correction = False,\n",
    "        skip_kilosort_preprocessing = True # we already did it\n",
    "    )\n",
    "    \n",
    "    sorting  = await runFunction(engine, GenerateDict, rec_training, sorter_folder, sorter_params, **engineParam)   \n",
    "    \n",
    "    display(sorting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf455d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not reAnalyse:\n",
    "    if os.path.isdir(sorter_folder):\n",
    "        # directory exists\n",
    "        print(f\"the previous folder {sorter_folder} was found, importing the data\")\n",
    "        sorting = si.read_sorter_folder(sorter_folder)\n",
    "        display(sorting)\n",
    "    else:\n",
    "        print(f\"the folder {sorter_folder} does not exist ; make sure of the path or reAnalyse the data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ff5c7",
   "metadata": {},
   "source": [
    "## Cure the clusters\n",
    "Here you should ensure that you are happy with the clusters that were found. For that, you should first compute analysis for the training clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037ccfe",
   "metadata": {},
   "source": [
    "### Fast initial curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cebbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sorting)\n",
    "if reAnalyse:\n",
    "    sorting = si.remove_duplicated_spikes(sorting=sorting)\n",
    "    sorting = si.remove_excess_spikes(sorting=sorting, recording=recording_corrected)\n",
    "    #Est-ce qu'on veut aussi remove_redundant_units?\n",
    "    display(sorting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfff988",
   "metadata": {},
   "source": [
    "### Construct analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse:\n",
    "    match engine,GPU_available:\n",
    "        case \"dask\", False:\n",
    "            engineParam=dict(\n",
    "                                queue='CPU',\n",
    "                                cores=1,\n",
    "                                memory=\"60GB\",\n",
    "                                job_cpu=40,\n",
    "                                worker_extra_args=[\"--lifetime\", \"2h\"],\n",
    "            )\n",
    "        case \"dask\", True:\n",
    "            engineParam=dict(\n",
    "                                queue='GPU',\n",
    "                                cores=1,\n",
    "                                memory=\"40GB\",\n",
    "                                worker_extra_args=[\"--lifetime\", \"2h\"],\n",
    "                                job_extra_directives=['--gres=gpu:1g.20gb:2'],\n",
    "\n",
    "            )                        \n",
    "        case \"submitit\", False:\n",
    "            engineParam=dict(\n",
    "                                slurm_array_parallelism=4,\n",
    "                                mem_gb=5,\n",
    "                                timeout_min=120,\n",
    "                                slurm_partition=\"CPU\",\n",
    "                                cpus_per_task=60\n",
    "            )\n",
    "        case \"submitit\", True:\n",
    "            engineParam=dict(\n",
    "                                mem_gb=5,\n",
    "                                timeout_min=10,\n",
    "                                slurm_partition=\"GPU\",\n",
    "                                cpus_per_task=2,\n",
    "            )\n",
    "        case _:\n",
    "            engineParam = dict()\n",
    "    \n",
    "    await runFunction(engine, compute_analyzer, sorting, rec_training, training_folder, **engineParam)    # add append=True to redo only a few analysis\n",
    "\n",
    "sorting_analyzer_training = si.load_sorting_analyzer(training_folder)\n",
    "display(sorting_analyzer_training)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036eba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt\n",
    "\n",
    "print('yeah')\n",
    "#si.plot_sorting_summary(sorting_analyzer_training, backend=\"spikeinterface_gui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4112693",
   "metadata": {},
   "source": [
    "Now, you have 2 options:\n",
    "- Either go back to local computer for full benefice of spikeinterface_gui\n",
    "1. First, copy the sorting_analyzer_training folder to crnldata ([see next cell](#download))\n",
    "1. Then, go on a local (not over ssh) script at [the most interactive viewing part](#local) at the end of this notebook, reload the sorting_analyzer older, and visualize everything on a gui.\n",
    "1. Finally, when you are happy with the spike clusters, you can upload it back to the crnl cluster to proceed with [full sorting](#sort-full-recording)\n",
    "- Or use [the following embedded plotting widgets](#widgets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9efe0c4",
   "metadata": {},
   "source": [
    "### Option1: go back to local PC\n",
    "<a id='download'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec648099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy initial sorting to crnldata for viewing\n",
    "reAnalyse=True\n",
    "if reAnalyse and engine==\"dask\":\n",
    "    #it takes about 10s\n",
    "\n",
    "    # takes about 10 minutes with dask\n",
    "    cluster = SLURMCluster(cores=1,\n",
    "                        memory=\"1GB\",\n",
    "                        job_cpu=1,\n",
    "                        walltime=\"00:05:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=1, # seems to divide ressources\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "       \n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    src=training_folder\n",
    "    currentFile_data = magicretrieve('currentFile_data')\n",
    "    dst=os.path.join(os.path.split(currentFile_data)[0],src)\n",
    "    print(dst)\n",
    "    start_time = time.time()\n",
    "    #future = client.submit(shutil.copytree, src, dst) \n",
    "    #_ = future.result() \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    display(sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cf3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/audrey.hay/HayLabAnalysis/python/sorting_analyzer_training\n",
    "cd /\n",
    "cd /mnt/data/ahay/\n",
    "cd /crnldata/forgetting/Aurelie/Annie/tests_analyses_annie/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806a8ac",
   "metadata": {},
   "source": [
    "Commandes pour copier des dossiers\n",
    "\n",
    "cp -r /home/audrey.hay/HayLabAnalysis/python/kilosort4_output /crnldata/forgetting/Aurelie/Annie/tests_analyses_annie/\n",
    "\n",
    "cp -r /home/audrey.hay/HayLabAnalysis/python/sorting_analyzer_training /crnldata/forgetting/Aurelie/Annie/tests_analyses_annie/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dbada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy initial sorting to crnldata for viewing\n",
    "if reAnalyse and engine==\"dask\":\n",
    "    #it takes about 10s\n",
    "\n",
    "    # takes about 10 minutes with dask\n",
    "    cluster = SLURMCluster(cores=1,\n",
    "                        memory=\"1GB\",\n",
    "                        job_cpu=1,\n",
    "                        walltime=\"00:05:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=1, # seems to divide ressources\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "       \n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    src=training_folder\n",
    "    currentFile_data = '//10.69.168.1/crnldata/waking/audrey_hay/NPX/tests_analyses_annie/'\n",
    "    dst=os.path.join(os.path.split(currentFile_data)[0],src)\n",
    "    start_time = time.time()\n",
    "    future = client.submit(shutil.copytree, src, dst) \n",
    "    _ = future.result() \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    display(sorting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890a0cdd",
   "metadata": {},
   "source": [
    "**Here is when you should work locally**\n",
    "and do the last part\n",
    "\n",
    "... and then come back on ssh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ce41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import back sorting_analyzer\n",
    "# reAnalyse = True\n",
    "if reAnalyse and engine==\"dask\":\n",
    "    #it takes about 10s\n",
    "\n",
    "    # takes about 10 minutes with dask\n",
    "    cluster = SLURMCluster(cores=1,\n",
    "                        memory=\"1GB\",\n",
    "                        job_cpu=1,\n",
    "                        walltime=\"00:05:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=1, # seems to divide ressources\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "       \n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    dst=training_folder\n",
    "    currentFile_data = magicretrieve('currentFile_data')\n",
    "    src=os.path.join(os.path.split(currentFile_data)[0],dst)\n",
    "    start_time = time.time()\n",
    "    future = client.submit(shutil.copytree, src, dst) \n",
    "    _ = future.result() \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    display(sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2477c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae12783",
   "metadata": {},
   "source": [
    "### Option2: inline visualisation\n",
    "<a id='widgets'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3241794",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_analyzer_training = si.load_sorting_analyzer(training_folder)\n",
    "display(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few ids (unités)\n",
    "#unit_ids=[1, 2, 5]\n",
    "unit_ids=sorting_analyzer_training.unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_unit_templates(sorting_analyzer_training, unit_ids=unit_ids, backend=\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97227a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few ids\n",
    "unit_ids=[1, 2, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14860987",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_rasters(sorting_analyzer_training, unit_ids=unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    si.plot_isi_distribution(sorting_analyzer_training,unit_ids=unit_ids)\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a628e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_autocorrelograms(sorting_analyzer_training, unit_ids=unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389668da",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_crosscorrelograms(sorting_analyzer_training, unit_ids=unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c822e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_unit_presence(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ca0a6",
   "metadata": {},
   "source": [
    "## Sort full recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse and engine==\"dask\":\n",
    "    gc.collect()\n",
    "    \n",
    "    if GPU_available: # takes about 10 minutes with dask\n",
    "        cluster = SLURMCluster(\n",
    "                        queue='GPU',\n",
    "                        cores=1,\n",
    "                        memory=\"70GB\",\n",
    "                        job_cpu=70,\n",
    "                        walltime=\"20:00:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=40, # seems to divide ressources\n",
    "                        #worker_extra_args=[\"--resources GPU=2\"],\n",
    "                        job_extra_directives=['--gpus=2'],\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "    else: # takes about 10 minutes\n",
    "       cluster = SLURMCluster(\n",
    "                        queue='CPU',\n",
    "                        cores=1,\n",
    "                        memory=\"6GB\",\n",
    "                        job_cpu=55,\n",
    "                        walltime=\"02:00:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=1, # seems to divide ressources\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "       \n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    print(cluster.job_script()) \n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    future = client.submit(compute_analyzer, sorting, recording_corrected, probe, fullAnalyzer_folder)\n",
    "    future.result()\n",
    "    #sorting_analyzer_training = future.results() \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    #display(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "    future = client.submit(GenerateDict, rec_training, probe, sorter_folder, **sorter_params)\n",
    "    sorting = future.result() \n",
    "    print(f\"job done in {time.time()- start_time} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adef6f8-a103-40aa-a640-8ad538b056ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse and engine=='submitit':\n",
    "    start_time = time.time()\n",
    "\n",
    "    executor = submitit.AutoExecutor(folder=os.getcwd()+'/si_logs/')\n",
    "    #executor.update_parameters(slurm_array_parallelism=2, mem_gb=30, timeout_min=10, slurm_partition=\"CPU\", cpus_per_task=50)\n",
    "    executor.update_parameters(mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=70) #cpus_per_task\n",
    "\n",
    "    # actually submit the job\n",
    "    job = executor.submit(compute_analyzer, sorting, recording_corrected, probe, fullAnalyzer_folder)\n",
    "\n",
    "    # print the ID of your job\n",
    "    print(\"submit job\" + str(job.job_id))  \n",
    "\n",
    "    # await a single result\n",
    "    await job.awaitable().results()\n",
    "    print(f\"job {job.job_id} completed in \" + str(time.time()-start_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a42b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#job 34074 completed in 408.1813361644745 second (slurm_array_parallelism=2, mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=50)\n",
    "#job 34078 completed in 437.409494638443 seconds (slurm_array_parallelism=3, mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=50)\n",
    "#job 34081 completed in 423.37460565567017 seconds (slurm_array_parallelism=2, mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", slurm_gres=\"gpu:2\", cpus_per_task=50)\n",
    "#job 34085 completed in 367.0000305175781 seconds (slurm_array_parallelism=2, mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=70)\n",
    "#job 34089 completed in 370.1982145309448 seconds (slurm_array_parallelism=2, mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=80)\n",
    "#job 34093 completed in 355.1876621246338 seconds (mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=70)\n",
    "\n",
    "last_job = job\n",
    "checkRessources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c4316",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not reAnalyse:\n",
    "    print(\"not running analysis but loading previous one\")\n",
    "    sorting_analyzer = si.load_sorting_analyzer(fullAnalyzer_folder)\n",
    "    display(sorting_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.export_report(sorting_analyzer=sorting_analyzer,output_folder='report')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2da484",
   "metadata": {},
   "source": [
    "# Most interactive viewing on local\n",
    "<a id='local'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 13s\n",
    "#from IPython.display import Javascript\n",
    "#Javascript(\"Jupyter.notebook.execute_cell_range(0,15)\")\n",
    "#TODO: here should find a way to execute previous cell, or have the function in a loadable module.\n",
    "currentFile_data = magicretrieve('currentFile_data')\n",
    "print(currentFile_data)\n",
    "sorting_analyzer_training = si.load_sorting_analyzer(os.path.join('//10.69.168.1',os.path.split(currentFile_data)[0],training_folder))\n",
    "display(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94970e70-d585-418e-ad3c-4ef9853d80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061be488",
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt\n",
    "si.plot_sorting_summary(sorting_analyzer_training, backend=\"spikeinterface_gui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d554903",
   "metadata": {},
   "outputs": [],
   "source": [
    "currentFile_data = magicretrieve('currentFile_data', storePath='')\n",
    "print(currentFile_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".si-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
