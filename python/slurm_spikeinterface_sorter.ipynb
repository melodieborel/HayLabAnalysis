{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a7dda0d",
   "metadata": {},
   "source": [
    "# Spike sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4cdd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reAnalyse = True\n",
    "engine = \"dask\"\n",
    "GPU_available = True\n",
    "sorterFolder='kilosort4_output'\n",
    "training_folder = 'sorting_analyzer_training'\n",
    "fullAnalyzer_folder = 'sorting_analyzer_full'\n",
    "whitenFirst = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd66156",
   "metadata": {},
   "source": [
    "## Set up everything\n",
    "You shouldn't have to change anything from here so you can keep that part folded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b006d50",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e4c2661-565d-4949-aa45-d44200c20f2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 2.0 or less. Got NumPy 2.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspikeinterface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfull\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msi\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[0;32m~/HayLabAnalysis/.si-env/lib/python3.11/site-packages/spikeinterface/full.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextractors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msorters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpostprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/HayLabAnalysis/.si-env/lib/python3.11/site-packages/spikeinterface/sorters/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasesorter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSorter\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msorterlist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainer_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContainerClient, install_package_in_container\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunsorter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_sorter, run_sorter_local, run_sorter_container, read_sorter_folder\n",
      "File \u001b[0;32m~/HayLabAnalysis/.si-env/lib/python3.11/site-packages/spikeinterface/sorters/sorterlist.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YassSorter\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# based on spikeinertface.sortingcomponents\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspyking_circus2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Spykingcircus2Sorter\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtridesclous2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tridesclous2Sorter\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimplesorter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleSorter\n",
      "File \u001b[0;32m~/HayLabAnalysis/.si-env/lib/python3.11/site-packages/spikeinterface/sorters/internal/spyking_circus2.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparsity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_sparsity\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msortinganalyzer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_sorting_analyzer\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_merge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_potential_auto_merge\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer_extension_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComputeTemplates\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparsity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelSparsity\n",
      "File \u001b[0;32m~/HayLabAnalysis/.si-env/lib/python3.11/site-packages/spikeinterface/curation/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuration_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_duplicated_spikes\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremove_redundant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_redundant_units, find_redundant_units\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremove_duplicated_spikes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_duplicated_spikes\n",
      "File \u001b[0;32m~/HayLabAnalysis/.si-env/lib/python3.11/site-packages/spikeinterface/curation/curation_tools.py:8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SortingAnalyzer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     HAVE_NUMBA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/HayLabAnalysis/.si-env/lib/python3.11/site-packages/numba/__init__.py:59\u001b[0m\n\u001b[1;32m     54\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m---> 59\u001b[0m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[0;32m~/HayLabAnalysis/.si-env/lib/python3.11/site-packages/numba/__init__.py:45\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_version \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     43\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba needs NumPy 2.0 or less. Got NumPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_version[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Numba needs NumPy 2.0 or less. Got NumPy 2.1."
     ]
    }
   ],
   "source": [
    "import spikeinterface.full as si\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import submitit\n",
    "from memory_profiler import memory_usage\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import asyncio\n",
    "import gc\n",
    "\n",
    "from mbTools import mbTools\n",
    "from ipyfilechooser import FileChooser\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython import get_ipython\n",
    "import IPython\n",
    "import pickleshare\n",
    "\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e190a",
   "metadata": {},
   "source": [
    "### Define a few variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf29f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_extract = 2 #min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac2f4e",
   "metadata": {},
   "source": [
    "#### Structural (important for the process but no need to change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e310982",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_job=None\n",
    "sorter='kilosort4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ed711",
   "metadata": {},
   "source": [
    "### Define a few functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def magicretrieve(stored_var):\n",
    "   # myvar will contain the variable previously stored with \"%store test\"\n",
    "   myvar_filename = get_ipython().ipython_dir + '/profile_default/db/autorestore/' + stored_var\n",
    "   with open(myvar_filename, 'rb') as f:\n",
    "      return pickle.load(f)\n",
    "\n",
    "def magicstore(stored_var, value):\n",
    "   # myvar will contain the variable previously stored with \"%store test\"\n",
    "   myvar_filename = get_ipython().ipython_dir + '/profile_default/db/autorestore/' + stored_var\n",
    "   with open(myvar_filename, 'wb') as f:\n",
    "      pickle.dump(value,f)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf1fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_my_expe_choice(chooser):\n",
    "    currentFile_data = str(chooser.selected)\n",
    "    currentFile_mnt = os.path.join('/mnt/data/ahay',os.path.split(currentFile_data)[1])\n",
    "    if not currentFile_data.startswith('/crnldata/'):\n",
    "        print('please make sure to select the file on crnldata')\n",
    "        return\n",
    "    # check if the file already exists on /mnt/\n",
    "    if os.path.isfile(currentFile_mnt):\n",
    "        print(f\"{currentFile_mnt} already exists\")\n",
    "    else:\n",
    "        print(f\"there is no version of {currentFile_data} on /mnt/\")\n",
    "        shouldCopy = False # it took a while (20 min for a file of about 150Gb)\n",
    "        if shouldCopy:\n",
    "            print(f'it will be copied to {currentFile_mnt}')\n",
    "            startTime = time.time()\n",
    "            shutil.copyfile(currentFile_data, currentFile_mnt)\n",
    "            print(f'the transfer is complete, it took {time.time()-startTime} seconds')\n",
    "        else:\n",
    "            print('it can be transfered from here by changing the shouldCopy parameter but probably best not to because it takes a while and uses massive ressources')\n",
    "\n",
    "    magicstore('currentFile_data', currentFile_data)\n",
    "    magicstore('currentFile_mnt', currentFile_mnt)\n",
    "\n",
    "\n",
    "def selectData(currentFile):\n",
    "    print(currentFile)\n",
    "    if currentFile is not None:\n",
    "        pathName, fileName = os.path.split(currentFile)\n",
    "    else:\n",
    "        pathName = '/crnldata/waking/audrey_hay/'\n",
    "        fileName = ''\n",
    "    fc = FileChooser(path=pathName, filename=fileName, filter_pattern='NP_spikes_*.raw', select_default=True, show_only_dirs = False, title = \"<b>Select file on crnldata</b>\")\n",
    "    display(fc)\n",
    "\n",
    "    # Register callback function\n",
    "    fc.register_callback(update_my_expe_choice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c82bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def runFunction(engine,funcName,*params,**engineParams):\n",
    "    start_time = time.time()\n",
    "    match engine:\n",
    "        case \"dask\":\n",
    "            print(\"dask\")\n",
    "            cluster = SLURMCluster(\n",
    "                                **engineParams,\n",
    "                                log_directory=\"si_dask_logs\",\n",
    "                                scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                                )\n",
    "            cluster.scale(1)\n",
    "            client = Client(cluster)\n",
    "\n",
    "            client.get_versions(check=True)\n",
    "    \n",
    "            from dask.distributed import PipInstall\n",
    "            plugin = PipInstall(packages=[\"git+https://github.com/SpikeInterface/spikeinterface.git\"], pip_options=[\"--upgrade\"])\n",
    "            client.register_plugin(plugin)\n",
    "            plugin = PipInstall(packages=[\"git+https://github.com/SpikeInterface/probeinterface.git\"], pip_options=[\"--upgrade\"])\n",
    "            client.register_plugin(plugin)\n",
    "            plugin = PipInstall(packages=[\"git+https://github.com/SpikeInterface/spikeinterface-gui.git\"], pip_options=[\"--upgrade\"])\n",
    "            client.register_plugin(plugin)\n",
    "            plugin = PipInstall(packages=[\"kilosort\"], pip_options=[\"--upgrade\"])\n",
    "            client.register_plugin(plugin)\n",
    "\n",
    "            plugin = PipInstall(packages=[\"numpy\"], pip_options=[\"--upgrade\"])\n",
    "            client.register_plugin(plugin)\n",
    "\n",
    "            print(cluster.job_script()) \n",
    "\n",
    "            future = client.submit(funcName, *params)\n",
    "            res = future.result()\n",
    "            \n",
    "            #recording_layers = preprocess_traces(raw_rec)\n",
    "            print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "            # Close cluster\n",
    "            client.close()\n",
    "            cluster.close()\n",
    "\n",
    "            return res\n",
    "        \n",
    "        case \"submitit\":\n",
    "            print(\"submitit\")\n",
    "\n",
    "            executor = submitit.AutoExecutor(folder=os.getcwd()+'/si_logs/')\n",
    "            executor.update_parameters(**engineParams)\n",
    "\n",
    "            job = executor.submit(funcName, *params)\n",
    "\n",
    "            # print the ID of your job\n",
    "            print(\"submit job\" + str(job.job_id))  \n",
    "\n",
    "            # await a single result\n",
    "            await job.awaitable().results()\n",
    "            print(f\"job {job.job_id} completed in \" + str(time.time()-start_time) + \" seconds\")\n",
    "\n",
    "            return job.result()\n",
    "\n",
    "        case _:\n",
    "            print(\"no engine\")\n",
    "\n",
    "            return funcName(*params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b539b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateDict(rec, probe, folder, **sorter_params):\n",
    "    rec = rec.set_probe(probe)\n",
    "    si.set_global_job_kwargs(n_jobs=40, progress_bar=True, chunk_duration=\"1s\")\n",
    "    sorting = si.run_sorter(sorter, rec, verbose=True, folder=folder, remove_existing_folder=True, **sorter_params)\n",
    "    return sorting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_analyzer(sorting,rec,probe,folder,append=False):\n",
    "\n",
    "    rec = rec.set_probe(probe)\n",
    "    si.set_global_job_kwargs(n_jobs=40, progress_bar=True, chunk_duration=\"1s\")\n",
    "    \n",
    "    if append:\n",
    "        sorting_analyzer = si.load_sorting_analyzer(folder)\n",
    "    else:\n",
    "        sorting_analyzer = si.create_sorting_analyzer(sorting, rec, sparse=True, folder=folder,overwrite=True)\n",
    "\n",
    "    sorting_analyzer.compute(\"random_spikes\", method=\"uniform\", max_spikes_per_unit=500)\n",
    "    sorting_analyzer.compute(\"waveforms\")\n",
    "    sorting_analyzer.compute(\"templates\")\n",
    "    sorting_analyzer.compute(\"noise_levels\")\n",
    "    sorting_analyzer.compute(\"unit_locations\", method=\"monopolar_triangulation\")\n",
    "    sorting_analyzer.compute(\"isi_histograms\")\n",
    "    sorting_analyzer.compute(\"correlograms\") #, window_ms=100, bin_ms=5.\n",
    "    sorting_analyzer.compute(\"principal_components\", n_components=3, mode='by_channel_global', whiten=True)\n",
    "    sorting_analyzer.compute(\"quality_metrics\", metric_names=[\"snr\", \"firing_rate\"])\n",
    "    sorting_analyzer.compute(\"template_similarity\")\n",
    "    sorting_analyzer.compute(\"spike_amplitudes\")\n",
    "\n",
    "\n",
    "    #sorting_analyzer.save_as(folder=folder, format='binary_folder')\n",
    "\n",
    "    #return sorting_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017793bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_traces(rec):\n",
    "    # filter traces\n",
    "    rec = rec.astype('float32')\n",
    "    rec_filt = si.bandpass_filter(rec)\n",
    "    display(rec_filt)\n",
    "\n",
    "    # Remove bad channels\n",
    "    try:\n",
    "        bad_channel_ids, channel_labels = si.detect_bad_channels(rec_filt)\n",
    "        print('bad_channel_ids', bad_channel_ids)\n",
    "        rec_filt = rec_filt.remove_channels(bad_channel_ids)\n",
    "    except Exception as error:\n",
    "        print(\"could not remove bad channels because there was an error:\")\n",
    "        print(error)\n",
    "\n",
    "    # Account for the slight delays in recordings due to the fact the 384 channels are only digitilized with 32 ADCs\n",
    "\n",
    "    inter_sample_shift=[0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                           0., 0., 0.07692308, 0.07692308, 0.15384615, 0.15384615, 0.23076923, 0.23076923, 0.30769231, 0.30769231,\n",
    "                         0.38461538, 0.38461538, 0.46153846, 0.46153846, 0.53846154, 0.53846154, 0.61538462, 0.61538462, 0.69230769,\n",
    "                           0.69230769, 0.76923077, 0.76923077, 0.84615385, 0.84615385,\n",
    "                               ]\n",
    "    rec_filt_shifted = si.phase_shift(rec_filt, inter_sample_shift=inter_sample_shift)\n",
    "\n",
    "    # Remove the common noisy events (artefacts)\n",
    "    rec_filt_ref = si.common_reference(rec_filt_shifted)\n",
    "    display(rec_filt_ref)\n",
    "\n",
    "    recording_layers = dict(\n",
    "        raw = rec,\n",
    "        filter = rec_filt,\n",
    "        realigned = rec_filt_shifted,\n",
    "        cmr = rec_filt_ref,\n",
    "    )\n",
    "    #si.plot_traces(recording_layers, backend='ipywidgets') #, mode='line'\n",
    "\n",
    "    return recording_layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_drift(rec, probe):\n",
    "    import numpy as np\n",
    "      \n",
    "    rec = rec.set_probe(probe)\n",
    "\n",
    "    job_kwargs = dict(n_jobs=40, progress_bar=True, chunk_duration=\"1s\")\n",
    "    si.set_global_job_kwargs(**job_kwargs)\n",
    "    \n",
    "    recording_corrected, motion, motion_info = si.correct_motion(\n",
    "            rec, preset=\"dredge_fast\", folder=None, output_motion=True, output_motion_info=True#, **job_kwargs\n",
    "        )\n",
    "    return (recording_corrected, motion, motion_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(rec,recording_layers,layerName):\n",
    "\n",
    "    rec_whitened = si.WhitenRecording(rec)\n",
    "\n",
    "    recording_layers[layerName] = rec_whitened\n",
    "\n",
    "    return recording_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339081a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkRessources():\n",
    "    # check node and CPU information\n",
    "    print(\"### Node counts: \\nA: currently in use \\B available\")\n",
    "    !sinfo -o%A\n",
    "    print(\"### CPU counts: \\nA: core currently in use \\nI: available \\nO: unavailable (maintenance, down, etc) \\nT: total\")\n",
    "    !sinfo -o%C\n",
    "    !sinfo\n",
    "\n",
    "    # check some stats of our last job\n",
    "    if last_job is not None:\n",
    "        print('### CPU time and MaxRSS of our last job (about 1000Mb should be added to your MaxRSS (Mb) in order to cover safely the memory needs of the python runtime)###')\n",
    "        os.system(f'sacct -j {last_job.job_id} --format=\"CPUTime,MaxRSS\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a24596",
   "metadata": {},
   "source": [
    "## Choose data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6dc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#currentFile_data = '/crnldata/waking/audrey_hay/NPX/NPX1/VB/Expe_2024-07-22_17-55-16/NP_spikes_2024-07-22T17_55_16.raw'\n",
    "#mbTools.magicstore('currentFile_data', currentFile_data)\n",
    "currentFile_data = magicretrieve('currentFile_data')\n",
    "selectData(currentFile_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8669ae",
   "metadata": {},
   "source": [
    "### Move data to /mnt/ if appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shouldCopy = False # it took a while (20 min for a file of about 150Gb)\n",
    "if shouldCopy:\n",
    "    currentFile_data = magicretrieve('currentFile_data')\n",
    "    currentFile_mnt = magicretrieve('currentFile_mnt')\n",
    "    print(f'The file {currentFile_data} will be copied to {currentFile_mnt}')\n",
    "    startTime = time.time()\n",
    "    shutil.copyfile(currentFile_data, currentFile_mnt)\n",
    "    print(f'the transfer is complete, it took {time.time()-startTime} seconds')\n",
    "else:\n",
    "    print('it can be transfered from here by changing the shouldCopy parameter but probably best not to because it takes a while and uses massive ressources')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532274e",
   "metadata": {},
   "source": [
    "### Lazy load data and probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8606c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "currentFile_data = magicretrieve('currentFile_data')\n",
    "currentFile_mnt = magicretrieve('currentFile_mnt')\n",
    "\n",
    "raw_rec = si.read_binary(currentFile_mnt, dtype='uint16', num_channels=384, sampling_frequency=30_000.,gain_to_uV=-1)\n",
    "raw_rec.annotate(raw_path = currentFile_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d0a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/crnldata/waking/audrey_hay/NPX/NPXprobe.pkl', 'rb') as outp: \n",
    "    probe = pickle.load(outp)\n",
    "probe.set_device_channel_indices(np.arange(384))\n",
    "\n",
    "raw_rec = raw_rec.set_probe(probe)\n",
    "display(si.plot_probe_map(raw_rec, with_channel_ids=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a43680",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c250e0",
   "metadata": {},
   "source": [
    "### Filter and apply common ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f39f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if engine==\"dask\":\n",
    "    # takes about 15s\n",
    "    cluster = SLURMCluster(\n",
    "                        queue='CPU',\n",
    "                        cores=1,\n",
    "                        memory=\"3GB\",\n",
    "                        walltime=\"00:02:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "\n",
    "    print(cluster.job_script()) \n",
    "\n",
    "    start_time = time.time()\n",
    "    future = client.submit(preprocess_traces, raw_rec)\n",
    "    recording_layers = future.result() \n",
    "\n",
    "\n",
    "    #recording_layers = preprocess_traces(raw_rec)\n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    si.plot_traces(recording_layers, backend='ipywidgets') #, mode='line'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if engine==\"submitit\":\n",
    "    # takes less than 10s\n",
    "    start_time = time.time()\n",
    "\n",
    "    executor = submitit.AutoExecutor(folder=os.getcwd()+'/si_logs/')\n",
    "    executor.update_parameters(mem_gb=3, timeout_min=2, slurm_partition=\"CPU\", cpus_per_task=4)\n",
    "    job = executor.submit(preprocess_traces, raw_rec)\n",
    "\n",
    "    # print the ID of your job\n",
    "    print(\"submit job\" + str(job.job_id))  \n",
    "\n",
    "    # await a single result\n",
    "    await job.awaitable().results()\n",
    "    print(f\"job {job.job_id} completed in \" + str(time.time()-start_time) + \" seconds\")\n",
    "\n",
    "    last_job = job\n",
    "    recording_layers = job.result()\n",
    "\n",
    "    si.plot_traces(recording_layers, backend='ipywidgets') #, mode='line'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637e35e",
   "metadata": {},
   "source": [
    "### Correct for motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "canal_lim=300 #limitation of canals to keep for drift correction\n",
    "#TODO : allow drift calculation on a few channels only\n",
    "\n",
    "if whitenFirst:\n",
    "    recording_layers = whiten(recording_layers[\"cmr\"],recording_layers,\"whitenFirst\")\n",
    "    rec_2_correct = recording_layers[\"whitenFirst\"]#.frame_slice(0,1000)#.channel_slice(np.arange(0,canal_lim))\n",
    "else:\n",
    "    rec_2_correct = recording_layers[\"cmr\"]#.frame_slice(0,100)#.channel_slice(np.arange(0,canal_lim))\n",
    "display(rec_2_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef37904",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'runFunction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mcase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01m_\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         engineParam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m---> 43\u001b[0m recording_corrected, motion, motion_info  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mrunFunction\u001b[49m(engine,check_drift,rec_2_correct, probe, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mengineParam)\n\u001b[1;32m     44\u001b[0m recording_layers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrected\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m recording_corrected\n\u001b[1;32m     45\u001b[0m display(motion)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'runFunction' is not defined"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    engine=\"dask\"\n",
    "    GPU_available=False\n",
    "    if reAnalyse:\n",
    "        match engine,GPU_available:\n",
    "            case \"dask\", True:\n",
    "                engineParam=dict(\n",
    "                                    job_extra_directives=['--gres=gpu:1g.20gb:2','--gpus=2'],\n",
    "                                    queue='GPU',\n",
    "                                    cores=1,\n",
    "                                    memory=\"40GB\",\n",
    "                                    nanny=False,\n",
    "                                    walltime=\"01:00:00\",\n",
    "                )\n",
    "            case \"dask\", False:\n",
    "                engineParam=dict(\n",
    "                                    queue='CPU',\n",
    "                                    cores=1,\n",
    "                                    memory=\"50GB\",\n",
    "                                    job_cpu=55,\n",
    "                                    nanny=False,\n",
    "                                    walltime=\"01:00:00\",\n",
    "                )\n",
    "            case \"submitit\", True:\n",
    "                engineParam=dict(\n",
    "                                    slurm_array_parallelism=40,\n",
    "                                    mem_gb=16,\n",
    "                                    timeout_min=20,\n",
    "                                    slurm_partition=\"GPU\",\n",
    "                                    cpus_per_task=2\n",
    "                )\n",
    "            case \"submitit\", False:\n",
    "                engineParam=dict(\n",
    "                                    slurm_array_parallelism=4,\n",
    "                                    mem_gb=60,\n",
    "                                    timeout_min=20,\n",
    "                                    slurm_partition=\"CPU\",\n",
    "                                    cpus_per_task=40\n",
    "                )\n",
    "            case _:\n",
    "                engineParam = dict()\n",
    "\n",
    "        recording_corrected, motion, motion_info  = await runFunction(engine,check_drift,rec_2_correct, probe, **engineParam)\n",
    "        recording_layers[\"corrected\"] = recording_corrected\n",
    "        display(motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: #reAnalyse and engine==\"dask\":\n",
    "    if False: #GPU_available: # takes about 8 minutes\n",
    "        cluster = SLURMCluster(\n",
    "                        queue='GPU',\n",
    "                        cores=1,\n",
    "                        memory=\"40GB\",\n",
    "                        #job_cpu=55, #should concider 1\n",
    "                        walltime=\"01:00:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        #n_workers=1, # seems to divide ressources\n",
    "                        job_extra_directives=['--gres=gpu:1g.20gb:2','--gpus=2'],\n",
    "                        scheduler_options={\n",
    "                            \"dashboard_address\": \":8780\",\n",
    "                                           } #port 8787 already used by jupyter\n",
    "                        )\n",
    "    else: # takes about 10 minutes\n",
    "        cluster = SLURMCluster(\n",
    "                        queue='CPU',\n",
    "                        cores=1,\n",
    "                        memory=\"40GB\",\n",
    "                        job_cpu=55,\n",
    "                        walltime=\"01:00:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=1, # seems to divide ressources\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    print(cluster.job_script())\n",
    "\n",
    "    start_time = time.time()\n",
    "    future = client.submit(check_drift, rec_2_correct, probe) \n",
    "    #TODO : allow drift calculation on a few channels only\n",
    "\n",
    "    #recording_corrected, motion, motion_info = future.result() \n",
    "    temp = future.result() \n",
    "    \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "\n",
    "    recording_layers[\"corrected\"] = recording_corrected\n",
    "    display(motion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab171b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine=\"submitit\"\n",
    "if False: #reAnalyse and engine==\"submitit\":\n",
    "    # takes about 10 min without slurm more like 16 min with submitit\n",
    "    GPU_available = False\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    executor = submitit.AutoExecutor(folder=os.getcwd()+'/si_logs/')\n",
    "    if GPU_available:\n",
    "        executor.update_parameters(slurm_array_parallelism=40, mem_gb=16, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=2)\n",
    "        #executor.update_parameters(mem_gb=16, timeout_min=20, slurm_partition=\"GPU\", slurm_gres='gpu:1')\n",
    "        #\n",
    "    else:\n",
    "        executor.update_parameters(slurm_array_parallelism=4, mem_gb=60, timeout_min=20, slurm_partition=\"CPU\", cpus_per_task=40)\n",
    "    job = executor.submit(check_drift, rec_2_correct, probe)\n",
    "\n",
    "    # print the ID of your job\n",
    "    print(\"submit job\" + str(job.job_id))  \n",
    "\n",
    "    # await a single result\n",
    "    await job.awaitable().results()\n",
    "    print(f\"job {job.job_id} completed in \" + str(time.time()-start_time) + \" seconds\")\n",
    "\n",
    "    last_job = job\n",
    "    (recording_corrected, motion, motion_info) = job.result()\n",
    "\n",
    "    display(motion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (72) 16 min (slurm_array_parallelism=4, mem_gb=60, timeout_min=20, slurm_partition=\"CPU\", cpus_per_task=40)\n",
    "print(\"\"\"rq: you should avoid submitting multiple small tasks with submitit, which would create many independent jobs\n",
    "      and possibly overload the cluster, while you can do it without any problem through dask.distributed.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7225843",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_motion_info(motion_info, recording_corrected,\n",
    "                   color_amplitude=True,\n",
    "        amplitude_cmap=\"inferno\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not whitenFirst:\n",
    "    recording_layers = whiten(recording_corrected,recording_layers,\"whitenLast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28144436",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_traces(recording_layers, backend='ipywidgets') #, mode='line'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caa0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if whitenFirst:\n",
    "    #rec = recording_layers[\"whitenFirst\"] #if whitened and not corrected\n",
    "    rec = recording_layers[\"corrected\"] #if whitened then corrected\n",
    "else:\n",
    "    rec = recording_layers[\"whitenLast\"] #if corrected the whiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9470dc2",
   "metadata": {},
   "source": [
    "## Identify spike clusters for the first few minutes of recording"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df08855",
   "metadata": {},
   "source": [
    "It is good practice to have a look at available ressources and current use of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f7a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkRessources()\n",
    "\n",
    "#!sinfo --nodes=node15 -o \"%50N  %10c  %20m  %30G \"\n",
    "!squeue --partition=\"GPU\"\n",
    "\n",
    "#torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 712.00 MiB. GPU 0 has a total capacity of 79.26 GiB of which 187.19 MiB is free. Process 1619368 has 77.78 GiB memory in use. Including non-PyTorch memory, this process has 1.28 GiB memory in use. Of the allocated memory 529.55 MiB is allocated by PyTorch, and 276.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for information, display a list of all parameters that can be modified for the sorter\n",
    "params = si.get_default_sorter_params(sorter_name_or_class=sorter)\n",
    "print(f\"For information, parameters that are available for the sorter {sorter} are:\\n\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7086848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse and engine is None:\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "    rec_training = recording_corrected.frame_slice(0, 30_000 * 60 * duration_extract)\n",
    "    sorter_params=dict(\n",
    "        do_correction = False,\n",
    "        skip_kilosort_preprocessing = True # we already did it\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    sorting = GenerateDict(rec_training, probe, sorterFolder, **sorter_params)\n",
    "\n",
    "    display(sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = \"dask\"\n",
    "if reAnalyse and engine==\"dask\":\n",
    "    gc.collect()\n",
    "\n",
    "    start_time = time.time()\n",
    "    rec_training = recording_corrected.frame_slice(0, 30_000 * 60 * duration_extract)\n",
    "    sorter_params=dict(\n",
    "        do_correction = False,\n",
    "        skip_kilosort_preprocessing = True # we already did it\n",
    "    )\n",
    "\n",
    "    \n",
    "    if GPU_available: # takes about 10 minutes with dask #longer?\n",
    "        cluster = SLURMCluster(\n",
    "                        queue='GPU',\n",
    "                        cores=1,\n",
    "                        memory=\"4GB\",\n",
    "                        job_cpu=1,\n",
    "                        walltime=\"00:10:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=1, # seems to divide ressources\n",
    "                        #worker_extra_args=[\"--resources GPU=2\"],\n",
    "                        job_extra_directives=['--gpus=2'],\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "    else: # takes about 40 minutes\n",
    "       cluster = SLURMCluster(\n",
    "                        queue='CPU',\n",
    "                        cores=1,\n",
    "                        memory=\"6GB\",\n",
    "                        job_cpu=55,\n",
    "                        walltime=\"02:00:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=1, # seems to divide ressources\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "       \n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "    client.get_versions(check=True)\n",
    "    \n",
    "    from dask.distributed import PipInstall\n",
    "    plugin = PipInstall(packages=[\"git+https://github.com/SpikeInterface/spikeinterface.git\"], pip_options=[\"--upgrade\"])\n",
    "    client.register_plugin(plugin)\n",
    "    plugin = PipInstall(packages=[\"git+https://github.com/SpikeInterface/probeinterface.git\"], pip_options=[\"--upgrade\"])\n",
    "    client.register_plugin(plugin)\n",
    "\n",
    "    print(cluster.job_script()) \n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    future = client.submit(GenerateDict, rec_training, probe, sorterFolder, **sorter_params)\n",
    "    sorting = future.result() \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    display(sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse and engine==\"submitit\":\n",
    "    #it takes about 90s with GPU (if available) ; 40 min otherwise\n",
    "    gc.collect()\n",
    "\n",
    "    GPU_available = True\n",
    "\n",
    "    start_time = time.time()\n",
    "    rec_training = recording_corrected.frame_slice(0, 30_000 * 60 * duration_extract)\n",
    "\n",
    "    executor = submitit.AutoExecutor(folder=os.getcwd()+'/si_logs/')\n",
    "    if GPU_available:\n",
    "        executor.update_parameters(mem_gb=5, timeout_min=10, slurm_partition=\"GPU\", cpus_per_task=2)\n",
    "        #executor.update_parameters(mem_gb=5, timeout_min=5, slurm_partition=\"GPU\", slurm_gres='gpu:1')\n",
    "    else:\n",
    "        executor.update_parameters(mem_gb=5, timeout_min=120, slurm_partition=\"CPU\", cpus_per_task=60)\n",
    "\n",
    "\n",
    "    # actually submit the job\n",
    "    job = executor.submit(GenerateDict, rec_training, probe)\n",
    "\n",
    "    # print the ID of your job\n",
    "    print(\"submit job\" + str(job.job_id))  \n",
    "\n",
    "    # await a single result\n",
    "    await job.awaitable().results()\n",
    "    print(f\"job {job.job_id} completed in \" + str(time.time()-start_time) + \" seconds\")\n",
    "\n",
    "    last_job = job\n",
    "    sorting = job.result()\n",
    "    display(sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277d8c6-e2e1-46e1-8dee-4424d98675d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kilosort4 run time 2212.34s for 8Gb 10cpus num1 (15.74, 15.42, 1.49, 2.11)\n",
    "#100%|██████████| 60/60 [39:44<00:00, 39.74s/it] 8/10 python\n",
    "\n",
    "#32%|███▏      | 19/60 [09:26<19:40, 28.78s/it] 8/30 submitit\n",
    "#32%|███▏      | 19/60 [09:13<19:55, 29.16s/it] 16/30 submitit\n",
    "#60%|██████    | 36/60 [12:10<07:36, 19.02s/it] 5/30 submitit\n",
    "#23%|██▎       | 14/60 [10:14<32:55, 42.95s/it] 5/10 submitit\n",
    "#42%|████▏     | 25/60 [06:35<07:23, 12.66s/it] 5/50 submitit\n",
    "#33%|███▎      | 20/60 [05:32<10:29, 15.75s/it] 5/50 submitit data in mnt\n",
    "\n",
    "\n",
    "#GPU\n",
    "#job completed: 33972 returned in 110.73936009407043 seconds 5/50\n",
    "#job completed: 33973 returned in 94.93399000167847 seconds 5/10\n",
    "#job completed: 33975 returned in 93.79518842697144 seconds 5/2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee34b754",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "file_path=\"kilosort4_output/spikeinterface_recording.pickle\"\n",
    "with open(file_path, \"rb\") as f:\n",
    "    d = pickle.load(f)\n",
    "\n",
    "for key in d[\"kwargs\"]:\n",
    "#for key in d:\n",
    "    print(key)\n",
    "\n",
    "print(d[\"kwargs\"][\"parent_recording\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf455d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not reAnalyse:\n",
    "    if os.path.isdir(sorterFolder):\n",
    "        # directory exists\n",
    "        print(f\"the previous folder {sorterFolder} was found, importing the data\")\n",
    "        sorting = si.read_sorter_folder(sorterFolder)\n",
    "        display(sorting)\n",
    "    else:\n",
    "        print(f\"the folder {sorterFolder} does not exist ; make sure of the path or reAnalyse the data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ff5c7",
   "metadata": {},
   "source": [
    "## Cure the clusters\n",
    "Here you should ensure that you are happy with the clusters that were found. For that, you should first compute analysis for the training clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037ccfe",
   "metadata": {},
   "source": [
    "### Fast initial curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cebbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(sorting)\n",
    "sorting = si.remove_duplicated_spikes(sorting=sorting)\n",
    "sorting = si.remove_excess_spikes(sorting=sorting, recording=recording_corrected)\n",
    "#Est-ce qu'on veut aussi remove_redundant_units?\n",
    "display(sorting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfff988",
   "metadata": {},
   "source": [
    "### Construct analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f54b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse and engine is None:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    compute_analyzer(sorting, rec_training, probe, training_folder) #, append=True) # uncomment to redo only a few analysis\n",
    "\n",
    "    #display(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reAnalyse=True\n",
    "GPU_available=False\n",
    "if reAnalyse and engine==\"dask\":\n",
    "    gc.collect()\n",
    "    \n",
    "    if GPU_available: # takes about 1.5 minutes with dask\n",
    "        cluster = SLURMCluster(\n",
    "                        queue='GPU',\n",
    "                        cores=1,\n",
    "                        memory=\"70GB\",\n",
    "                        job_cpu=70,\n",
    "                        walltime=\"01:20:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=40, # seems to divide ressources\n",
    "                        #worker_extra_args=[\"--resources GPU=2\"],\n",
    "                        job_extra_directives=['--gpus=2'],\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "    else: # takes about 1.5 min\n",
    "       cluster = SLURMCluster(\n",
    "                        queue='CPU',\n",
    "                        cores=1,\n",
    "                        memory=\"60GB\",\n",
    "                        job_cpu=40,\n",
    "                        walltime=\"02:00:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=40, # seems to divide ressources\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "       \n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    print(cluster.job_script()) \n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    future = client.submit(compute_analyzer, sorting, rec_training, probe, training_folder) #, append=True) # uncomment to redo only a few analysis\n",
    "    future.result()\n",
    "    #sorting_analyzer_training = future.results() \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    #display(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4112693",
   "metadata": {},
   "source": [
    "Now, you have 2 options:\n",
    "- Either go back to local computer for full benefice of spikeinterface_gui\n",
    "1. First, copy the sorting_analyzer_training folder to crnldata ([see next cell](#download))\n",
    "1. Then, go on a local (not over ssh) script at [the most interactive viewing part](#local) at the end of this notebook, reload the sorting_analyzer older, and visualize everything on a gui.\n",
    "1. Finally, when you are happy with the spike clusters, you can upload it back to the crnl cluster to proceed with [full sorting](#sort-full-recording)\n",
    "- Or use [the following embedded plotting widgets](#widgets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9efe0c4",
   "metadata": {},
   "source": [
    "### Option1: go back to local PC\n",
    "<a id='download'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec648099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy initial sorting to crnldata for viewing\n",
    "if reAnalyse and engine==\"dask\":\n",
    "    #it takes about 10s\n",
    "\n",
    "    # takes about 10 minutes with dask\n",
    "    cluster = SLURMCluster(cores=1,\n",
    "                        memory=\"1GB\",\n",
    "                        job_cpu=1,\n",
    "                        walltime=\"00:05:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=1, # seems to divide ressources\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "       \n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    src=training_folder\n",
    "    currentFile_data = magicretrieve('currentFile_data')\n",
    "    dst=os.path.join(os.path.split(currentFile_data)[0],src)\n",
    "    start_time = time.time()\n",
    "    future = client.submit(shutil.copytree, src, dst) \n",
    "    _ = future.result() \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    display(sorting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890a0cdd",
   "metadata": {},
   "source": [
    "**Here is when you should work locally**\n",
    "and do the last part\n",
    "\n",
    "... and then come back on ssh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ce41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import back sorting_analyzer\n",
    "# reAnalyse = True\n",
    "if reAnalyse and engine==\"dask\":\n",
    "    #it takes about 10s\n",
    "\n",
    "    # takes about 10 minutes with dask\n",
    "    cluster = SLURMCluster(cores=1,\n",
    "                        memory=\"1GB\",\n",
    "                        job_cpu=1,\n",
    "                        walltime=\"00:05:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=1, # seems to divide ressources\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "       \n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    dst=training_folder\n",
    "    currentFile_data = magicretrieve('currentFile_data')\n",
    "    src=os.path.join(os.path.split(currentFile_data)[0],dst)\n",
    "    start_time = time.time()\n",
    "    future = client.submit(shutil.copytree, src, dst) \n",
    "    _ = future.result() \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    display(sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2477c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae12783",
   "metadata": {},
   "source": [
    "### Option2: inline visualisation\n",
    "<a id='widgets'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3241794",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_analyzer_training = si.load_sorting_analyzer(\"sorting_analyzer_training\")\n",
    "display(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few ids (unités)\n",
    "#unit_ids=[1, 2, 5]\n",
    "unit_ids=sorting_analyzer_training.unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_unit_templates(sorting_analyzer_training, unit_ids=unit_ids, backend=\"ipywidgets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97227a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few ids\n",
    "unit_ids=[1, 2, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14860987",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_rasters(sorting_analyzer_training, unit_ids=unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    si.plot_isi_distribution(sorting_analyzer_training,unit_ids=unit_ids)\n",
    "except Exception as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a628e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_autocorrelograms(sorting_analyzer_training, unit_ids=unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389668da",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_crosscorrelograms(sorting_analyzer_training, unit_ids=unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c822e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_unit_presence(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ca0a6",
   "metadata": {},
   "source": [
    "## Sort full recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse and engine==\"dask\":\n",
    "    gc.collect()\n",
    "    \n",
    "    if GPU_available: # takes about 10 minutes with dask\n",
    "        cluster = SLURMCluster(\n",
    "                        queue='GPU',\n",
    "                        cores=1,\n",
    "                        memory=\"70GB\",\n",
    "                        job_cpu=70,\n",
    "                        walltime=\"00:20:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=40, # seems to divide ressources\n",
    "                        #worker_extra_args=[\"--resources GPU=2\"],\n",
    "                        job_extra_directives=['--gpus=2'],\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "    else: # takes about 10 minutes\n",
    "       cluster = SLURMCluster(\n",
    "                        queue='CPU',\n",
    "                        cores=1,\n",
    "                        memory=\"6GB\",\n",
    "                        job_cpu=55,\n",
    "                        walltime=\"02:00:00\",\n",
    "                        log_directory=\"si_dask_logs\",\n",
    "                        nanny=False,\n",
    "                        n_workers=1, # seems to divide ressources\n",
    "                        scheduler_options={\"dashboard_address\": \":8780\"} #port 8787 already used by jupyter\n",
    "                        )\n",
    "       \n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster)\n",
    "\n",
    "    print(cluster.job_script()) \n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    future = client.submit(compute_analyzer, sorting, recording_corrected, probe, fullAnalyzer_folder)\n",
    "    future.result()\n",
    "    #sorting_analyzer_training = future.results() \n",
    "    print(f\"job done in {time.time()- start_time} s\")\n",
    "\n",
    "    # Close cluster\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "\n",
    "    #display(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adef6f8-a103-40aa-a640-8ad538b056ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reAnalyse and engine=='submitit':\n",
    "    start_time = time.time()\n",
    "\n",
    "    executor = submitit.AutoExecutor(folder=os.getcwd()+'/si_logs/')\n",
    "    #executor.update_parameters(slurm_array_parallelism=2, mem_gb=30, timeout_min=10, slurm_partition=\"CPU\", cpus_per_task=50)\n",
    "    executor.update_parameters(mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=70) #cpus_per_task\n",
    "\n",
    "    # actually submit the job\n",
    "    job = executor.submit(compute_analyzer, sorting, recording_corrected, probe, fullAnalyzer_folder)\n",
    "\n",
    "    # print the ID of your job\n",
    "    print(\"submit job\" + str(job.job_id))  \n",
    "\n",
    "    # await a single result\n",
    "    await job.awaitable().results()\n",
    "    print(f\"job {job.job_id} completed in \" + str(time.time()-start_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a42b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#job 34074 completed in 408.1813361644745 second (slurm_array_parallelism=2, mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=50)\n",
    "#job 34078 completed in 437.409494638443 seconds (slurm_array_parallelism=3, mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=50)\n",
    "#job 34081 completed in 423.37460565567017 seconds (slurm_array_parallelism=2, mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", slurm_gres=\"gpu:2\", cpus_per_task=50)\n",
    "#job 34085 completed in 367.0000305175781 seconds (slurm_array_parallelism=2, mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=70)\n",
    "#job 34089 completed in 370.1982145309448 seconds (slurm_array_parallelism=2, mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=80)\n",
    "#job 34093 completed in 355.1876621246338 seconds (mem_gb=60, timeout_min=20, slurm_partition=\"GPU\", cpus_per_task=70)\n",
    "\n",
    "last_job = job\n",
    "checkRessources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c4316",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not reAnalyse:\n",
    "    print(\"not running analysis but loading previous one\")\n",
    "    sorting_analyzer = si.load_sorting_analyzer(fullAnalyzer_folder)\n",
    "    display(sorting_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.export_report(sorting_analyzer=sorting_analyzer,output_folder='report')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2da484",
   "metadata": {},
   "source": [
    "# Most interactive viewing on local\n",
    "<a id='local'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc5b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 13s\n",
    "#TODO: here should find a way to retrieve magicdata from ssh\n",
    "baseName = '/crnldata/waking/audrey_hay/NPX/NPX1/VB/Expe_2024-07-22_17-55-16/'\n",
    "sorting_analyzer_training = si.load_sorting_analyzer(os.path.join('//10.69.168.1',baseName,training_folder))\n",
    "display(sorting_analyzer_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061be488",
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt\n",
    "si.plot_sorting_summary(sorting_analyzer_training, backend=\"spikeinterface_gui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee0a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".si-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
