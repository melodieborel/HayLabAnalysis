{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spdl oscillations analysis...\n",
      "... for L1 neurons...\n",
      "Spdl_Global_BlackLinesOK.pkl\n",
      "Spdl_Global_BlueLinesOK.pkl\n",
      "Spdl_Global_GreenDotsOK.pkl\n",
      "Spdl_Global_GreenLinesOK.pkl\n",
      "Spdl_Global_RedLinesOK.pkl\n",
      "Spdl_Global_BlueLinesOK.pkl\n",
      "Spdl_Global_BlackLinesOK.pkl\n",
      "Spdl_Global_GreenDotsOK.pkl\n",
      "180 L1 neurons recorded\n",
      "179 L1 neurons in the cross-registration\n",
      "2064 Spdl recorded in total\n",
      "Spdl_CaPSTH_S1Baseline\n",
      "Spdl_CaPSTH_UnCoupledS1Baseline\n",
      "Spdl_CaPSTH_CoupledS1Baseline\n",
      "Spdl_CaPSTH_PFCBaseline\n",
      "Spdl_CaPSTH_UnCoupledPFCBaseline\n",
      "Spdl_CaPSTH_CoupledPFCBaseline\n",
      "Spdl_CaPSTH_S1PFCBaseline\n",
      "Spdl_CaPSTH_UnCoupledS1PFCBaseline\n",
      "Spdl_CaPSTH_CoupledS1PFCBaseline\n",
      "Spdl_SpPSTH_S1Baseline\n",
      "Spdl_SpPSTH_UnCoupledS1Baseline\n",
      "Spdl_SpPSTH_CoupledS1Baseline\n",
      "Spdl_SpPSTH_PFCBaseline\n",
      "Spdl_SpPSTH_UnCoupledPFCBaseline\n",
      "Spdl_SpPSTH_CoupledPFCBaseline\n",
      "Spdl_SpPSTH_S1PFCBaseline\n",
      "Spdl_SpPSTH_UnCoupledS1PFCBaseline\n",
      "Spdl_SpPSTH_CoupledS1PFCBaseline\n",
      "... for L2&3 neurons...\n",
      "Spdl_Global_Purple.pkl\n",
      "Spdl_Global_ThreeBlueCrossesOK.pkl\n",
      "Spdl_Global_ThreeColDotsOK.pkl\n",
      "Spdl_Global_Purple.pkl\n",
      "Spdl_Global_ThreeColDotsOK.pkl\n",
      "805 L2&3 neurons recorded\n",
      "803 L2&3 neurons in the cross-registration\n",
      "1994 Spdl recorded in total\n",
      "Spdl_CaPSTH_S1Baseline\n",
      "Spdl_CaPSTH_UnCoupledS1Baseline\n",
      "Spdl_CaPSTH_CoupledS1Baseline\n",
      "Spdl_CaPSTH_PFCBaseline\n",
      "Spdl_CaPSTH_UnCoupledPFCBaseline\n",
      "Spdl_CaPSTH_CoupledPFCBaseline\n",
      "Spdl_CaPSTH_S1PFCBaseline\n",
      "Spdl_CaPSTH_UnCoupledS1PFCBaseline\n",
      "Spdl_CaPSTH_CoupledS1PFCBaseline\n",
      "Spdl_SpPSTH_S1Baseline\n",
      "Spdl_SpPSTH_UnCoupledS1Baseline\n",
      "Spdl_SpPSTH_CoupledS1Baseline\n",
      "Spdl_SpPSTH_PFCBaseline\n",
      "Spdl_SpPSTH_UnCoupledPFCBaseline\n",
      "Spdl_SpPSTH_CoupledPFCBaseline\n",
      "Spdl_SpPSTH_S1PFCBaseline\n",
      "Spdl_SpPSTH_UnCoupledS1PFCBaseline\n",
      "Spdl_SpPSTH_CoupledS1PFCBaseline\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 290\u001b[0m\n\u001b[0;32m    288\u001b[0m filenameOut \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_to_save2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mList_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOsc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNrSubtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mPSTH_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mctx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrug\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#keep each responses of each cells for all rec Osc\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filenameOut, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pickle_file:\n\u001b[1;32m--> 290\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(df_per_sheet, pickle_file)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_per_sheet\u001b[38;5;241m.\u001b[39mkeys()): \u001b[38;5;66;03m# not empty\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# if PSTH is full lenght\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     arrayLen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# Stats on Ca2+ imaging with miniscope and Osc\n",
    "#######################################################################################\n",
    "                            # Define Experiment type #\n",
    "#######################################################################################\n",
    "\n",
    "DrugExperiment=0 # 1 if CGP Experiment, 0 if Baseline Experiment\n",
    "\n",
    "suffix='_AH_1secPSTHpkl'\n",
    "\n",
    "Local=1\n",
    "\n",
    "#choosed_folder='Osc_2024_08_04_10_35_33_AH_FINAL' if DrugExperiment else 'Osc_2024_08_04_09_24_52_AH_FINAL'\n",
    "choosed_folder1='Osc_2024_09_04_18_10_30_AH_DS' # for Baseline Expe\n",
    "choosed_folder2='Osc_2024_09_04_20_16_42_AH_DS' # for CGP Expe\n",
    "\n",
    "PrefVigExcel_file = 'AVG_VigSt_2024-09-03_11_16_02_AB_wRealTS_forCGP' if DrugExperiment else 'AVG_VigSt_2024-09-03_11_00_49_AB_wRealTS'\n",
    "#PrefVigExcel_file = 'AVG_VigSt_2024-08-22_13_50_33_AB_Clustering_forCGP' if DrugExperiment else 'AVG_VigSt_2024-08-22_13_28_19_AB_Clustering'\n",
    "\n",
    "#######################################################################################\n",
    "                                # Load packages #\n",
    "#######################################################################################\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import quantities as pq\n",
    "import numpy as np\n",
    "import math \n",
    "import neo\n",
    "import json\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button, Cursor\n",
    "import pickle\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import sem\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "from scipy.stats import norm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def column_with_max_single_per_row(row):\n",
    "    max_val = row.max()  # Find the max value in the row\n",
    "    max_columns = row == max_val  # Boolean Series of columns with max value\n",
    "    \n",
    "    if max_columns.sum() > 1:\n",
    "        return 'NoPref'  # More than one column has the max value\n",
    "    else:\n",
    "        return max_columns.idxmax()  # Return the name of the column with max value\n",
    "\n",
    "def divide_keys(data):\n",
    "    it=list(data.keys())[0]\n",
    "    d=data[it]\n",
    "    data[it]=d.replace(0, np.nan)\n",
    "    for sheet in list(data.keys())[1:]: \n",
    "        data[sheet].div(data[it][sheet], axis=0)\n",
    "    del data[it]\n",
    "    return data    \n",
    "\n",
    "########################################################################\n",
    "        # SCRIPT 27AB_GrandAverages&Stats_for_Osc\n",
    "########################################################################\n",
    "\n",
    "# Specify the directory containing the Excel files\n",
    "InitialDirectory1 = \"//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/Baseline_recording_ABmodified/AB_Analysis\" if Local else \"/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/Baseline_recording_ABmodified/AB_Analysis\" \n",
    "directory1= f'{InitialDirectory1}/{choosed_folder1}'\n",
    "InitialDirectory2 =\"//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/CGP/AB_Analysis\" if Local else \"/crnldata/waking///L1imaging/AnalysedMarch2023/Gaelle/CGP/AB_Analysis\"\n",
    "directory2= f'{InitialDirectory2}/{choosed_folder2}'\n",
    "\n",
    "# Get the current date and time\n",
    "FolderNameSave=str(datetime.now())[:19]\n",
    "FolderNameSave = FolderNameSave.replace(\" \", \"_\").replace(\".\", \"_\").replace(\":\", \"_\")\n",
    "destination_folder= f\"//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/AB_GlobalAnalysis/AVG_Osc_{FolderNameSave}{suffix}/\"\n",
    "os.makedirs(destination_folder)\n",
    "folder_to_save=Path(destination_folder)\n",
    "\n",
    "# Copy the script file to the destination folder\n",
    "source_script = \"C:/Users/Manip2/SCRIPTS/CodePythonAudrey/CodePythonAurelie/HayLabAnalysis/python/25_28_GrandAverage&Stats_for_DownStatesSpdl&SWR_FullAuto.ipynb\"\n",
    "destination_file_path = f\"{destination_folder}/25_28_GrandAverage&Stats_for_DownStatesSpdl&SWR_FullAuto.txt\"\n",
    "shutil.copy(source_script, destination_file_path)\n",
    "\n",
    "NrSubtypeList=['L1', 'L2&3']\n",
    "CTX=['S1_', 'PFC_', 'S1PFC_']\n",
    "CTX2=['S1', 'PFC', 'S1PFC']\n",
    "Coupling=['', 'UnCoupled', 'Coupled']\n",
    "OscList=['Spdl', 'SWR', 'DS']\n",
    "Drugs= ['Baseline', 'CGP'] if DrugExperiment else ['Baseline']\n",
    "\n",
    "for o, Osc in enumerate(OscList): \n",
    "    \n",
    "    print(Osc, 'oscillations analysis...')\n",
    "\n",
    "    AllOscStatut=pd.DataFrame()\n",
    "    AllOscDuration=pd.DataFrame()\n",
    "    AllOscCoupling=pd.DataFrame()\n",
    "    AllGlobalSpindle=pd.DataFrame()\n",
    "    AllOscStartLocation=pd.DataFrame()\n",
    "    \n",
    "    AllOscStatutS1=pd.DataFrame()\n",
    "    AllOscDurationS1=pd.DataFrame()\n",
    "    AllOscCouplingS1=pd.DataFrame()\n",
    "\n",
    "    AllOscStatutPFC=pd.DataFrame()\n",
    "    AllOscDurationPFC=pd.DataFrame()\n",
    "    AllOscCouplingPFC=pd.DataFrame()\n",
    "\n",
    "    AllOscStatutS1PFC=pd.DataFrame()\n",
    "    AllOscDurationS1PFC=pd.DataFrame()\n",
    "    AllOscCouplingS1PFC=pd.DataFrame()\n",
    "\n",
    "    for NrSubtype in NrSubtypeList: \n",
    "        \n",
    "        print('... for', NrSubtype, 'neurons...')\n",
    "        \n",
    "        # Initialize an empty list to store the dataframes\n",
    "        dfs = []\n",
    "        filtered_df=[]\n",
    "\n",
    "        if NrSubtype=='L1':\n",
    "            MiceList=['BlackLinesOK', 'BlueLinesOK', 'GreenDotsOK', 'GreenLinesOK', 'RedLinesOK']\n",
    "        else:\n",
    "            MiceList=['Purple', 'ThreeColDotsOK', 'ThreeBlueCrossesOK']\n",
    "\n",
    "        nametofind=f'{Osc}_Global'\n",
    "\n",
    "        ###########################################################################\n",
    "                                    ##### GLOBAL #####\n",
    "        ###########################################################################\n",
    "\n",
    "        # Recursively traverse the directory structure\n",
    "        for directory in [directory1, directory2]: #both Baseline & CGP experiments \n",
    "            for root, _, files in os.walk(directory):\n",
    "                for filename in files:\n",
    "                    # Check if the file is an Excel file and contains the specified name\n",
    "                    if filename.endswith('.pkl') and nametofind in filename:\n",
    "                        if any(name in filename for name in MiceList):  \n",
    "                            # Construct the full path to the file\n",
    "                            filepath = os.path.join(root, filename)\n",
    "                            # Read the file and append it to the list\n",
    "                            with open(filepath, 'rb') as pickle_file:\n",
    "                                df = pickle.load(pickle_file)\n",
    "                            dfs.append(df)\n",
    "                            print(filename)\n",
    "\n",
    "        # Concatenate all dataframes into a single dataframe\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        combined_df['Unique_Unit'] = combined_df['Unique_Unit'].astype(str)\n",
    "        combined_df['UnitNumber'] = combined_df['UnitNumber'].astype(str)\n",
    "        combined_df['UnitValue'] = combined_df['UnitValue'].astype(str)\n",
    "\n",
    "        combined_df[f'{Osc}Statut'] = combined_df[f'{Osc}Statut'].astype(str)\n",
    "\n",
    "        combined_df['Unit_ID'] = combined_df['Mice'] + combined_df['Unique_Unit']\n",
    "        combined_df['Session_ID'] = combined_df['Mice'] + combined_df['Session'].astype(str)\n",
    "\n",
    "        unique_count = combined_df['Unit_ID'].nunique()\n",
    "        print(unique_count, f'{NrSubtype} neurons recorded') \n",
    "\n",
    "        # Remove non defined Unique Units \n",
    "        combined_df = combined_df[combined_df['Unique_Unit'] != '[]']\n",
    "        combined_df = combined_df.dropna(subset=['Unique_Unit'])\n",
    "        unique_count = combined_df['Unit_ID'].nunique()\n",
    "        print(unique_count, f'{NrSubtype} neurons in the cross-registration') \n",
    "        \n",
    "        combined_df[f'{Osc}_ID'] = combined_df['Mice'] + combined_df['Session'] + combined_df[f'{Osc}Number'].astype(str)\n",
    "        \n",
    "        unique_count = combined_df[f'{Osc}_ID'].nunique()\n",
    "        print(unique_count, f'{Osc} recorded in total')\n",
    "\n",
    "        filenameOut = f'{folder_to_save}/{NrSubtype}_{Osc}_Global.xlsx'\n",
    "        writer = pd.ExcelWriter(filenameOut)\n",
    "        combined_df.to_excel(writer)\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "        ###########################################################################\n",
    "                                    ##### PSTH #####\n",
    "        ###########################################################################\n",
    "\n",
    "        Data=['Ca', 'Sp']\n",
    "        for data in Data:\n",
    "            for drug in Drugs: \n",
    "                for ctx in CTX2:\n",
    "                    for coup in Coupling:  \n",
    "\n",
    "                        ctx= '' if Osc=='SWR' else ctx  \n",
    "                        \n",
    "                        locals()[f'{data}PSTH{Osc}_{coup}{ctx}{drug}']={}\n",
    "                        dfsPSTH_per_sheet = locals()[f'{data}PSTH{Osc}_{coup}{ctx}{drug}']\n",
    "                        nametofind2=f'{Osc}_{data}PSTH_{coup}{ctx}{drug}'\n",
    "                        print(nametofind2)\n",
    "                        # Recursively traverse the directory structure\n",
    "                        for directory in [directory1, directory2]:\n",
    "                            for root, _, files in os.walk(directory):\n",
    "                                for filename in files:\n",
    "                                    if filename.endswith('.pkl') and nametofind2 in filename: \n",
    "                                        if any(name in filename for name in MiceList): \n",
    "                                            # Construct the full path to the file\n",
    "                                            filepath = os.path.join(root, filename)\n",
    "                                            with open(filepath, 'rb') as pickle_file:\n",
    "                                                df = pickle.load(pickle_file)\n",
    "                                            for key, value in df.items():\n",
    "                                                if key in dfsPSTH_per_sheet:\n",
    "                                                    dfsPSTH_per_sheet[key]=pd.concat([pd.DataFrame(dfsPSTH_per_sheet[key]),pd.DataFrame(value)], ignore_index=False, axis=0)\n",
    "                                                else:\n",
    "                                                    dfsPSTH_per_sheet[key]=pd.DataFrame(value)\n",
    "\n",
    "\n",
    "        ###########################################################################\n",
    "                                ##### PREFERENCE #####\n",
    "        ########################################################################### \n",
    "\n",
    "        for drug in Drugs: \n",
    "            \n",
    "            combined_df_Drug = combined_df.copy()\n",
    "            try:\n",
    "                combined_df_Drug = combined_df_Drug[combined_df_Drug['Drug'] == drug]\n",
    "            except: \n",
    "                combined_df_Drug=combined_df_Drug\n",
    "            \n",
    "            folder_to_save2= f'{folder_to_save}/{drug}/'\n",
    "            if NrSubtype=='L1' and o==0 :\n",
    "                os.makedirs(folder_to_save2)\n",
    "                        \n",
    "            # Load the Excel file and read each sheet into a separate DataFrame\n",
    "            excel_file = f'//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/AB_GlobalAnalysis/{PrefVigExcel_file}/Baseline/{NrSubtype}_ActivityPreference.xlsx'\n",
    "            sheets = pd.read_excel(excel_file, sheet_name=None, header=None, index_col=0)  # sheet_name=None reads all sheets\n",
    "            AllUnits=combined_df_Drug['Unit_ID'].unique()\n",
    "            sheets['AllUnits']=pd.DataFrame(AllUnits)\n",
    "\n",
    "            # Print the names of the sheets and their corresponding DataFrames\n",
    "            for List_name, listI in sheets.items():\n",
    "                \n",
    "                thelist=listI[0].tolist() if List_name=='AllUnits' else listI[1].tolist()\n",
    "                filtered_df = combined_df_Drug[combined_df_Drug['Unit_ID'].isin(thelist)]\n",
    "                List_name=List_name.replace( '\\\\', '/')\n",
    "\n",
    "                GroupList=['AllNr', 'PsNr', 'NoModNr', 'NgNr']\n",
    "\n",
    "                if NrSubtype=='L1' :#and o==0:\n",
    "                    new_folder= f\"{folder_to_save2}/{List_name}/{Osc}/\"\n",
    "                    os.makedirs(new_folder)\n",
    "\n",
    "                ###########################################################################\n",
    "                                        ##### AVERAGE PSTH #####\n",
    "                ###########################################################################\n",
    "\n",
    "                for coup in Coupling:\n",
    "                    filenameOutAVG = f'{folder_to_save2}/{List_name}/{Osc}/{NrSubtype}_{coup}AVG&SEM.xlsx'\n",
    "                    locals()[f'excel_writerAVG{coup}']= pd.ExcelWriter(filenameOutAVG)\n",
    "\n",
    "                    filenameOutMean = f'{folder_to_save2}/{List_name}/{Osc}/{NrSubtype}_{coup}During.xlsx'\n",
    "                    locals()[f'excel_writerMean{coup}']= pd.ExcelWriter(filenameOutMean)\n",
    "                \n",
    "                for data in Data:\n",
    "                    \n",
    "                    if data=='Ca':\n",
    "                        filenameOutPN = f'{folder_to_save2}/{List_name}/{Osc}/{NrSubtype}_NrProportion.xlsx'\n",
    "                        excel_writerPN= pd.ExcelWriter(filenameOutPN)\n",
    "                    \n",
    "                    NbNr=pd.DataFrame()\n",
    "\n",
    "                    for coup in Coupling:  \n",
    "\n",
    "                        BigArray=pd.DataFrame()\n",
    "                        BigArrayBSL=pd.DataFrame()\n",
    "                        AVGArray=pd.DataFrame()\n",
    "                        AVGArrayBSL=pd.DataFrame()\n",
    "                        \n",
    "                        for ctx in CTX2:\n",
    "\n",
    "                            ctx= '' if Osc=='SWR' else ctx\n",
    "                            df_per_sheet=locals()[f'{data}PSTH{Osc}_{coup}{ctx}{drug}']\n",
    "\n",
    "                            #filenameOut = f'{folder_to_save2}/{List_name}/{Osc}/{NrSubtype}_{data}PSTH_{coup}{ctx}_{drug}.pkl' #keep each responses of each cells for all rec Osc\n",
    "                            #with open(filenameOut, 'wb') as pickle_file:\n",
    "                            #    pickle.dump(df_per_sheet, pickle_file)\n",
    "\n",
    "                            if len(df_per_sheet.keys()): # not empty\n",
    "\n",
    "                                # if PSTH is full lenght\n",
    "                                arrayLen = 0\n",
    "                                for key, value in df_per_sheet.items():\n",
    "                                    if value.size > 0:\n",
    "                                        arrayLen = np.shape(value)[1]\n",
    "                                        break  \n",
    "                                # if PSTH is 2 sec lenght    \n",
    "                                arrayLen=40\n",
    "                                \n",
    "                                for group in GroupList:\n",
    "                                    locals()[f'{group}Array']=pd.DataFrame(columns=np.arange(0,arrayLen,1))\n",
    "                                \n",
    "                                for nr in df_per_sheet.keys():\n",
    "                                   \n",
    "                                    if df_per_sheet[nr].shape[0] >= 10 : # at least 10 oscillations recorded\n",
    "                                        \n",
    "                                        if Osc== 'Spdl':\n",
    "                                            PSTH=df_per_sheet[nr].iloc[:, df_per_sheet[nr].shape[1] // 10 *4  : df_per_sheet[nr].shape[1] // 10 * 6] # -1 to 1 sec for Spdl (-5 to 5s)\n",
    "                                        else:\n",
    "                                            PSTH=df_per_sheet[nr].iloc[:, df_per_sheet[nr].shape[1] // 6 *2  : df_per_sheet[nr].shape[1] // 6 * 4] # -1 to 1 sec for SWR & DS (-3 to 3s)\n",
    "\n",
    "                                        AVGtrace=np.nanmean(PSTH, axis=0)\n",
    "                                        \n",
    "                                        BaselineSTD=np.std(AVGtrace[:PSTH.shape[1]//4], axis=0)\n",
    "\n",
    "                                        upperThrs=np.nanmean(AVGtrace[:PSTH.shape[1]//4], axis=0) + BaselineSTD*2\n",
    "                                        lowerThrs=np.nanmean(AVGtrace[:PSTH.shape[1]//4], axis=0) - BaselineSTD*2\n",
    "\n",
    "                                        AVGbefore=np.nanmean(AVGtrace[PSTH.shape[1]//4:PSTH.shape[1]//4*2], axis=0)\n",
    "                                        AVGearly=np.nanmean(AVGtrace[PSTH.shape[1]//4*2:PSTH.shape[1]//4*3], axis=0)\n",
    "                                        AVGlate=np.nanmean(AVGtrace[PSTH.shape[1]//4*3:PSTH.shape[1]//4*4], axis=0)\n",
    "                                        \n",
    "                                        if AVGearly > upperThrs:\n",
    "                                            group='PsNr'\n",
    "                                        elif AVGearly < lowerThrs:\n",
    "                                            group='NgNr'\n",
    "                                        else: \n",
    "                                            group='NoModNr'\n",
    "                                        \n",
    "                                        Array=locals()[f'{group}Array']\n",
    "                                        Array.loc[nr]=AVGtrace\n",
    "                                        Array=locals()[f'AllNrArray']\n",
    "                                        Array.loc[nr]=AVGtrace\n",
    "\n",
    "                                for group in GroupList:\n",
    "\n",
    "                                    Array=locals()[f'{group}Array']\n",
    "\n",
    "                                    if not Array.isna().all().all():\n",
    "\n",
    "                                        # Only keep the neurons belonging to the list (All units, NREM active, REM active, etc)\n",
    "                                        present_indices = [idx for idx in thelist if idx in Array.index]\n",
    "                                        Array = Array.loc[present_indices] \n",
    "\n",
    "                                        n=Array.shape[0]\n",
    "                                        NbNr.loc[group,f'{ctx}{coup}']=n\n",
    "\n",
    "                                        # Leave a blanck space for units not recorded in that condition\n",
    "                                        missing_indexes = set(thelist) - set(Array.index)\n",
    "                                        Array = Array.reindex(Array.index.union(missing_indexes))\n",
    "                                        Array = Array.sort_index()\n",
    "\n",
    "                                        ArrayO=Array.copy()\n",
    "                                        mArray=Array.mean(axis=0)\n",
    "                                        semArray = stats.sem(Array, axis=0, ddof=1, nan_policy='omit')\n",
    "                                        icArray = norm.ppf((1 +  0.95) / 2) * (np.std(Array, axis=0) / np.sqrt(Array.shape[0]))\n",
    "                                        #SmallArray=pd.DataFrame(np.transpose([mArray,semArray,icArray]), columns=[f'{ctx}{coup}{Osc} Mean', f'{ctx}{coup}{Osc} SEM', f'{ctx}{coup}{Osc} IC'])\n",
    "                                        SmallArray=pd.DataFrame(np.transpose([mArray,semArray]), columns=[f'{group}{ctx}mean', f'{group}{ctx}sem'])\n",
    "                                    \n",
    "                                        BigArray=pd.concat([BigArray,SmallArray], axis=1)\n",
    "\n",
    "                                        SecondHalf_columns = Array.iloc[:, Array.shape[1] //2:]\n",
    "                                        mean_baseline = SecondHalf_columns.mean(axis=1)\n",
    "                                        avgarray=pd.DataFrame(mean_baseline, columns=[f'{group}{ctx}'])\n",
    "                                        AVGArray=pd.concat([AVGArray,avgarray], axis=1)\n",
    "                                        \n",
    "                                        # Baseline signals\n",
    "                                        baseline_columns = Array.iloc[:, :Array.shape[1] // 4]\n",
    "                                        mean_baseline = baseline_columns.mean(axis=1)\n",
    "                                        Array = Array.sub(mean_baseline, axis=0)\n",
    "                                    \n",
    "                                        #excel_writer=locals()[f'excel_writer{group}']\n",
    "                                        #Array.to_excel(excel_writer, sheet_name=f'BSL_{ctx}{coup}{Osc}', index=True, header=False)\n",
    "                                        #ArrayO.to_excel(excel_writer, sheet_name=f'{ctx}{coup}{Osc}', index=True, header=False)\n",
    "\n",
    "                                        mArray=Array.mean(axis=0)\n",
    "                                        semArray = stats.sem(Array, axis=0, ddof=1, nan_policy='omit')\n",
    "                                        icArray = norm.ppf((1 +  0.95) / 2) * (np.std(Array, axis=0) / np.sqrt(Array.shape[0]))\n",
    "                                        #SmallArray=pd.DataFrame(np.transpose([mArray,semArray,icArray]), columns=[f'{ctx}{coup}{Osc} Mean', f'{ctx}{coup}{Osc} SEM', f'{ctx}{coup}{Osc} IC'])\n",
    "                                        SmallArray=pd.DataFrame(np.transpose([mArray,semArray]), columns=[f'{group}{ctx}mean', f'{group}{ctx}sem'])\n",
    "                                        \n",
    "                                        BigArrayBSL=pd.concat([BigArrayBSL,SmallArray], axis=1)\n",
    "\n",
    "                                        SecondHalf_columns = Array.iloc[:, Array.shape[1]//2:]\n",
    "                                        mean_baseline = SecondHalf_columns.mean(axis=1)\n",
    "                                        avgarray=pd.DataFrame(mean_baseline, columns=[f'{group}{ctx}'])\n",
    "                                        AVGArrayBSL=pd.concat([AVGArrayBSL,avgarray], axis=1)\n",
    "                                        \n",
    "                        #excel_writer=locals()[f'excel_writer{coup}']\n",
    "                        excel_writerAVGn=locals()[f'excel_writerAVG{coup}']\n",
    "                        excel_writerMeann=locals()[f'excel_writerMean{coup}']\n",
    "                        #excel_writer.close()\n",
    "                        \n",
    "                        BigArrayBSL = BigArrayBSL.sort_index(axis=1)\n",
    "                        AVGArrayBSL = AVGArrayBSL.sort_index(axis=1)\n",
    "                        BigArrayBSL.to_excel(excel_writerAVGn, sheet_name=f'BSL_{data}PSTH', index=True, header=True)\n",
    "                        AVGArrayBSL.to_excel(excel_writerMeann, sheet_name=f'BSL_{data}PSTH', index=True, header=True)\n",
    "\n",
    "                        BigArray = BigArray.sort_index(axis=1)\n",
    "                        AVGArray = AVGArray.sort_index(axis=1)\n",
    "                        BigArray.to_excel(excel_writerAVGn, sheet_name=f'{data}PSTH', index=True, header=True)\n",
    "                        AVGArray.to_excel(excel_writerMeann, sheet_name=f'{data}PSTH', index=True, header=True)\n",
    "\n",
    "                    if data=='Ca':\n",
    "                        try: \n",
    "                            NbNr.loc['PsNr %'] = round(NbNr.loc['PsNr'] / NbNr.loc['AllNr'] *100)\n",
    "                            NbNr.loc['NoModNr %'] = round(NbNr.loc['NoModNr'] / NbNr.loc['AllNr'] *100)\n",
    "                            NbNr.loc['NgNr %'] = round(NbNr.loc['NgNr'] / NbNr.loc['AllNr']*100)\n",
    "                        except:                        \n",
    "                            None\n",
    "                        NbNr.to_excel(excel_writerPN, sheet_name=f'{data}', index=True, header=True)\n",
    "                        excel_writerPN.close()\n",
    "\n",
    "                for coup in Coupling:\n",
    "                    excel_writerAVGn=locals()[f'excel_writerAVG{coup}']\n",
    "                    excel_writerMeann=locals()[f'excel_writerMean{coup}']\n",
    "                    excel_writerAVGn.close()                   \n",
    "                    excel_writerMeann.close()                   \n",
    "\n",
    "        #######################\n",
    "        # Propreties Osc\n",
    "        #######################\n",
    "        filenameOut = f'{folder_to_save}/{Osc}Propreties.xlsx'\n",
    "        writer = pd.ExcelWriter(filenameOut)\n",
    "\n",
    "        combined_dfOsc = combined_df.drop_duplicates(subset=f'{Osc}_ID', keep='first')\n",
    "        #All Spdl\n",
    "        OscStatut = pd.crosstab(index=combined_dfOsc['Session_ID'],columns=[combined_dfOsc['Drug'], combined_dfOsc[f'{Osc}Statut']])\n",
    "        AllOscStatut=pd.concat([AllOscStatut, OscStatut], axis=0)\n",
    "        OscDuration = combined_dfOsc.pivot_table(index='Session_ID', columns='Drug', values=f'{Osc}Duration', aggfunc='mean')\n",
    "        AllOscDuration=pd.concat([AllOscDuration, OscDuration], axis=0)\n",
    "        if Osc== 'Spdl' or Osc== 'SWR':\n",
    "            OscCoupling = pd.crosstab(index=combined_dfOsc['Session_ID'],columns=[combined_dfOsc['Drug'], combined_dfOsc[f'SWR_inside_Spdl']])\n",
    "            AllOscCoupling=pd.concat([AllOscCoupling, OscCoupling], axis=0)\n",
    "\n",
    "        if Osc== 'Spdl':\n",
    "            GlobalSpindle = pd.crosstab(index=combined_dfOsc['Session_ID'],columns=[combined_dfOsc['Drug'], combined_dfOsc[f'GlobalSpindle']])\n",
    "            AllGlobalSpindle=pd.concat([AllGlobalSpindle, GlobalSpindle], axis=0)\n",
    "            OscStartLocation = pd.crosstab(index=combined_dfOsc['Session_ID'],columns=[combined_dfOsc['Drug'], combined_dfOsc[f'{Osc}StartLocation']])\n",
    "            AllOscStartLocation=pd.concat([AllOscStartLocation, OscStartLocation], axis=0)\n",
    "            \"\"\"\n",
    "            #S1 Spdl\n",
    "            combined_dfOscS1=combined_dfOsc[combined_dfOsc[f'{Osc}StartLocation']=='S1' and combined_dfOscS1[f'GlobalSpindle']=='Local']\n",
    "            OscStatut = pd.crosstab(index=combined_dfOscS1['Session_ID'],columns=[combined_dfOscS1['Drug'], combined_dfOscS1[f'{Osc}Statut']])\n",
    "            AllOscStatutS1=pd.concat([AllOscStatut, OscStatut], axis=0)\n",
    "            OscDuration = combined_dfOscS1.pivot_table(index='Session_ID', columns='Drug', values=f'{Osc}Duration', aggfunc='mean')\n",
    "            AllOscDurationS1=pd.concat([AllOscDuration, OscDuration], axis=0)\n",
    "            OscCoupling = pd.crosstab(index=combined_dfOscS1['Session_ID'],columns=[combined_dfOscS1['Drug'], combined_dfOscS1[f'SWR_inside_Spdl']])\n",
    "            AllOscCouplingS1=pd.concat([AllOscCoupling, OscCoupling], axis=0)\n",
    "\n",
    "            #PFC Spdl\n",
    "            combined_dfOscPFC=combined_dfOsc[combined_dfOsc[f'{Osc}StartLocation']=='PFC' and combined_dfOscS1[f'GlobalSpindle']=='Local']\n",
    "            OscStatut = pd.crosstab(index=combined_dfOscPFC['Session_ID'],columns=[combined_dfOscPFC['Drug'], combined_dfOscPFC[f'{Osc}Statut']])\n",
    "            AllOscStatutPFC=pd.concat([AllOscStatut, OscStatut], axis=0)\n",
    "            OscDuration = combined_dfOscPFC.pivot_table(index='Session_ID', columns='Drug', values=f'{Osc}Duration', aggfunc='mean')\n",
    "            AllOscDurationPFC=pd.concat([AllOscDuration, OscDuration], axis=0)\n",
    "            OscCoupling = pd.crosstab(index=combined_dfOscPFC['Session_ID'],columns=[combined_dfOscPFC['Drug'], combined_dfOscPFC[f'SWR_inside_Spdl']])\n",
    "            AllOscCouplingPFC=pd.concat([AllOscCoupling, OscCoupling], axis=0)\n",
    "\n",
    "            #S1PFC Spdl\n",
    "            combined_dfOscS1PFC=combined_dfOsc[combined_dfOsc[combined_dfOscS1[f'GlobalSpindle']=='Global']]\n",
    "            OscStatut = pd.crosstab(index=combined_dfOscS1PFC['Session_ID'],columns=[combined_dfOscS1PFC['Drug'], combined_dfOscS1PFC[f'{Osc}Statut']])\n",
    "            AllOscStatutS1PFC=pd.concat([AllOscStatut, OscStatut], axis=0)\n",
    "            OscDuration = combined_dfOscS1PFC.pivot_table(index='Session_ID', columns='Drug', values=f'{Osc}Duration', aggfunc='mean')\n",
    "            AllOscDurationS1PFC=pd.concat([AllOscDuration, OscDuration], axis=0)\n",
    "            OscCoupling = pd.crosstab(index=combined_dfOscS1PFC['Session_ID'],columns=[combined_dfOscS1PFC['Drug'], combined_dfOscS1PFC[f'SWR_inside_Spdl']])\n",
    "            AllOscCouplingS1PFC=pd.concat([AllOscCoupling, OscCoupling], axis=0)\n",
    "            \"\"\"    \n",
    "\n",
    "    AllOscStatut.to_excel(writer, sheet_name=f'CouplingStatut')\n",
    "    AllOscDuration.to_excel(writer, sheet_name=f'MeanDuration')\n",
    "    if Osc== 'Spdl' or Osc== 'SWR':\n",
    "        AllOscCoupling.to_excel(writer, sheet_name=f'SWR_inside_Spdl')    \n",
    "    if Osc== 'Spdl':\n",
    "        AllGlobalSpindle.to_excel(writer, sheet_name=f'GlobalSpindle')\n",
    "        AllOscStartLocation.to_excel(writer, sheet_name=f'StartLocation')\n",
    "        \"\"\"\n",
    "        AllOscStatutS1.to_excel(writer, sheet_name=f'S1_CouplingStatut')\n",
    "        AllOscDurationS1.to_excel(writer, sheet_name=f'S1_MeanDuration')\n",
    "        AllOscCouplingS1.to_excel(writer, sheet_name=f'S1_SWR_inside_Spdl') \n",
    "        \n",
    "        AllOscStatutPFC.to_excel(writer, sheet_name=f'PFC_CouplingStatut')\n",
    "        AllOscDurationPFC.to_excel(writer, sheet_name=f'PFC_MeanDuration')\n",
    "        AllOscCouplingPFC.to_excel(writer, sheet_name=f'PFC_SWR_inside_Spdl') \n",
    "        \n",
    "        AllOscStatutS1PFC.to_excel(writer, sheet_name=f'S1PFC_CouplingStatut')\n",
    "        AllOscDurationS1PFC.to_excel(writer, sheet_name=f'S1PFC_MeanDuration')\n",
    "        AllOscCouplingS1PFC.to_excel(writer, sheet_name=f'S1PFC_SWR_inside_Spdl')\n",
    "        \"\"\"\n",
    "\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minian311new2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
