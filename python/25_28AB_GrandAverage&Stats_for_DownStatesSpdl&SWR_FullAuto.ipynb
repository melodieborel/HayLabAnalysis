{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spdl oscillations analysis...\n",
      "... for All neurons...\n",
      "... in the PFC\n",
      "Spindles_PFC_ABdetection_CalciumAvgResultsAB_Purple.xlsx\n",
      "Spindles_PFC_ABdetection_CalciumAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "Spindles_PFC_ABdetection_GlobalResultsAB_Purple.xlsx\n",
      "Spindles_PFC_ABdetection_GlobalResultsAB_ThreeColDotsOK.xlsx\n",
      "Spindles_PFC_ABdetection_SpikeAvgResultsAB_Purple.xlsx\n",
      "Spindles_PFC_ABdetection_SpikeAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "Spindles_PFC_ABdetection_SpikeSumResultsAB_Purple.xlsx\n",
      "Spindles_PFC_ABdetection_SpikeSumResultsAB_ThreeColDotsOK.xlsx\n",
      "455 All neurons recorded\n",
      "455 All neurons in the cross-registration\n",
      "83 Spdl recorded in total in the PFC\n",
      "... in the S1\n",
      "Spindles_S1_ABdetection_CalciumAvgResultsAB_Purple.xlsx\n",
      "Spindles_S1_ABdetection_CalciumAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "Spindles_S1_ABdetection_GlobalResultsAB_Purple.xlsx\n",
      "Spindles_S1_ABdetection_GlobalResultsAB_ThreeColDotsOK.xlsx\n",
      "Spindles_S1_ABdetection_SpikeAvgResultsAB_Purple.xlsx\n",
      "Spindles_S1_ABdetection_SpikeAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "Spindles_S1_ABdetection_SpikeSumResultsAB_Purple.xlsx\n",
      "Spindles_S1_ABdetection_SpikeSumResultsAB_ThreeColDotsOK.xlsx\n",
      "429 All neurons recorded\n",
      "429 All neurons in the cross-registration\n",
      "58 Spdl recorded in total in the S1\n",
      "... for L1 neurons...\n",
      "... in the PFC\n",
      "Spindles_PFC_ABdetection_CalciumAvgResultsAB_BlackLinesOK.xlsx\n",
      "Spindles_PFC_ABdetection_CalciumAvgResultsAB_BlueLinesOK.xlsx\n",
      "Spindles_PFC_ABdetection_CalciumAvgResultsAB_GreenDotsOK.xlsx\n",
      "Spindles_PFC_ABdetection_GlobalResultsAB_BlackLinesOK.xlsx\n",
      "Spindles_PFC_ABdetection_GlobalResultsAB_BlueLinesOK.xlsx\n",
      "Spindles_PFC_ABdetection_GlobalResultsAB_GreenDotsOK.xlsx\n",
      "Spindles_PFC_ABdetection_SpikeAvgResultsAB_BlackLinesOK.xlsx\n",
      "Spindles_PFC_ABdetection_SpikeAvgResultsAB_BlueLinesOK.xlsx\n",
      "Spindles_PFC_ABdetection_SpikeAvgResultsAB_GreenDotsOK.xlsx\n",
      "Spindles_PFC_ABdetection_SpikeSumResultsAB_BlackLinesOK.xlsx\n",
      "Spindles_PFC_ABdetection_SpikeSumResultsAB_BlueLinesOK.xlsx\n",
      "Spindles_PFC_ABdetection_SpikeSumResultsAB_GreenDotsOK.xlsx\n",
      "63 L1 neurons recorded\n",
      "62 L1 neurons in the cross-registration\n",
      "91 Spdl recorded in total in the PFC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... in the S1\n",
      "Spindles_S1_ABdetection_CalciumAvgResultsAB_BlackLinesOK.xlsx\n",
      "Spindles_S1_ABdetection_CalciumAvgResultsAB_BlueLinesOK.xlsx\n",
      "Spindles_S1_ABdetection_CalciumAvgResultsAB_GreenDotsOK.xlsx\n",
      "Spindles_S1_ABdetection_GlobalResultsAB_BlackLinesOK.xlsx\n",
      "Spindles_S1_ABdetection_GlobalResultsAB_BlueLinesOK.xlsx\n",
      "Spindles_S1_ABdetection_GlobalResultsAB_GreenDotsOK.xlsx\n",
      "Spindles_S1_ABdetection_SpikeAvgResultsAB_BlackLinesOK.xlsx\n",
      "Spindles_S1_ABdetection_SpikeAvgResultsAB_BlueLinesOK.xlsx\n",
      "Spindles_S1_ABdetection_SpikeAvgResultsAB_GreenDotsOK.xlsx\n",
      "Spindles_S1_ABdetection_SpikeSumResultsAB_BlackLinesOK.xlsx\n",
      "Spindles_S1_ABdetection_SpikeSumResultsAB_BlueLinesOK.xlsx\n",
      "Spindles_S1_ABdetection_SpikeSumResultsAB_GreenDotsOK.xlsx\n",
      "63 L1 neurons recorded\n",
      "62 L1 neurons in the cross-registration\n",
      "77 Spdl recorded in total in the S1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWR oscillations analysis...\n",
      "... for All neurons...\n",
      "... in the PFC\n",
      "SWR_PFC_ABdetection_CalciumAvgResultsAB_Purple.xlsx\n",
      "SWR_PFC_ABdetection_CalciumAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "SWR_PFC_ABdetection_GlobalResultsAB_Purple.xlsx\n",
      "SWR_PFC_ABdetection_GlobalResultsAB_ThreeColDotsOK.xlsx\n",
      "SWR_PFC_ABdetection_SpikeAvgResultsAB_Purple.xlsx\n",
      "SWR_PFC_ABdetection_SpikeAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "SWR_PFC_ABdetection_SpikeSumResultsAB_Purple.xlsx\n",
      "SWR_PFC_ABdetection_SpikeSumResultsAB_ThreeColDotsOK.xlsx\n",
      "507 All neurons recorded\n",
      "507 All neurons in the cross-registration\n",
      "821 SWR recorded in total in the PFC\n",
      "... in the S1\n",
      "SWR_S1_ABdetection_CalciumAvgResultsAB_Purple.xlsx\n",
      "SWR_S1_ABdetection_CalciumAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "SWR_S1_ABdetection_GlobalResultsAB_Purple.xlsx\n",
      "SWR_S1_ABdetection_GlobalResultsAB_ThreeColDotsOK.xlsx\n",
      "SWR_S1_ABdetection_SpikeAvgResultsAB_Purple.xlsx\n",
      "SWR_S1_ABdetection_SpikeAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "SWR_S1_ABdetection_SpikeSumResultsAB_Purple.xlsx\n",
      "SWR_S1_ABdetection_SpikeSumResultsAB_ThreeColDotsOK.xlsx\n",
      "507 All neurons recorded\n",
      "507 All neurons in the cross-registration\n",
      "821 SWR recorded in total in the S1\n",
      "... for L1 neurons...\n",
      "... in the PFC\n",
      "SWR_PFC_ABdetection_CalciumAvgResultsAB_BlackLinesOK.xlsx\n",
      "SWR_PFC_ABdetection_CalciumAvgResultsAB_BlueLinesOK.xlsx\n",
      "SWR_PFC_ABdetection_CalciumAvgResultsAB_GreenDotsOK.xlsx\n",
      "SWR_PFC_ABdetection_GlobalResultsAB_BlackLinesOK.xlsx\n",
      "SWR_PFC_ABdetection_GlobalResultsAB_BlueLinesOK.xlsx\n",
      "SWR_PFC_ABdetection_GlobalResultsAB_GreenDotsOK.xlsx\n",
      "SWR_PFC_ABdetection_SpikeAvgResultsAB_BlackLinesOK.xlsx\n",
      "SWR_PFC_ABdetection_SpikeAvgResultsAB_BlueLinesOK.xlsx\n",
      "SWR_PFC_ABdetection_SpikeAvgResultsAB_GreenDotsOK.xlsx\n",
      "SWR_PFC_ABdetection_SpikeSumResultsAB_BlackLinesOK.xlsx\n",
      "SWR_PFC_ABdetection_SpikeSumResultsAB_BlueLinesOK.xlsx\n",
      "SWR_PFC_ABdetection_SpikeSumResultsAB_GreenDotsOK.xlsx\n",
      "63 L1 neurons recorded\n",
      "62 L1 neurons in the cross-registration\n",
      "653 SWR recorded in total in the PFC\n",
      "... in the S1\n",
      "SWR_S1_ABdetection_CalciumAvgResultsAB_BlackLinesOK.xlsx\n",
      "SWR_S1_ABdetection_CalciumAvgResultsAB_BlueLinesOK.xlsx\n",
      "SWR_S1_ABdetection_CalciumAvgResultsAB_GreenDotsOK.xlsx\n",
      "SWR_S1_ABdetection_GlobalResultsAB_BlackLinesOK.xlsx\n",
      "SWR_S1_ABdetection_GlobalResultsAB_BlueLinesOK.xlsx\n",
      "SWR_S1_ABdetection_GlobalResultsAB_GreenDotsOK.xlsx\n",
      "SWR_S1_ABdetection_SpikeAvgResultsAB_BlackLinesOK.xlsx\n",
      "SWR_S1_ABdetection_SpikeAvgResultsAB_BlueLinesOK.xlsx\n",
      "SWR_S1_ABdetection_SpikeAvgResultsAB_GreenDotsOK.xlsx\n",
      "SWR_S1_ABdetection_SpikeSumResultsAB_BlackLinesOK.xlsx\n",
      "SWR_S1_ABdetection_SpikeSumResultsAB_BlueLinesOK.xlsx\n",
      "SWR_S1_ABdetection_SpikeSumResultsAB_GreenDotsOK.xlsx\n",
      "63 L1 neurons recorded\n",
      "62 L1 neurons in the cross-registration\n",
      "653 SWR recorded in total in the S1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Manip2\\.conda\\envs\\minian311new2\\Lib\\site-packages\\numpy\\core\\_methods.py:195: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS oscillations analysis...\n",
      "... for All neurons...\n",
      "... in the PFC\n",
      "DS_PFC_ABdetection_CalciumAvgResultsAB_Purple.xlsx\n",
      "DS_PFC_ABdetection_CalciumAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "DS_PFC_ABdetection_GlobalResultsAB_Purple.xlsx\n",
      "DS_PFC_ABdetection_GlobalResultsAB_ThreeColDotsOK.xlsx\n",
      "DS_PFC_ABdetection_SpikeAvgResultsAB_Purple.xlsx\n",
      "DS_PFC_ABdetection_SpikeAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "DS_PFC_ABdetection_SpikeSumResultsAB_Purple.xlsx\n",
      "DS_PFC_ABdetection_SpikeSumResultsAB_ThreeColDotsOK.xlsx\n",
      "507 All neurons recorded\n",
      "507 All neurons in the cross-registration\n",
      "854 DS recorded in total in the PFC\n",
      "... in the S1\n",
      "DS_S1_ABdetection_CalciumAvgResultsAB_Purple.xlsx\n",
      "DS_S1_ABdetection_CalciumAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "DS_S1_ABdetection_GlobalResultsAB_Purple.xlsx\n",
      "DS_S1_ABdetection_GlobalResultsAB_ThreeColDotsOK.xlsx\n",
      "DS_S1_ABdetection_SpikeAvgResultsAB_Purple.xlsx\n",
      "DS_S1_ABdetection_SpikeAvgResultsAB_ThreeColDotsOK.xlsx\n",
      "DS_S1_ABdetection_SpikeSumResultsAB_Purple.xlsx\n",
      "DS_S1_ABdetection_SpikeSumResultsAB_ThreeColDotsOK.xlsx\n",
      "507 All neurons recorded\n",
      "507 All neurons in the cross-registration\n",
      "1180 DS recorded in total in the S1\n",
      "... for L1 neurons...\n",
      "... in the PFC\n",
      "DS_PFC_ABdetection_CalciumAvgResultsAB_BlackLinesOK.xlsx\n",
      "DS_PFC_ABdetection_CalciumAvgResultsAB_BlueLinesOK.xlsx\n",
      "DS_PFC_ABdetection_CalciumAvgResultsAB_GreenDotsOK.xlsx\n",
      "DS_PFC_ABdetection_GlobalResultsAB_BlackLinesOK.xlsx\n",
      "DS_PFC_ABdetection_GlobalResultsAB_BlueLinesOK.xlsx\n",
      "DS_PFC_ABdetection_GlobalResultsAB_GreenDotsOK.xlsx\n",
      "DS_PFC_ABdetection_SpikeAvgResultsAB_BlackLinesOK.xlsx\n",
      "DS_PFC_ABdetection_SpikeAvgResultsAB_BlueLinesOK.xlsx\n",
      "DS_PFC_ABdetection_SpikeAvgResultsAB_GreenDotsOK.xlsx\n",
      "DS_PFC_ABdetection_SpikeSumResultsAB_BlackLinesOK.xlsx\n",
      "DS_PFC_ABdetection_SpikeSumResultsAB_BlueLinesOK.xlsx\n",
      "DS_PFC_ABdetection_SpikeSumResultsAB_GreenDotsOK.xlsx\n",
      "63 L1 neurons recorded\n",
      "62 L1 neurons in the cross-registration\n",
      "1076 DS recorded in total in the PFC\n",
      "... in the S1\n",
      "DS_S1_ABdetection_CalciumAvgResultsAB_BlackLinesOK.xlsx\n",
      "DS_S1_ABdetection_CalciumAvgResultsAB_BlueLinesOK.xlsx\n",
      "DS_S1_ABdetection_CalciumAvgResultsAB_GreenDotsOK.xlsx\n",
      "DS_S1_ABdetection_GlobalResultsAB_BlackLinesOK.xlsx\n",
      "DS_S1_ABdetection_GlobalResultsAB_BlueLinesOK.xlsx\n",
      "DS_S1_ABdetection_GlobalResultsAB_GreenDotsOK.xlsx\n",
      "DS_S1_ABdetection_SpikeAvgResultsAB_BlackLinesOK.xlsx\n",
      "DS_S1_ABdetection_SpikeAvgResultsAB_BlueLinesOK.xlsx\n",
      "DS_S1_ABdetection_SpikeAvgResultsAB_GreenDotsOK.xlsx\n",
      "DS_S1_ABdetection_SpikeSumResultsAB_BlackLinesOK.xlsx\n",
      "DS_S1_ABdetection_SpikeSumResultsAB_BlueLinesOK.xlsx\n",
      "DS_S1_ABdetection_SpikeSumResultsAB_GreenDotsOK.xlsx\n",
      "63 L1 neurons recorded\n",
      "62 L1 neurons in the cross-registration\n",
      "1529 DS recorded in total in the S1\n"
     ]
    }
   ],
   "source": [
    "# Stats on Ca2+ imaging with miniscope and Osc\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import quantities as pq\n",
    "import numpy as np\n",
    "import math \n",
    "import neo\n",
    "import json\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button, Cursor\n",
    "import pickle\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import sem\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from scipy import stats\n",
    "from scipy.stats import t\n",
    "from scipy.stats import norm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def column_with_max_single_per_row(row):\n",
    "    max_val = row.max()  # Find the max value in the row\n",
    "    max_columns = row == max_val  # Boolean Series of columns with max value\n",
    "    \n",
    "    if max_columns.sum() > 1:\n",
    "        return 'NoPref'  # More than one column has the max value\n",
    "    else:\n",
    "        return max_columns.idxmax()  # Return the name of the column with max value\n",
    "\n",
    "########################################################################\n",
    "        # SCRIPT 27AB_GrandAverages&Stats_for_Osc\n",
    "########################################################################\n",
    "\n",
    "# Specify the directory containing the Excel files\n",
    "directory = \"//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/Baseline_recording_ABmodified/AB_Analysis/OscillationsAnalysis_PerMouse_2024_06_14_13_44_08_181080_deconvSpike/\"\n",
    "directory = \"//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/CGP/AB_Analysis/OscillationsAnalysis_PerMouse_2024_06_25_23_01_19_479394/\"\n",
    "\n",
    "# Get the current date and time\n",
    "FolderNameSave=str(datetime.now())\n",
    "FolderNameSave = FolderNameSave.replace(\" \", \"_\").replace(\".\", \"_\").replace(\":\", \"_\")\n",
    "destination_folder= f\"//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/Baseline_recording_ABmodified/AB_Analysis/GlobalOscillationsAnalysis_{FolderNameSave}/\"\n",
    "destination_folder= f\"//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/CGP/AB_Analysis/GlobalOscillationsAnalysis_{FolderNameSave}/\"\n",
    "os.makedirs(destination_folder)\n",
    "folder_to_save=Path(destination_folder)\n",
    "\n",
    "# Copy the script file to the destination folder\n",
    "source_script = \"C:/Users/Manip2/SCRIPTS/Code python audrey/code python aurelie/HayLabAnalysis/python/25_28AB_GrandAverage&Stats_for_DownStatesSpdl&SWR_FullAuto.ipynb\"\n",
    "destination_file_path = f\"{destination_folder}/25_28AB_GrandAverage&Stats_for_DownStatesSpdl&SWR_FullAuto.txt\"\n",
    "shutil.copy(source_script, destination_file_path)\n",
    "\n",
    "NrSubtypeList=['All', 'L1']\n",
    "CortexList=['PFC', 'S1']\n",
    "\n",
    "OscList=['Spdl', 'SWR', 'DS']\n",
    "OscillationList=['Spindles', 'SWR', 'DS']\n",
    "\n",
    "for o, Osc in enumerate(OscList): \n",
    "    \n",
    "    print(Osc, 'oscillations analysis...')\n",
    "\n",
    "    for NrSubtype in NrSubtypeList: \n",
    "        \n",
    "        print('... for', NrSubtype, 'neurons...')\n",
    "\n",
    "        for Cortex in CortexList:\n",
    "            \n",
    "            print('... in the', Cortex)\n",
    "\n",
    "            # Initialize an empty list to store the dataframes\n",
    "            dfs = []\n",
    "            df=[]\n",
    "            dfs2 = []\n",
    "            df2=[]\n",
    "            dfs2_per_sheet = {}\n",
    "            dfs3 = []\n",
    "            df3=[]\n",
    "            dfs3_per_sheet = {}\n",
    "            dfs4 = []\n",
    "            df4=[]\n",
    "            dfs4_per_sheet = {}\n",
    "            filtered_df=[]\n",
    "\n",
    "            if NrSubtype=='L1':\n",
    "                MiceList=['BlackLinesOK', 'BlueLinesOK', 'GreenDotsOK', 'GreenLinesOK', 'RedLinesOK']\n",
    "            else:\n",
    "                MiceList=['Purple', 'ThreeColDotsOK', 'ThreeBlueCrossesOK']\n",
    "\n",
    "            nametofind=f'{OscillationList[o]}_{Cortex}_ABdetection_GlobalResultsAB'\n",
    "            nametofind2=f'{OscillationList[o]}_{Cortex}_ABdetection_CalciumAvgResultsAB'\n",
    "            nametofind3=f'{OscillationList[o]}_{Cortex}_ABdetection_SpikeAvgResultsAB'\n",
    "            nametofind4=f'{OscillationList[o]}_{Cortex}_ABdetection_SpikeSumResultsAB'\n",
    "\n",
    "            # Recursively traverse the directory structure\n",
    "            for root, _, files in os.walk(directory):\n",
    "                for filename in files:\n",
    "                    # Check if the file is an Excel file and contains the specified name\n",
    "                    if filename.endswith('.xlsx') and nametofind in filename:\n",
    "                        if any(name in filename for name in MiceList):  \n",
    "                            # Construct the full path to the file\n",
    "                            filepath = os.path.join(root, filename)\n",
    "                            # Read the Excel file into a dataframe and append it to the list\n",
    "                            df = pd.read_excel(filepath, index_col=0)\n",
    "                            dfs.append(df)\n",
    "                            print(filename)\n",
    "                    if filename.endswith('.xlsx') and nametofind2 in filename: \n",
    "                        if any(name in filename for name in MiceList): \n",
    "                            # Construct the full path to the file\n",
    "                            filepath = os.path.join(root, filename)\n",
    "                            # Read the Excel file into a dataframe and append it to the list\n",
    "                            excel_data = pd.read_excel(filepath, sheet_name=None, index_col=0, header=None)           \n",
    "                            for sheet_name, df2 in excel_data.items():\n",
    "                                if len(df2)>0:\n",
    "                                    if sheet_name in dfs2_per_sheet:                                       \n",
    "                                        updated_matrix = pd.concat([dfs2_per_sheet[sheet_name], df2], ignore_index=False, axis=0)                    \n",
    "                                        dfs2_per_sheet[sheet_name] = updated_matrix    \n",
    "                                    else:\n",
    "                                        dfs2_per_sheet[sheet_name] = df2  #one average trace per unique unit, len(df2)==nb unit recorded for that mouse\n",
    "                            print(filename)\n",
    "                    if filename.endswith('.xlsx') and nametofind3 in filename: \n",
    "                        if any(name in filename for name in MiceList): \n",
    "                            # Construct the full path to the file\n",
    "                            filepath = os.path.join(root, filename)\n",
    "                            # Read the Excel file into a dataframe and append it to the list\n",
    "                            excel_data = pd.read_excel(filepath, sheet_name=None, index_col=0, header=None)           \n",
    "                            for sheet_name, df3 in excel_data.items():\n",
    "                                if sheet_name in dfs3_per_sheet:   \n",
    "                                    updated_matrix = pd.concat((dfs3_per_sheet[sheet_name],df3), ignore_index=False, axis=0)                \n",
    "                                    dfs3_per_sheet[sheet_name] = updated_matrix                    \n",
    "                                else:                    \n",
    "                                    dfs3_per_sheet[sheet_name] = df3 #one average trace per unique unit, len(df3)==nb unit recorded for that mouse\n",
    "                            print(filename)\n",
    "                    if filename.endswith('.xlsx') and nametofind4 in filename: \n",
    "                        if any(name in filename for name in MiceList): \n",
    "                            # Construct the full path to the file\n",
    "                            filepath = os.path.join(root, filename)\n",
    "                            # Read the Excel file into a dataframe and append it to the list\n",
    "                            excel_data = pd.read_excel(filepath, sheet_name=None, index_col=0, header=None)           \n",
    "                            for sheet_name, df4 in excel_data.items():\n",
    "                                if sheet_name in dfs4_per_sheet:   \n",
    "                                    updated_matrix = pd.concat((dfs4_per_sheet[sheet_name],df4), ignore_index=False, axis=0)                \n",
    "                                    dfs4_per_sheet[sheet_name] = updated_matrix                    \n",
    "                                else:                    \n",
    "                                    dfs4_per_sheet[sheet_name] = df4 #one average trace per unique unit, len(df4)==nb unit recorded for that mouse\n",
    "                            print(filename)\n",
    "\n",
    "\n",
    "            ###########################################################################\n",
    "                                        ##### GLOBAL #####\n",
    "            ###########################################################################\n",
    "\n",
    "            # Concatenate all dataframes into a single dataframe\n",
    "            combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "            combined_df['Unique_Unit'] = combined_df['Unique_Unit'].astype(str)\n",
    "            combined_df['UnitNumber'] = combined_df['UnitNumber'].astype(str)\n",
    "            combined_df['UnitValue'] = combined_df['UnitValue'].astype(str)\n",
    "\n",
    "            combined_df[f'{Osc}Statut'] = combined_df[f'{Osc}Statut'].astype(str)\n",
    "\n",
    "            combined_df['Unit_ID'] = combined_df['Mice'] + combined_df['Unique_Unit']\n",
    "\n",
    "            unique_count = combined_df['Unit_ID'].nunique()\n",
    "            print(unique_count, f'{NrSubtype} neurons recorded') \n",
    "\n",
    "            # Remove non defined Unique Units \n",
    "            combined_df = combined_df[combined_df['Unique_Unit'] != '[]']\n",
    "            combined_df = combined_df.dropna(subset=['Unique_Unit'])\n",
    "            unique_count = combined_df['Unit_ID'].nunique()\n",
    "            print(unique_count, f'{NrSubtype} neurons in the cross-registration') \n",
    "            \n",
    "            combined_df[f'{Osc}_ID'] = combined_df['Mice'] + combined_df['Session'] + combined_df[f'{Osc}Number'].astype(str)\n",
    "            \n",
    "            unique_count = combined_df[f'{Osc}_ID'].nunique()\n",
    "            print(unique_count, f'{Osc} recorded in total in the {Cortex}')\n",
    "\n",
    "            filenameOut = f'{folder_to_save}/{NrSubtype}_{Osc}_{Cortex}_ABdetection_GrandGlobalAB.xlsx'\n",
    "            writer = pd.ExcelWriter(filenameOut)\n",
    "            combined_df.to_excel(writer)\n",
    "            writer.close()\n",
    "\n",
    "            #####################\n",
    "            # PREFERENCE #\n",
    "            #####################\n",
    "                        \n",
    "            # Load the Excel file and read each sheet into a separate DataFrame\n",
    "            #excel_file = f'//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/Baseline_recording_ABmodified/AB_analysis/Analysis_AVG_VigStates_2024-06-14_11_47_46_676850_pref/{NrSubtype}_SignFiringPreference.xlsx'\n",
    "            excel_file = f'//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/Baseline_recording_ABmodified/AB_analysis/Analysis_AVG_VigStates_2024-06-14_15_32_24_728606_AvgPref/{NrSubtype}_AverageFiringPreference.xlsx'\n",
    "            excel_file = f'//10.69.168.1/crnldata/waking/audrey_hay/L1imaging/AnalysedMarch2023/Gaelle/CGP/AB_analysis/Analysis_AVG_VigStates_2024-06-26_09_05_46_981786_noN2/{NrSubtype}_AverageFiringPreference.xlsx'\n",
    "            sheets = pd.read_excel(excel_file, sheet_name=None, header=None, index_col=0)  # sheet_name=None reads all sheets\n",
    "\n",
    "            # Print the names of the sheets and their corresponding DataFrames\n",
    "            for List_name, listI in sheets.items():\n",
    "                \n",
    "                list=listI[1].tolist() #convert df to list\n",
    "                filtered_df = combined_df[combined_df['Unit_ID'].isin(list)]\n",
    "                List_name=List_name.replace( '\\\\', '/')\n",
    "\n",
    "                if NrSubtype=='All' and o==0 and Cortex=='PFC':\n",
    "                    new_folder= f\"{folder_to_save}/{List_name}/\"\n",
    "                    os.makedirs(new_folder)\n",
    "\n",
    "                ###########################################################################\n",
    "                                            ##### Before/During/After #####\n",
    "                ###########################################################################\n",
    "\n",
    "                filenameOut = f'{folder_to_save}/{List_name}/{NrSubtype}_{Osc}_{Cortex}_SpikeAct_BeforeDuringAfter.xlsx'          \n",
    "                excel_writer = pd.ExcelWriter(filenameOut)\n",
    "\n",
    "                SpikeActivityBefore_perUnit = filtered_df.groupby('Unit_ID')['SpikeActivityBefore'].sum()\n",
    "                SpikeActivityDuring_perUnit = filtered_df.groupby('Unit_ID')['SpikeActivityDuring'].sum()\n",
    "                SpikeActivityAfter_perUnit = filtered_df.groupby('Unit_ID')['SpikeActivityAfter'].sum()\n",
    "                \n",
    "                allSPresult = pd.concat([SpikeActivityBefore_perUnit,SpikeActivityDuring_perUnit,SpikeActivityAfter_perUnit], axis=1).rename(columns={'SpikeActivityBefore': 'Before', 'SpikeActivityDuring': 'During', 'SpikeActivityAfter': 'After'})\n",
    "                allSPresult_ActiveOnly = allSPresult.loc[~(allSPresult == 0).all(axis=1)]\n",
    "                allSPresult_ActiveOnly = allSPresult_ActiveOnly.copy()\n",
    "                allSPresult_ActiveOnly['Pref']  = allSPresult_ActiveOnly.apply(column_with_max_single_per_row, axis=1)\n",
    "                allSPresult = allSPresult.copy()\n",
    "                allSPresult['Pref']  = allSPresult.apply(column_with_max_single_per_row, axis=1)\n",
    "                CountPref=allSPresult['Pref'].value_counts()\n",
    "                CountPref_ActiveOnly=allSPresult_ActiveOnly['Pref'].value_counts()            \n",
    "                allSPresult.to_excel(excel_writer, sheet_name='Sum', index=True, header=True)\n",
    "                allSPresult_ActiveOnly.to_excel(excel_writer, sheet_name='Sum_ActiveOnly', index=True, header=True)\n",
    "\n",
    "                SpikeActivityBefore_perUnit = filtered_df.groupby('Unit_ID')['SpikeActivityBefore'].mean()\n",
    "                SpikeActivityDuring_perUnit = filtered_df.groupby('Unit_ID')['SpikeActivityDuring'].mean()\n",
    "                SpikeActivityAfter_perUnit = filtered_df.groupby('Unit_ID')['SpikeActivityAfter'].mean()\n",
    "                \n",
    "                allSPresult = pd.concat([SpikeActivityBefore_perUnit,SpikeActivityDuring_perUnit,SpikeActivityAfter_perUnit], axis=1, keys=['Before','During', 'After'])\n",
    "                allSPresult_ActiveOnly = allSPresult.loc[~(allSPresult == 0).all(axis=1)]\n",
    "                allSPresult_ActiveOnly = allSPresult_ActiveOnly.copy()\n",
    "                allSPresult_ActiveOnly['Pref']  = allSPresult_ActiveOnly.apply(column_with_max_single_per_row, axis=1)\n",
    "                allSPresult = allSPresult.copy()\n",
    "                allSPresult['Pref']  = allSPresult.apply(column_with_max_single_per_row, axis=1)\n",
    "                CountPref2=allSPresult['Pref'].value_counts()\n",
    "                CountPref_ActiveOnly2=allSPresult_ActiveOnly['Pref'].value_counts()     \n",
    "                allSPresult.to_excel(excel_writer, sheet_name='Mean', index=True, header=True)\n",
    "                allSPresult_ActiveOnly.to_excel(excel_writer, sheet_name='Mean_ActiveOnly', index=True, header=True)\n",
    "\n",
    "                CountP=pd.concat([CountPref, CountPref_ActiveOnly,CountPref2, CountPref_ActiveOnly2], axis=1, keys=['Sum', 'Sum_ActiveOnly', 'Mean', 'Mean_ActiveOnly'])\n",
    "                CountP.to_excel(excel_writer, sheet_name='Preference', index=True, header=True)\n",
    "\n",
    "                excel_writer.close()\n",
    "\n",
    "                ###########################################################################\n",
    "                                            ##### PSTH #####\n",
    "                ###########################################################################\n",
    "\n",
    "                filenameOutAVG = f'{folder_to_save}/{List_name}/{NrSubtype}_{Osc}_{Cortex}_Average&SEM.xlsx'\n",
    "                excel_writerAVG = pd.ExcelWriter(filenameOutAVG)\n",
    "\n",
    "                FileNames= ['CalciumGrandAverage', 'SpikeGrandSum', 'SpikeGrandAverage',]\n",
    "                FileNamesNormalized= ['Normalized_CalciumGrandAverage', 'Normalized_SpikeGrandSum', 'Normalized_SpikeGrandAverage',]\n",
    "                dfs_per_sheet=[dfs2_per_sheet, dfs4_per_sheet, dfs3_per_sheet]\n",
    "\n",
    "                for d, df_per_sheet in enumerate(dfs_per_sheet):\n",
    "\n",
    "                    FileName=FileNames[d]\n",
    "                    FileNameNormalized=FileNamesNormalized[d]\n",
    "                    \n",
    "                    filenameOut = f'{folder_to_save}/{List_name}/{NrSubtype}_{Osc}_{Cortex}_ABdetection_{FileName}.xlsx'\n",
    "                    excel_writer = pd.ExcelWriter(filenameOut)\n",
    "\n",
    "                    Array=[]\n",
    "                    ArrayUn=[]\n",
    "                    ArrayPre=[]\n",
    "                    ArrayPost=[]\n",
    "                    \n",
    "                    Array=pd.DataFrame(df_per_sheet[f'All_{OscillationList[o]}'])\n",
    "                    ArrayUn=pd.DataFrame(df_per_sheet[f'Uncoupled_{OscillationList[o]}'])\n",
    "                    ArrayPre=pd.DataFrame(df_per_sheet[f'Precoupled_{OscillationList[o]}'])\n",
    "                    ArrayPost=pd.DataFrame(df_per_sheet[f'Postcoupled_{OscillationList[o]}'])\n",
    "                    \n",
    "                    present_indices = [idx for idx in list if idx in Array.index]\n",
    "                    Array = Array.loc[present_indices] \n",
    "                    present_indices = [idx for idx in list if idx in ArrayUn.index]\n",
    "                    ArrayUn = ArrayUn.loc[present_indices] \n",
    "                    present_indices = [idx for idx in list if idx in ArrayPre.index]\n",
    "                    ArrayPre = ArrayPre.loc[present_indices] \n",
    "                    present_indices = [idx for idx in list if idx in ArrayPost.index]\n",
    "                    ArrayPost = ArrayPost.loc[present_indices] \n",
    "\n",
    "                    Array.to_excel(excel_writer, sheet_name=f'All_{OscillationList[o]}', index=True, header=False)\n",
    "                    ArrayUn.to_excel(excel_writer, sheet_name=f'Uncoupled_{OscillationList[o]}', index=True, header=False)\n",
    "                    ArrayPre.to_excel(excel_writer, sheet_name=f'Precoupled_{OscillationList[o]}', index=True, header=False)\n",
    "                    ArrayPost.to_excel(excel_writer, sheet_name=f'Postcoupled_{OscillationList[o]}', index=True, header=False)\n",
    "\n",
    "                    mArray=Array.mean(axis=0)\n",
    "                    semArray = stats.sem(Array, axis=0, ddof=1, nan_policy='omit')\n",
    "                    icArray = norm.ppf((1 +  0.95) / 2) * (np.std(Array, axis=0) / np.sqrt(Array.shape[0]))\n",
    "                    mArrayUn=ArrayUn.mean(axis=0)\n",
    "                    semArrayUn = stats.sem(ArrayUn, axis=0, ddof=1, nan_policy='omit')\n",
    "                    icArrayUn = norm.ppf((1 +  0.95) / 2) * (np.std(ArrayUn, axis=0) / np.sqrt(ArrayUn.shape[0]))\n",
    "                    mArrayPre=ArrayPre.mean(axis=0)\n",
    "                    semArrayPre = stats.sem(ArrayPre, axis=0, ddof=1, nan_policy='omit')\n",
    "                    icArrayPre = norm.ppf((1 +  0.95) / 2) * (np.std(ArrayPre, axis=0) / np.sqrt(ArrayPre.shape[0]))\n",
    "                    mArrayPost=ArrayPost.mean(axis=0)\n",
    "                    semArrayPost = stats.sem(ArrayPost, axis=0, ddof=1, nan_policy='omit')\n",
    "                    icArrayPost = norm.ppf((1 +  0.95) / 2) * (np.std(ArrayPost, axis=0) / np.sqrt(ArrayPost.shape[0]))\n",
    "                    BigArray=[mArray,semArray,icArray , mArrayUn, semArrayUn, icArrayUn, mArrayPre, semArrayPre, icArrayPre, mArrayPost, semArrayPost, icArrayPost]\n",
    "                    BigArray=pd.DataFrame(np.transpose(BigArray), columns=[f'All {Osc} Mean', f'All {Osc} SEM', f'All {Osc} IC', f'Uncoupled {Osc} Mean', f'Uncoupled {Osc} SEM',f'Uncoupled {Osc} IC', f'Postcoupled {Osc} Mean', f'Postcoupled {Osc} SEM',f'Postcoupled {Osc} IC', f'Precoupled {Osc} Mean', f'Precoupled {Osc} SEM',  f'Precoupled {Osc} IC'])\n",
    "\n",
    "                    if Osc=='Spdl':\n",
    "                        \n",
    "                        ArrayGlobal=[]\n",
    "                        ArrayLocal=[]\n",
    "                        ArrayGlobal=pd.DataFrame(df_per_sheet[f'Global_Spindles'])\n",
    "                        ArrayLocal=pd.DataFrame(df_per_sheet[f'Local_Spindles'])\n",
    "\n",
    "                        present_indices = [idx for idx in list if idx in ArrayGlobal.index]\n",
    "                        ArrayGlobal = ArrayGlobal.loc[present_indices] \n",
    "                        present_indices = [idx for idx in list if idx in ArrayLocal.index]\n",
    "                        ArrayLocal = ArrayLocal.loc[present_indices]\n",
    "\n",
    "                        ArrayGlobal.to_excel(excel_writer, sheet_name=f'Global_Spindles', index=True, header=False)\n",
    "                        ArrayLocal.to_excel(excel_writer, sheet_name=f'Local_Spindles', index=True, header=False)\n",
    "\n",
    "                        mArrayLocal=ArrayLocal.mean(axis=0)\n",
    "                        semArrayLocal = stats.sem(ArrayLocal, axis=0, ddof=1, nan_policy='omit')\n",
    "                        icArrayLocal = norm.ppf((1 +  0.95) / 2) * (np.std(ArrayLocal, axis=0) / np.sqrt(ArrayLocal.shape[0]))\n",
    "                        mArrayGlobal=ArrayGlobal.mean(axis=0)\n",
    "                        semArrayGlobal = stats.sem(ArrayGlobal, axis=0, ddof=1, nan_policy='omit')\n",
    "                        icArrayGlobal = norm.ppf((1 +  0.95) / 2) * (np.std(ArrayGlobal, axis=0) / np.sqrt(ArrayGlobal.shape[0]))\n",
    "                        BigArray=[mArray,semArray,icArray , mArrayUn, semArrayUn, icArrayUn, mArrayPre, semArrayPre, icArrayPre, mArrayPost, semArrayPost, icArrayPost,mArrayLocal, semArrayLocal, icArrayLocal , mArrayGlobal, semArrayGlobal, icArrayGlobal]\n",
    "                        BigArray=pd.DataFrame(np.transpose(BigArray), columns=[f'All {Osc} Mean', f'All {Osc} SEM', f'All {Osc} IC', f'Uncoupled {Osc} Mean', f'Uncoupled {Osc} SEM',f'Uncoupled {Osc} IC', f'Postcoupled {Osc} Mean', f'Postcoupled {Osc} SEM',f'Postcoupled {Osc} IC', f'Precoupled {Osc} Mean', f'Precoupled {Osc} SEM',  f'Precoupled {Osc} IC', f'Local {Osc} Mean', f'Local {Osc} SEM',f'Local {Osc} IC',f'Global {Osc} Mean', f'Global {Osc} SEM', f'Global {Osc} IC'])\n",
    "\n",
    "                    BigArray.to_excel(excel_writerAVG, sheet_name=FileName, index=True, header=True)\n",
    "                    excel_writer.close()\n",
    "\n",
    "                    # CALCIUM traces Normalization dfs2_per_sheet\n",
    "\n",
    "                    filenameOut = f'{folder_to_save}/{List_name}/{NrSubtype}_{Osc}_{Cortex}_ABdetection_{FileNameNormalized}.xlsx'\n",
    "                    excel_writer = pd.ExcelWriter(filenameOut)\n",
    "                    \n",
    "                    row_sums = Array.sum(axis=1)\n",
    "                    Array = Array.div(row_sums, axis=0)\n",
    "                    row_sums = ArrayUn.sum(axis=1)\n",
    "                    ArrayUn = ArrayUn.div(row_sums, axis=0)\n",
    "                    row_sums = ArrayPre.sum(axis=1)\n",
    "                    ArrayPre = ArrayPre.div(row_sums, axis=0)\n",
    "                    row_sums = ArrayPost.sum(axis=1)\n",
    "                    ArrayPost = ArrayPost.div(row_sums, axis=0)\n",
    "\n",
    "                    Array.to_excel(excel_writer, sheet_name=f'All_{OscillationList[o]}', index=True, header=False)\n",
    "                    ArrayUn.to_excel(excel_writer, sheet_name=f'Uncoupled_{OscillationList[o]}', index=True, header=False)\n",
    "                    ArrayPre.to_excel(excel_writer, sheet_name=f'Precoupled_{OscillationList[o]}', index=True, header=False)\n",
    "                    ArrayPost.to_excel(excel_writer, sheet_name=f'Postcoupled_{OscillationList[o]}', index=True, header=False)\n",
    "\n",
    "                    mArray=Array.mean(axis=0)\n",
    "                    semArray = stats.sem(Array, axis=0, ddof=1, nan_policy='omit')\n",
    "                    icArray = norm.ppf((1 +  0.95) / 2) * (np.std(Array, axis=0) / np.sqrt(Array.shape[0]))\n",
    "                    mArrayUn=ArrayUn.mean(axis=0)\n",
    "                    semArrayUn = stats.sem(ArrayUn, axis=0, ddof=1, nan_policy='omit')\n",
    "                    icArrayUn = norm.ppf((1 +  0.95) / 2) * (np.std(ArrayUn, axis=0) / np.sqrt(ArrayUn.shape[0]))\n",
    "                    mArrayPre=ArrayPre.mean(axis=0)\n",
    "                    semArrayPre = stats.sem(ArrayPre, axis=0, ddof=1, nan_policy='omit')\n",
    "                    icArrayPre = norm.ppf((1 +  0.95) / 2) * (np.std(ArrayPre, axis=0) / np.sqrt(ArrayPre.shape[0]))\n",
    "                    mArrayPost=ArrayPost.mean(axis=0)\n",
    "                    semArrayPost = stats.sem(ArrayPost, axis=0, ddof=1, nan_policy='omit')\n",
    "                    icArrayPost = norm.ppf((1 +  0.95) / 2) * (np.std(ArrayPost, axis=0) / np.sqrt(ArrayPost.shape[0]))\n",
    "                    BigArray=[mArray,semArray,icArray , mArrayUn, semArrayUn, icArrayUn, mArrayPre, semArrayPre, icArrayPre, mArrayPost, semArrayPost, icArrayPost]\n",
    "                    BigArray=pd.DataFrame(np.transpose(BigArray), columns=[f'All {Osc} Mean', f'All {Osc} SEM', f'All {Osc} IC', f'Uncoupled {Osc} Mean', f'Uncoupled {Osc} SEM',f'Uncoupled {Osc} IC', f'Postcoupled {Osc} Mean', f'Postcoupled {Osc} SEM',f'Postcoupled {Osc} IC', f'Precoupled {Osc} Mean', f'Precoupled {Osc} SEM',  f'Precoupled {Osc} IC'])\n",
    "\n",
    "                    if Osc=='Spdl':\n",
    "                        \n",
    "                        #ArrayGlobal=[]\n",
    "                        #ArrayLocal=[]\n",
    "                        #ArrayGlobal=pd.DataFrame(df_per_sheet[f'Global_Spindles'])\n",
    "                        #ArrayLocal=pd.DataFrame(df_per_sheet[f'Local_Spindles'])\n",
    "                        row_sums = ArrayGlobal.sum(axis=1)\n",
    "                        ArrayGlobal = ArrayGlobal.div(row_sums, axis=0)\n",
    "                        row_sums = ArrayLocal.sum(axis=1)\n",
    "                        ArrayLocal = ArrayLocal.div(row_sums, axis=0)\n",
    "\n",
    "                        ArrayGlobal.to_excel(excel_writer, sheet_name=f'Global_Spindles', index=True, header=False)\n",
    "                        ArrayLocal.to_excel(excel_writer, sheet_name=f'Local_Spindles', index=True, header=False)\n",
    "\n",
    "                        mArrayLocal=ArrayLocal.mean(axis=0)\n",
    "                        semArrayLocal = stats.sem(ArrayLocal, axis=0, ddof=1, nan_policy='omit')\n",
    "                        icArrayLocal = norm.ppf((1 +  0.95) / 2) * (np.std(ArrayLocal, axis=0) / np.sqrt(ArrayLocal.shape[0]))\n",
    "                        mArrayGlobal=ArrayGlobal.mean(axis=0)\n",
    "                        semArrayGlobal = stats.sem(ArrayGlobal, axis=0, ddof=1, nan_policy='omit')\n",
    "                        icArrayGlobal = norm.ppf((1 +  0.95) / 2) * (np.std(ArrayGlobal, axis=0) / np.sqrt(ArrayGlobal.shape[0]))\n",
    "                        BigArray=[mArray,semArray,icArray , mArrayUn, semArrayUn, icArrayUn, mArrayPre, semArrayPre, icArrayPre, mArrayPost, semArrayPost, icArrayPost,mArrayLocal, semArrayLocal, icArrayLocal , mArrayGlobal, semArrayGlobal, icArrayGlobal]\n",
    "                        BigArray=pd.DataFrame(np.transpose(BigArray), columns=[f'All {Osc} Mean', f'All {Osc} SEM', f'All {Osc} IC', f'Uncoupled {Osc} Mean', f'Uncoupled {Osc} SEM',f'Uncoupled {Osc} IC', f'Postcoupled {Osc} Mean', f'Postcoupled {Osc} SEM',f'Postcoupled {Osc} IC', f'Precoupled {Osc} Mean', f'Precoupled {Osc} SEM',  f'Precoupled {Osc} IC', f'Local {Osc} Mean', f'Local {Osc} SEM',f'Local {Osc} IC',f'Global {Osc} Mean', f'Global {Osc} SEM', f'Global {Osc} IC'])\n",
    "\n",
    "                    excel_writer.close() \n",
    "                    BigArray.to_excel(excel_writerAVG, sheet_name=FileNameNormalized, index=True, header=True)\n",
    "\n",
    "                excel_writerAVG.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minian311new2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
