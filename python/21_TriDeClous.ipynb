{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "tridesclous example with locust dataset\n",
    "\n",
    "Here a detail notebook that detail the locust dataset recodring by Christophe Pouzat.\n",
    "\n",
    "This dataset is our classic. It has be analyse yet by several tools in R, Python or C:\n",
    "\n",
    "    https://github.com/christophe-pouzat/PouzatDetorakisEuroScipy2014\n",
    "    https://github.com/christophe-pouzat/SortingABigDataSetWithPython\n",
    "    http://xtof.perso.math.cnrs.fr/locust.html\n",
    "\n",
    "So we can compare the result.\n",
    "\n",
    "The original datasets is here https://zenodo.org/record/21589\n",
    "\n",
    "But we will work on a very small subset on github https://github.com/tridesclous/tridesclous_datasets/tree/master/locust\n",
    "Overview\n",
    "\n",
    "In tridesclous, the spike sorting is done in several step:\n",
    "\n",
    "Define the datasource and working path. (class DataIO)\n",
    "* Construct a catalogue (class CatalogueConstructor) on a short chunk of data (for instance 60s) with several sub step :\n",
    "    * signal pre-processing:\n",
    "        * high pass filter (optional)\n",
    "        * removal of common reference (optional)\n",
    "        * noise estimation (median/mad) on a small chunk\n",
    "        * normalisation = robust z-score\n",
    "    * peak detection\n",
    "    * select a subset of peaks. Unecessary and impossible to extract them all.\n",
    "    * extract some waveform.\n",
    "    * project theses waveforms in smaller dimention (pca, ...)\n",
    "    * find cluster\n",
    "    * auto clean cluster with euritics merge/split/trash\n",
    "    * clean manually with GUI (class CatalogueWindow) : merge/split/trash\n",
    "    * save centroids (median+mad + first and second derivative)\n",
    "* Apply the Peeler (class Peeler) on the long term signals. With several sub steps:\n",
    "    * same signal preprocessing than before\n",
    "    * find peaks\n",
    "    * find the best cluster in catalogue for each peak\n",
    "    * find the intersample jitter\n",
    "    * remove the oversampled waveforms from the signals until there are not peaks in the signals.\n",
    "    * check with GUI (class PeelerWindow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tridesclous as tdc\n",
    "from pathlib import Path\n",
    "from tridesclous import DataIO, CatalogueConstructor, Peeler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_base = Path('/Users/ahay/Documents/DATA/DREADDthalamus/TestDCZ/2206/20230227/Baseline')\n",
    "filename = folder_base / f'Tetrodes.npy'\n",
    "sigs = np.load(filename, mmap_mode= 'r')\n",
    "with open('/Users/ahay/Documents/DATA/DREADDthalamus/TestDCZ/2206/20230227/Baseline/'+'.Tetrodes.npy', mode='wb') as f:\n",
    "    f.write(sigs.tobytes())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a small dataset\n",
    "\n",
    "trideclous provide some datasets than can be downloaded with download_dataset.\n",
    "\n",
    "Note this dataset contains 2 trials in 2 different files. (the original contains more!)\n",
    "\n",
    "Each file is considers as a segment. tridesclous automatically deal with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download dataset\n",
    "localdir, filenames, params = tdc.download_dataset(name='locust')\n",
    "print(filenames)\n",
    "print(params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataIO = define datasource and working dir\n",
    "\n",
    "Theses 2 files are in RawData format this means binary format with interleaved channels.\n",
    "\n",
    "Our dataset contains 2 segment of 28.8 second each, 4 channels. The sample rate is 15kHz.\n",
    "\n",
    "Note that there is only one channel_group here (0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a DataIO\n",
    "import os, shutil\n",
    "dirname = 'tridesclous_locust'\n",
    "if os.path.exists(dirname):\n",
    "    #remove is already exists\n",
    "    shutil.rmtree(dirname)    \n",
    "dataio = DataIO(dirname=dirname)\n",
    "\n",
    "# feed DataIO\n",
    "dataio.set_data_source(type='RawData', filenames=filenames, **params)\n",
    "print(dataio)\n",
    "\n",
    "#no need to setup the prb with dataio.set_probe_file() or dataio.download_probe()\n",
    "#because it is a tetrode\n",
    "\n",
    "#CatalogueConstructor\n",
    "\n",
    "cc = CatalogueConstructor(dataio=dataio)\n",
    "print(cc)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatalogueConstructor\n",
    "  chan_grp 0 - ch0 ch1 ch2 ch3\n",
    "  Signal pre-processing not done yet\n",
    "\n",
    "Set some parameters\n",
    "\n",
    "For a complet description of each params see main documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global params\n",
    "cc.set_global_params(chunksize=1024,mode='dense')\n",
    "\n",
    "# pre processing filetring normalisation\n",
    "cc.set_preprocessor_params(\n",
    "            common_ref_removal=False,\n",
    "            highpass_freq=300.,\n",
    "            lowpass_freq=5000.,                                             \n",
    "            lostfront_chunksize=64)\n",
    "\n",
    "cc.set_peak_detector_params(\n",
    "            peak_sign='-',\n",
    "            relative_threshold=6.5,\n",
    "            peak_span_ms=0.1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the median and mad of noiseon a small chunk of filtered signals.\n",
    "\n",
    "This compute medians and mad of each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.estimate_signals_noise(seg_num=0, duration=15.)\n",
    "print(cc.signals_medians)\n",
    "print(cc.signals_mads)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the main loop: signal preprocessing + peak detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.perf_counter()\n",
    "cc.run_signalprocessor(duration=60.)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print('run_signalprocessor', t2-t1, 's')\n",
    "print(cc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean peaks\n",
    "\n",
    "Whis try to detect \"bad peaks\". They are artifact with very big amplitude value. This peaks have to removed early and not be include in waveform extaction and pca.\n",
    "\n",
    "Strange peak are tag with -9 (alien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.clean_peaks(alien_value_threshold=60., mode='extremum_amplitude')\n",
    "print(cc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample some peaks for waveforms extraction\n",
    "\n",
    "Take some waveforms in the signals n_left/n_right must be choosen carfully.\n",
    "\n",
    "It is not necessary to intensive to select all peaks.\n",
    "\n",
    "There are several method to select peaks the most simple is to select randomly.\n",
    "\n",
    "Note that waveform are extracted now. It is too intensive. They are extacted on-the-fly when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.set_waveform_extractor_params(wf_left_ms=-1.5, wf_right_ms=2.5)\n",
    "\n",
    "cc.sample_some_peaks(mode='rand', nb_max=20000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extact some noise snippet.\n",
    "\n",
    "Here a step to extact snippet of noise (in between real peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.extract_some_noise(nb_snippet=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project to smaller space\n",
    "\n",
    "To reduce dimension of the waveforms (n_peaks, peak_width, n_channel) we chosse global_pca method which is appropriate for tetrode.\n",
    "\n",
    "It consists of flatenning some_waveforms.shape (n_peaks, peak_width, n_channel) to (n_peaks, peak_width*n_channel) and then apply a standard PCA on it with sklearn.\n",
    "\n",
    "Let's keep 5 component of it.\n",
    "\n",
    "In case of more channel we could also do a 'by_channel_pca'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.extract_some_features(method='global_pca', n_components=5)\n",
    "print(cc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find clusters\n",
    "\n",
    "There are many option to cluster this features. here a simple one the well known kmeans method.\n",
    "\n",
    "Unfortunatly we need to choose the number of cluster. Too bad... Let's take 12.\n",
    "\n",
    "Later on we will be able to refine this manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.find_clusters(method='kmeans', n_clusters=12)\n",
    "print(cc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual clean with CatalogueWindow (or visual check)\n",
    "\n",
    "This open a CatalogueWindow, here we can check, split merge, trash, play as long as we are not happy.\n",
    "\n",
    "If we are happy, we can save the catalogue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt5\n",
    "import pyqtgraph as pg\n",
    "app = pg.mkQApp()\n",
    "win = tdc.CatalogueWindow(catalogueconstructor)\n",
    "win.show()\n",
    "app.exec_()    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a snappshot of CatalogueWindow\n",
    "Auto clean of catatalogue\n",
    "\n",
    "tridesclous offer some method for auto merge/trash/split some cluster.\n",
    "\n",
    "After this we can re order cluster and construct the catalogue for the peeler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.auto_split_cluster()\n",
    "    \n",
    "cc.auto_merge_cluster()\n",
    "    \n",
    "cc.trash_low_extremum(min_extremum_amplitude=6.6)\n",
    "\n",
    "cc.trash_small_cluster(minimum_size=10)\n",
    "\n",
    "#order cluster by waveforms rms\n",
    "cc.order_clusters(by='waveforms_rms')\n",
    "\n",
    "#save the catalogue\n",
    "cc.make_catalogue_for_peeler(inter_sample_oversampling=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peeler\n",
    "\n",
    "Create and run the Peeler. It should be pretty fast, here the computation take 1.32s for 28.8x2s of signal. This is a speed up of 43 over real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue = dataio.load_catalogue(chan_grp=0)\n",
    "\n",
    "peeler = Peeler(dataio)\n",
    "peeler.change_params(catalogue=catalogue)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "peeler.run()\n",
    "t2 = time.perf_counter()\n",
    "print('peeler.run', t2-t1)\n",
    "\n",
    "print()\n",
    "for seg_num in range(dataio.nb_segment):\n",
    "    spikes = dataio.get_spikes(seg_num)\n",
    "    print('seg_num', seg_num, 'nb_spikes', spikes.size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open PeelerWindow for visual checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt5\n",
    "import pyqtgraph as pg\n",
    "app = pg.mkQApp()\n",
    "win = tdc.PeelerWindow(dataio=dataio, catalogue=initial_catalogue)\n",
    "win.show()\n",
    "app.exec_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TriDeClous",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d7858efc83fec26f38bcd99847b6e5b7c4675aaab3d472f84cef4e4f75eba48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
