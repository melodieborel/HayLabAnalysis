{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e08123b",
   "metadata": {},
   "source": [
    "# Associate Ca2+ signal with position on cheeseboard for each session using crossregistration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea39cc",
   "metadata": {},
   "source": [
    "Define Experiment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8de0d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_expe_types =['Cheeseboard']\n",
    "\n",
    "dir = \"//10.69.168.1/crnldata/forgetting/Aurelie/MiniscopeOE_data/L2_3_mice/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"C:/Users/Manip2/SCRIPTS/minian/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4760e991",
   "metadata": {},
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd387518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import quantities as pq\n",
    "import math \n",
    "import neo\n",
    "import json\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button, Cursor\n",
    "import pickle\n",
    "import sys \n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from ast import literal_eval\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import zscore\n",
    "from scipy import stats\n",
    "from itertools import groupby\n",
    "from IPython.display import display\n",
    "from scipy.interpolate import griddata\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import numpy.matlib\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import resample\n",
    "from scipy.signal import resample_poly\n",
    "from math import gcd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "from scipy.interpolate import interp1d\n",
    "from collections import defaultdict\n",
    "import bisect\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import random\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "    def write(self, obj):\n",
    "        for f in self.files:\n",
    "            f.write(obj)\n",
    "            f.flush()\n",
    "    def flush(self):\n",
    "        for f in self.files:\n",
    "            f.flush()\n",
    "\n",
    "\n",
    "minian_path = os.path.join(os.path.abspath('.'),'minian')\n",
    "print(\"The folder used for minian procedures is : {}\".format(minian_path))\n",
    "sys.path.append(minian_path)\n",
    "\n",
    "from minian.utilities import (\n",
    "    TaskAnnotation,\n",
    "    get_optimal_chk,\n",
    "    load_videos,\n",
    "    open_minian,\n",
    "    save_minian,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573175c",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaeb391",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_to_cm = 2.25  \n",
    "table_center_x, table_center_y = 313, 283  # Center of the cheeseboard table on the video\n",
    "table_center_x, table_center_y = 300, 270  # Center of the cheeseboard table on the video\n",
    "table_radius = 290 / 2\n",
    "square_size = pixel_to_cm * 6\n",
    "\n",
    "bin_size = 10\n",
    "bin_edges = np.arange(0, 361, bin_size)\n",
    "bin_centers = np.radians(bin_edges[:-1] + bin_size/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b0815",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_avg_filter(data):\n",
    "    data = np.array(data, dtype=float)  # Ensure NumPy array with float type\n",
    "    filtered_data = np.copy(data)  # Copy to avoid modifying original data\n",
    "    for i in range(len(data)):\n",
    "        if not np.isnan(data[i]):  # Skip valid values\n",
    "            continue\n",
    "        # Find the closest previous non-NaN value\n",
    "        prev_idx = i - 1\n",
    "        while prev_idx >= 0 and np.isnan(data[prev_idx]):\n",
    "            prev_idx -= 1        \n",
    "        # Find the closest next non-NaN value\n",
    "        next_idx = i + 1\n",
    "        while next_idx < len(data) and np.isnan(data[next_idx]):\n",
    "            next_idx += 1\n",
    "        # Compute average if both values exist\n",
    "        if prev_idx >= 0 and next_idx < len(data):\n",
    "            filtered_data[i] = (data[prev_idx] + data[next_idx]) / 2\n",
    "        # If neither exists, NaN remains\n",
    "    return filtered_data\n",
    "\n",
    "def find_closest_index_sorted(arr, target):\n",
    "    idx = bisect.bisect_left(arr, target)  # Find the insertion point\n",
    "    if idx == 0:\n",
    "        return 0\n",
    "    if idx == len(arr):\n",
    "        return len(arr) - 1\n",
    "    before = idx - 1\n",
    "    after = idx\n",
    "    return before if abs(arr[before] - target) <= abs(arr[after] - target) else after\n",
    "\n",
    "# Sample callback function\n",
    "def update_my_folder(chooser):\n",
    "    global dpath\n",
    "    dpath = chooser.selected\n",
    "    %store dpath\n",
    "    return \n",
    "\n",
    "def detect_longest_lowest_sequence(arr, margin=0):\n",
    "    min_val = np.nanmin(arr)  # Find minimum value\n",
    "    threshold = min_val + margin  # Define threshold based on margin    \n",
    "    # Get indices where values are within the threshold\n",
    "    min_indices = np.where(arr <= threshold)[0]\n",
    "    # Identify consecutive sequences\n",
    "    longest_sequence = None\n",
    "    if len(min_indices) > 0:\n",
    "        start = min_indices[0]\n",
    "        max_duration = 0  # Track longest duration        \n",
    "        for i in range(1, len(min_indices)):\n",
    "            if min_indices[i] != min_indices[i - 1] + 1:  # Not consecutive\n",
    "                duration = min_indices[i - 1] - start + 1\n",
    "                if duration > max_duration:\n",
    "                    max_duration = duration\n",
    "                    longest_sequence = (start, min_indices[i - 1], duration)\n",
    "                start = min_indices[i]  # Reset start index        \n",
    "        # Check last detected sequence\n",
    "        duration = min_indices[-1] - start + 1\n",
    "        if duration > max_duration:\n",
    "            longest_sequence = (start, min_indices[-1], duration)\n",
    "    return min_val, threshold, longest_sequence\n",
    "\n",
    "\n",
    "def resample_matrix(data, orig_rate, target_rate, axis=0):\n",
    "    if orig_rate == target_rate:\n",
    "        return data.copy()\n",
    "    # Compute integer up/down factors using GCD\n",
    "    up = int(target_rate)\n",
    "    down = int(orig_rate)\n",
    "    factor = gcd(up, down)\n",
    "    up //= factor\n",
    "    down //= factor\n",
    "    return resample_poly(data, up=up, down=down, axis=axis)\n",
    "\n",
    "def calculate_relative_distance(x1, y1, x2, y2):\n",
    "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "def calculate_distance_run(x_coords, y_coords):\n",
    "    distances = np.sqrt(np.diff(x_coords) ** 2 + np.diff(y_coords) ** 2)\n",
    "    for i in range(1, len(distances) - 1):\n",
    "        if np.isnan(distances[i]):\n",
    "            neighbors = [distances[i-1], distances[i+1]]\n",
    "            distances[i] = np.mean([x for x in neighbors if not np.isnan(x)])\n",
    "    total_distance_cm = np.nansum(distances) / pixel_to_cm  # Convert to cm\n",
    "    return total_distance_cm, distances\n",
    "\n",
    "def find_long_non_nan_sequences(arr, min_length=100):\n",
    "    mask = ~np.isnan(arr)  # True for non-NaN values\n",
    "    diff = np.diff(np.concatenate(([0], mask.astype(int), [0])))  # Add padding to detect edges\n",
    "    starts = np.where(diff == 1)[0]  # Where a sequence starts\n",
    "    ends = np.where(diff == -1)[0]   # Where a sequence ends\n",
    "    sequences = [arr[start:end] for start, end in zip(starts, ends) if (end - start) > min_length]\n",
    "    return sequences\n",
    "\n",
    "def remove_outliers_median_filter(data, window=1):\n",
    "    data = np.array(data, dtype=float)  # Ensure NumPy array with float type\n",
    "    filtered_data = np.copy(data)  # Copy to avoid modifying original data\n",
    "    half_window = window // 2\n",
    "    for i in range(len(data)):\n",
    "        # Define window range, ensuring it doesn't exceed bounds\n",
    "        start = max(0, i - half_window)\n",
    "        end = min(len(data), i + half_window + 1)\n",
    "        # Extract local values in window\n",
    "        local_values = data[start:end]\n",
    "        # Check if the window contains at least one non-NaN value\n",
    "        if np.all(np.isnan(local_values)):\n",
    "            median_value = np.nan  # Keep NaN if no valid numbers\n",
    "        else:\n",
    "            median_value = np.nanmedian(local_values)  # Compute median ignoring NaNs\n",
    "        # Replace only if the current value is not NaN\n",
    "        if not np.isnan(data[i]):\n",
    "            filtered_data[i] = median_value\n",
    "    return filtered_data\n",
    "\n",
    "def replace_high_speed_points_with_nan(x, y, speed_threshold):\n",
    "    x = np.array(x, dtype='float')\n",
    "    y = np.array(y, dtype='float')\n",
    "    # Compute speed between consecutive points\n",
    "    dx = np.diff(x)\n",
    "    dy = np.diff(y)\n",
    "    speeds = np.sqrt(dx**2 + dy**2)\n",
    "    # Create mask for speed exceeding threshold\n",
    "    high_speed_mask = speeds > speed_threshold\n",
    "    # We mark i+1 as NaN if speed between them is too high\n",
    "    x_out = x.copy()\n",
    "    y_out = y.copy()\n",
    "    for i in range(len(high_speed_mask)):\n",
    "        if high_speed_mask[i]:\n",
    "            # Only mark the faster of the two points\n",
    "            if i > 0 and i < len(x) - 1:\n",
    "                if speeds[i] > speeds[i - 1]:\n",
    "                    x_out[i + 1] = np.nan\n",
    "                    y_out[i + 1] = np.nan\n",
    "                else:\n",
    "                    x_out[i] = np.nan\n",
    "                    y_out[i] = np.nan\n",
    "    return x_out, y_out\n",
    "\n",
    "def interpolate_2d_path(x, y, kind='linear', fill='extrapolate'):\n",
    "    x = np.array(x, dtype='float')\n",
    "    y = np.array(y, dtype='float')\n",
    "    indices = np.arange(len(x))\n",
    "    valid_mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "    if np.sum(valid_mask) < 2:\n",
    "        raise ValueError(\"Not enough valid points to interpolate/extrapolate.\")\n",
    "    interp_x = interp1d(indices[valid_mask], x[valid_mask], kind=kind, fill_value=fill, bounds_error=False)\n",
    "    interp_y = interp1d(indices[valid_mask], y[valid_mask], kind=kind, fill_value=fill, bounds_error=False)\n",
    "    x_filled = x.copy()\n",
    "    y_filled = y.copy()\n",
    "    nan_mask = np.isnan(x) | np.isnan(y)\n",
    "    x_filled[nan_mask] = interp_x(indices[nan_mask])\n",
    "    y_filled[nan_mask] = interp_y(indices[nan_mask])\n",
    "    return x_filled, y_filled\n",
    "\n",
    "def limit_speed(x, y, max_speed):\n",
    "    dx = np.diff(x.copy())\n",
    "    dy = np.diff(y.copy())\n",
    "    speeds = np.sqrt(dx**2 + dy**2)\n",
    "    for i,t in enumerate(speeds):\n",
    "        if t > max_speed:        \n",
    "            x[i+1] = x[i] \n",
    "            y[i+1] = y[i] \n",
    "            x[i+2] = x[i] \n",
    "            y[i+2] = y[i] \n",
    "    return x, y\n",
    "\n",
    "def remove_short_sequences(arr, max_len=10):\n",
    "    arr = np.array(arr, dtype='float')\n",
    "    result = arr.copy()\n",
    "    is_value = ~np.isnan(arr)\n",
    "    i = 0\n",
    "    while i < len(arr):\n",
    "        if is_value[i]:\n",
    "            start = i\n",
    "            while i < len(arr) and is_value[i]:\n",
    "                i += 1\n",
    "            end = i\n",
    "            seq_len = end - start\n",
    "            # Check if surrounded by NaNs and short enough\n",
    "            if seq_len <= max_len:\n",
    "                left_nan = (start == 0) or np.isnan(arr[start - 1])\n",
    "                right_nan = (end == len(arr)) or np.isnan(arr[end])  # safe for edge\n",
    "                if left_nan and right_nan:\n",
    "                    result[start:end] = np.nan\n",
    "        else:\n",
    "            i += 1\n",
    "    return result\n",
    "\n",
    "def Convert(string):\n",
    "            li = list(string.split(\", \"))\n",
    "            li2 = len(li)\n",
    "            return li2\n",
    "\n",
    "def marcenkopastur(significance):\n",
    "    nbins = significance.nbins\n",
    "    nneurons = significance.nneurons\n",
    "    tracywidom = significance.tracywidom\n",
    "    q = float(nbins)/float(nneurons)\n",
    "    lambdaMax = pow((1+np.sqrt(1/q)),2)\n",
    "    lambdaMax += tracywidom*pow(nneurons,-2./3)\n",
    "    return lambdaMax\n",
    "\n",
    "def getlambdacontrol(zactmat_):\n",
    "    significance_ = PCA()\n",
    "    significance_.fit(zactmat_.T)\n",
    "    lambdamax_ = np.max(significance_.explained_variance_)\n",
    "    return lambdamax_\n",
    "\n",
    "def binshuffling(zactmat,significance):\n",
    "    np.random.seed()\n",
    "    lambdamax_ = np.zeros(significance.nshu)\n",
    "    for shui in range(significance.nshu):\n",
    "        zactmat_ = np.copy(zactmat)\n",
    "        for (neuroni,activity) in enumerate(zactmat_):\n",
    "            randomorder = np.argsort(np.random.rand(significance.nbins))\n",
    "            zactmat_[neuroni,:] = activity[randomorder]\n",
    "        lambdamax_[shui] = getlambdacontrol(zactmat_)\n",
    "    lambdaMax = np.percentile(lambdamax_,significance.percentile)\n",
    "    return lambdaMax\n",
    "\n",
    "def circshuffling(zactmat,significance):\n",
    "    np.random.seed()\n",
    "    lambdamax_ = np.zeros(significance.nshu)\n",
    "    for shui in range(significance.nshu):\n",
    "        zactmat_ = np.copy(zactmat)\n",
    "        for (neuroni,activity) in enumerate(zactmat_):\n",
    "            cut = int(np.random.randint(significance.nbins*2))\n",
    "            zactmat_[neuroni,:] = np.roll(activity,cut)\n",
    "        lambdamax_[shui] = getlambdacontrol(zactmat_)\n",
    "    lambdaMax = np.percentile(lambdamax_,significance.percentile)\n",
    "    return lambdaMax\n",
    "\n",
    "def runSignificance(zactmat,significance):\n",
    "    if significance.nullhyp == 'mp':\n",
    "        lambdaMax = marcenkopastur(significance)\n",
    "    elif significance.nullhyp == 'bin':\n",
    "        lambdaMax = binshuffling(zactmat,significance)\n",
    "    elif significance.nullhyp == 'circ':\n",
    "        lambdaMax = circshuffling(zactmat,significance)\n",
    "    else:\n",
    "        print('ERROR !')\n",
    "        print('    nyll hypothesis method '+str(nullhyp)+' not understood')\n",
    "        significance.nassemblies = np.nan\n",
    "    nassemblies = np.sum(significance.explained_variance_>lambdaMax)\n",
    "    significance.nassemblies = nassemblies\n",
    "    return significance\n",
    "\n",
    "def extractPatterns(actmat,significance,method):\n",
    "    nassemblies = significance.nassemblies\n",
    "    if method == 'pca':\n",
    "        idxs = np.argsort(-significance.explained_variance_)[0:nassemblies]\n",
    "        patterns = significance.components_[idxs,:]\n",
    "    elif method == 'ica':\n",
    "        from sklearn.decomposition import FastICA\n",
    "        ica = FastICA(n_components=nassemblies, max_iter=1000)\n",
    "        ica.fit(actmat.T)\n",
    "        patterns = ica.components_\n",
    "    else:\n",
    "        print('ERROR !')\n",
    "        print('    assembly extraction method '+str(method)+' not understood')\n",
    "        patterns = np.nan\n",
    "    if patterns is not np.nan:\n",
    "        patterns = patterns.reshape(nassemblies,-1)\n",
    "        norms = np.linalg.norm(patterns,axis=1)\n",
    "        patterns /= np.matlib.repmat(norms,np.size(patterns,1),1).T\n",
    "    return patterns\n",
    "\n",
    "def runPatterns(actmat, method='ica', nullhyp = 'mp', nshu = 1000, percentile = 99, tracywidom = False):\n",
    "    nneurons = np.size(actmat,0)\n",
    "    nbins = np.size(actmat,1)\n",
    "    silentneurons = np.var(actmat,axis=1)==0\n",
    "    actmat_ = actmat[~silentneurons,:]\n",
    "    zactmat_ = stats.zscore(actmat_,axis=1)\n",
    "    significance = PCA()\n",
    "    significance.fit(zactmat_.T)\n",
    "    significance.nneurons = nneurons\n",
    "    significance.nbins = nbins\n",
    "    significance.nshu = nshu\n",
    "    significance.percentile = percentile\n",
    "    significance.tracywidom = tracywidom\n",
    "    significance.nullhyp = nullhyp\n",
    "    significance = runSignificance(zactmat_,significance)\n",
    "    if np.isnan(significance.nassemblies):\n",
    "        patterns = []\n",
    "        zactmat = []\n",
    "        significance = []\n",
    "        #return\n",
    "    if significance.nassemblies<1:\n",
    "        print('WARNING 1!')\n",
    "        print('    no assembly detecded!')\n",
    "        patterns = []\n",
    "        zactmat = []\n",
    "        significance = []\n",
    "    else:\n",
    "        patterns_ = extractPatterns(zactmat_,significance,method)\n",
    "        if patterns_ is np.nan:\n",
    "            patterns = []\n",
    "            zactmat = []\n",
    "            significance = []\n",
    "            #return\n",
    "        patterns = np.zeros((np.size(patterns_,0),nneurons))\n",
    "        patterns[:,~silentneurons] = patterns_\n",
    "        zactmat = np.copy(actmat)\n",
    "        zactmat[~silentneurons,:] = zactmat_\n",
    "    return patterns,significance,zactmat\n",
    "\n",
    "def computeAssemblyActivity(patterns,zactmat,zerodiag = True):\n",
    "    if len(patterns) == 0:\n",
    "        print('WARNING 2!')\n",
    "        print('    no assembly detecded!')\n",
    "        assemblyAct = []\n",
    "    else:\n",
    "        nassemblies = len(patterns)\n",
    "        nbins = np.size(zactmat,1)\n",
    "        assemblyAct = np.zeros((nassemblies,nbins))\n",
    "        for (assemblyi,pattern) in enumerate(patterns):\n",
    "            projMat = np.outer(pattern,pattern)\n",
    "            projMat -= zerodiag*np.diag(np.diag(projMat))\n",
    "            for bini in range(nbins):\n",
    "                assemblyAct[assemblyi,bini] = np.dot(np.dot(zactmat[:,bini],projMat),zactmat[:,bini])\n",
    "    return assemblyAct\n",
    "\n",
    "# Conversion des quaternions en angles d'Euler\n",
    "def quaternion_to_euler(qw, qx, qy, qz):\n",
    "    sinr_cosp = 2 * (qw * qx + qy * qz)\n",
    "    cosr_cosp = 1 - 2 * (qx * qx + qy * qy)\n",
    "    roll = np.arctan2(sinr_cosp, cosr_cosp)\n",
    "\n",
    "    sinp = 2 * (qw * qy - qz * qx)\n",
    "    sinp = np.clip(sinp, -1.0, 1.0)\n",
    "    pitch = np.arcsin(sinp)\n",
    "\n",
    "    siny_cosp = 2 * (qw * qz + qx * qy)\n",
    "    cosy_cosp = 1 - 2 * (qy * qy + qz * qz)\n",
    "    yaw = np.arctan2(siny_cosp, cosy_cosp)\n",
    "\n",
    "    return roll, pitch, yaw\n",
    "\n",
    "def direction(x1, y1, x2, y2):\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    if dx == 0 and dy == 0:\n",
    "        return np.nan  # undefined direction\n",
    "    angle_rad = math.atan2(dy, dx)\n",
    "    angle_deg = math.degrees(angle_rad)\n",
    "    return (angle_deg + 360) % 360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f0953",
   "metadata": {},
   "source": [
    "Load sleep score and Ca2+ time series numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68634fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and time\n",
    "FolderNameSave=str(datetime.now())[:19]\n",
    "FolderNameSave = FolderNameSave.replace(\" \", \"_\").replace(\".\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "#destination_folder= f\"//10.69.168.1/crnldata/forgetting/Aurelie/MiniscopeOE_analysis/PlaceCells_experiment/Cheeseboard_{FolderNameSave}{AnalysisID}\" \n",
    "#os.makedirs(destination_folder)\n",
    "#folder_to_save=Path(destination_folder)\n",
    "\n",
    "AllCellDict={}\n",
    "AllTimeDict={}\n",
    "AllCellDictSess={}\n",
    "AllTimeDictSess={}\n",
    "\n",
    "AllCellDict_HD={}\n",
    "AllTimeDict_HD={}\n",
    "AllCellDictSess_HD={}\n",
    "AllTimeDictSess_HD={}\n",
    "\n",
    "for dpath in Path(dir).glob('**/PlaceCells_experiment/mappingsAB.pkl'):\n",
    "\n",
    "    mappfile = open(dpath.parents[0]/ f'mappingsAB.pkl', 'rb')\n",
    "    mapping = pickle.load(mappfile)\n",
    "    mapping_sess = mapping['session']   \n",
    "        \n",
    "    centfile = open(dpath.parents[0]/ f'centsAB.pkl', 'rb')\n",
    "    centroids = pickle.load(centfile) \n",
    "\n",
    "    mice = dpath.parents[1].parts[-1]\n",
    "    NeuronType = dpath.parents[2].parts[-1]\n",
    "    \n",
    "    print(f\"Processing mouse {mice} of type {NeuronType}\")\n",
    "\n",
    "    sess=0\n",
    "\n",
    "    minian_folders = [f for f in dpath.parents[0].rglob('minian') if f.is_dir()]\n",
    "\n",
    "    for minianpath in minian_folders: # for each minian folders found in this mouse \n",
    "\n",
    "        if any(p in all_expe_types for p in minianpath.parts): # have to be to the expe_types\n",
    "\n",
    "            try: \n",
    "                session_path=minianpath.parents[1]        \n",
    "                tsmini= pd.read_csv(list(session_path.glob('*V4_Miniscope/timeStamps.csv'))[0])['Time Stamp (ms)']\n",
    "                V4subfolder=False\n",
    "                session_type=minianpath.parents[2].name\n",
    "                session_date=minianpath.parents[3].name\n",
    "                session_time=minianpath.parents[1].name\n",
    "                session=session_time\n",
    "         \n",
    "            except:\n",
    "                session_path=minianpath.parents[2]      \n",
    "                tsmini= pd.read_csv(list(session_path.glob('*V4_Miniscope/timeStamps.csv'))[0])['Time Stamp (ms)']                      \n",
    "                V4subfolder=True\n",
    "                session_type=minianpath.parents[3].name\n",
    "                session_date=minianpath.parents[4].name\n",
    "                session_time=minianpath.parents[0].name\n",
    "                session=session_time\n",
    "            print(f\"Processing {session_type} session: {session} on the {session_date} \")\n",
    "\n",
    "            # Minian data \n",
    "                        \n",
    "            minian_ds = open_minian(minianpath)\n",
    "            Co = minian_ds['C']  # calcium traces\n",
    "            minian_freq=round(1/np.mean(np.diff(np.array(tsmini)/1000)))\n",
    "            print('... miniscope sample rate =', minian_freq, 'Hz')            \n",
    "            try: \n",
    "                TodropFile = minianpath / f'TodropFileAB.json'\n",
    "                with open(TodropFile, 'r') as f:\n",
    "                    unit_to_drop = json.load(f)\n",
    "            except:\n",
    "                TodropFile = minianpath.parent / f'TodropFileAB.json'\n",
    "                with open(TodropFile, 'r') as f:\n",
    "                    unit_to_drop = json.load(f)\n",
    "\n",
    "            if V4subfolder:\n",
    "                V4subfolder_id = int(minianpath.parent.name[-1]) - 1 \n",
    "                ts_start = V4subfolder_id*15*1000 # cause 15 videos per subfolders and 1000 frames per videos\n",
    "                ts_stop = np.shape(Co)[1] + ts_start\n",
    "                tsmini_sub=tsmini[ts_start:ts_stop]\n",
    "                tsmini_sub=tsmini_sub.reset_index(drop=True)    \n",
    "            else:\n",
    "                tsmini_sub=tsmini\n",
    "\n",
    "\n",
    "            # Extraction des quaternions\n",
    "\n",
    "            head_df= pd.read_csv(list(session_path.glob('*V4_Miniscope/headOrientation.csv'))[0])\n",
    "            qw = head_df['qw'].to_numpy()\n",
    "            qx = head_df['qx'].to_numpy()\n",
    "            qy = head_df['qy'].to_numpy()\n",
    "            qz = head_df['qz'].to_numpy()\n",
    "            roll, pitch, yaw = quaternion_to_euler(qw, qx, qy, qz)\n",
    "            roll_deg = np.degrees(roll) # nose goes clockwise or anticlockwise\n",
    "            pitch_deg = np.degrees(pitch) # nose up or down\n",
    "            yaw_deg = np.degrees(yaw) # nose moves from side to side                \n",
    "            \n",
    "\n",
    "            # DeepLabCut data\n",
    "\n",
    "            dlcfile=''\n",
    "            dlcpath=Path(f'{Path(session_path)}/My_First_WebCam/')\n",
    "            for file in os.listdir(dlcpath):\n",
    "                if file.endswith(('.h5')):\n",
    "                    dlcfile=file\n",
    "                    break\n",
    "            dlc_path = os.path.join(dlcpath, dlcfile)\n",
    "            df = pd.read_hdf(dlc_path)\n",
    "            directory = os.path.dirname(dlc_path)\n",
    "            timestamps_path = Path(directory,'timeStamps.csv')\n",
    "            if timestamps_path.exists():\n",
    "                timestamps = pd.read_csv(timestamps_path)\n",
    "                tswebcam = np.array(timestamps['Time Stamp (ms)'])\n",
    "                frame_rate = round(1/(np.mean(np.diff(timestamps.iloc[:,1]))/1000))  # fps\n",
    "            else:\n",
    "                frame_rate = 16  # fps /!\\ CHANGE ACCORDING TO YOUR DATA\n",
    "            \n",
    "            # Head \n",
    "\n",
    "            df_head= df.copy()\n",
    "            df_head.iloc[:, 0] = df_head.apply(lambda row: row.iloc[0] if row.iloc[2] > 0.5 else np.nan, axis=1)\n",
    "            df_head.iloc[:, 1] = df_head.apply(lambda row: row.iloc[1] if row.iloc[2] > 0.5 else np.nan, axis=1)\n",
    "            X = df_head.iloc[:, 0]\n",
    "            Y = df_head.iloc[:, 1]        \n",
    "            individual_xO= np.array(X.values) \n",
    "            individual_yO = np.array(Y.values)\n",
    "            for i, x in enumerate(individual_xO):# Define when the mouse is on the cheeseboard (start)\n",
    "                y = individual_yO[i]\n",
    "                if calculate_relative_distance(x, y, table_center_x, table_center_y) >= table_radius:\n",
    "                    individual_xO[i] = np.nan\n",
    "                    individual_yO[i] = np.nan\n",
    "            individual_xOO = remove_short_sequences(individual_xO, max_len=3)\n",
    "            individual_yOO = remove_short_sequences(individual_yO, max_len=3)\n",
    "            x_start = find_long_non_nan_sequences(individual_xOO)[0][0] # first value of the first long non nan sequence\n",
    "            y_start = find_long_non_nan_sequences(individual_yOO)[0][0] # first value of the first long non nan sequence\n",
    "            start_frame = np.where(individual_xOO == x_start)[0][0].item()\n",
    "            \n",
    "            individual_xOO[:start_frame]=np.nan # remove any path before the real start\n",
    "            individual_yOO[:start_frame]=np.nan # remove any path before the real start\n",
    "            individual_x1, individual_y1 = replace_high_speed_points_with_nan(individual_xOO, individual_yOO, speed_threshold=10)\n",
    "            last_frame = len(individual_x1)\n",
    "            individual_x2, individual_y2 = interpolate_2d_path(individual_x1[start_frame:last_frame], individual_y1[start_frame:last_frame], kind='nearest')\n",
    "            individual_x3, individual_y3 = limit_speed(individual_x2, individual_y2, max_speed=20)\n",
    "            individual_x = np.concatenate((individual_x1[:start_frame], individual_x3))\n",
    "            individual_y = np.concatenate((individual_y1[:start_frame], individual_y3))\n",
    "            \n",
    "            # Tail \n",
    "\n",
    "            df_tail= df.copy()\n",
    "            df_tail.iloc[:, 3] = df_tail.apply(lambda row: row.iloc[3] if row.iloc[5] > 0.5 else np.nan, axis=1)\n",
    "            df_tail.iloc[:, 4] = df_tail.apply(lambda row: row.iloc[4] if row.iloc[5] > 0.5 else np.nan, axis=1)\n",
    "            Xt = df_tail.iloc[:, 3]\n",
    "            Yt = df_tail.iloc[:, 4]        \n",
    "            tail_xO= np.array(Xt.values) \n",
    "            tail_yO = np.array(Yt.values)\n",
    "            for i, x in enumerate(tail_xO):# Define when the mouse is on the cheeseboard (start)\n",
    "                y = tail_yO[i]\n",
    "                if calculate_relative_distance(x, y, table_center_x, table_center_y) >= table_radius:\n",
    "                    tail_xO[i] = np.nan\n",
    "                    tail_yO[i] = np.nan\n",
    "            tail_xOO = remove_short_sequences(tail_xO, max_len=3)\n",
    "            tail_yOO = remove_short_sequences(tail_yO, max_len=3)\n",
    "            tail_xOO[:start_frame]=np.nan # remove any path before the real start\n",
    "            tail_yOO[:start_frame]=np.nan # remove any path before the real start\n",
    "            tail_x1, tail_y1 = replace_high_speed_points_with_nan(tail_xOO, tail_yOO, speed_threshold=10)\n",
    "            tail_x2, tail_y2 = interpolate_2d_path(tail_x1[start_frame:last_frame], tail_y1[start_frame:last_frame], kind='nearest')\n",
    "            tail_x3, tail_y3 = limit_speed(tail_x2, tail_y2, max_speed=20)\n",
    "            tail_x = np.concatenate((tail_x1[:start_frame], tail_x3))\n",
    "            tail_y = np.concatenate((tail_y1[:start_frame], tail_y3))\n",
    "                        \n",
    "\n",
    "            # Keep only crossregistered cells\n",
    "\n",
    "            C_sel=Co.drop_sel(unit_id=unit_to_drop)\n",
    "            indexMappList=mapping_sess[session]\n",
    "            kept_uniq_unit_List=[]\n",
    "            for unit in C_sel['unit_id'].values:\n",
    "                indexMapp = np.where(indexMappList == unit)[0]\n",
    "                kept_uniq_unit_List.append(str(indexMapp))\n",
    "            nb_unit=len(C_sel['unit_id'].values)\n",
    "            if nb_unit == 0:\n",
    "                print(f'... no cells kept in the session: {session}')\n",
    "                continue\n",
    "            print(f'... {nb_unit} kept cells in the session')\n",
    "\n",
    "\n",
    "            # If the miniscope recording started after the webcam recording, cut the webcam data\n",
    "            if tsmini_sub.iloc[0] > tswebcam[start_frame]:\n",
    "                Newstart_frame = np.where(tswebcam >= tsmini_sub.iloc[0].item())[0][1].item()\n",
    "                print(f'... webcam data cut to match miniscope length, new start at frame {Newstart_frame} (instead of {start_frame})')\n",
    "                start_frame = Newstart_frame \n",
    "            # If the miniscope recording is shorter than the webcam recording, cut the webcam data\n",
    "            if tsmini_sub.iloc[-1] < tswebcam[last_frame-1]:\n",
    "                Newlast_frame = np.where(tswebcam <= tsmini_sub.iloc[-1].item())[0][-1].item()\n",
    "                print(f'... webcam data cut to match miniscope length, new end at frame {Newlast_frame} (instead of {last_frame})')\n",
    "                last_frame = Newlast_frame\n",
    "            \n",
    "\n",
    "\n",
    "            # Align data\n",
    "\n",
    "            x = individual_x[start_frame:last_frame]\n",
    "            y = individual_y[start_frame:last_frame]\n",
    "            xt = tail_x[start_frame:last_frame]\n",
    "            yt = tail_y[start_frame:last_frame]\n",
    "            angles_deg = np.array([direction(x1, y1, x2, y2) for x1, y1, x2, y2 in zip(xt, yt, x, y)])\n",
    "\n",
    "            closest_start= find_closest_index_sorted(tsmini_sub, tswebcam[start_frame])\n",
    "            closest_end= find_closest_index_sorted(tsmini_sub, tswebcam[last_frame-1])\n",
    "\n",
    "            roll_deg_ = roll_deg[closest_start:closest_end]\n",
    "            pitch_deg_ = pitch_deg[closest_start:closest_end]\n",
    "            yaw_deg_ = yaw_deg[closest_start:closest_end]\n",
    "\n",
    "\n",
    "            # Keep only crossregistered cells\n",
    "\n",
    "            Carray = C_sel.to_numpy().T\n",
    "            Calcium = pd.DataFrame(Carray.T, index=C_sel['unit_id'].values.tolist())            \n",
    "\n",
    "            n = int(np.floor(2 * table_radius / square_size))\n",
    "            for unit in range(nb_unit): \n",
    "                indexMapp = np.where(mapping_sess[session] == Calcium.index[unit])[0]\n",
    "                if len(indexMapp)>0 : # The neuron needs to be in the cross-registration\n",
    "                    unique_unit = f'{mice}{str(indexMapp[0])}'\n",
    "                    sess_unit = f'{mice}{str(indexMapp[0])}_s{sess}_{unit}'\n",
    "                    Carray_unit = Carray[:,unit]\n",
    "                    nr_tot_act_biaised = defaultdict(int) # Neuron activity per square\n",
    "                    counts = defaultdict(int) # Count visits per square\n",
    "                    nr_tot_act_biaised_HD = np.zeros(len(bin_centers))\n",
    "                    counts_HD = np.zeros(len(bin_centers))\n",
    "\n",
    "                    for idx, (px, py, angle) in enumerate(zip(x, y, angles_deg)):\n",
    "                        if np.sqrt((px - table_center_x)**2 + (py - table_center_y)**2) > table_radius:\n",
    "                            continue  # skip points outside circle\n",
    "                        ix = int(np.floor((px - (table_center_x - n/2 * square_size)) / square_size))\n",
    "                        iy = int(np.floor((py - (table_center_y - n/2 * square_size)) / square_size))\n",
    "                        closest_point = find_closest_index_sorted(tsmini_sub, tswebcam[start_frame+idx])\n",
    "                        nr_tot_act_biaised[(ix, iy)] += Carray_unit[closest_point]\n",
    "                        counts[(ix, iy)] += 1/frame_rate \n",
    "\n",
    "                        bin_idx = int(angle // bin_size)\n",
    "                        nr_tot_act_biaised_HD[bin_idx] += Carray_unit[closest_point]\n",
    "                        counts_HD[bin_idx] += 1                        \n",
    "\n",
    "                    if unique_unit in AllCellDict:\n",
    "                        AllCellDict[unique_unit][session] = nr_tot_act_biaised \n",
    "                        AllTimeDict[unique_unit][session] = counts\n",
    "\n",
    "                        AllCellDict_HD[unique_unit][session] = nr_tot_act_biaised_HD \n",
    "                        AllTimeDict_HD[unique_unit][session] = counts_HD\n",
    "\n",
    "                    else :\n",
    "                        AllCellDict[unique_unit] = {}\n",
    "                        AllCellDict[unique_unit][session] = nr_tot_act_biaised \n",
    "                        AllTimeDict[unique_unit] = {}\n",
    "                        AllTimeDict[unique_unit][session] = counts\n",
    "                        \n",
    "                        AllCellDict_HD[unique_unit] = {}\n",
    "                        AllCellDict_HD[unique_unit][session] = nr_tot_act_biaised_HD \n",
    "                        AllTimeDict_HD[unique_unit] = {}\n",
    "                        AllTimeDict_HD[unique_unit][session] = counts_HD\n",
    "\n",
    "                    AllCellDictSess[sess_unit] = nr_tot_act_biaised \n",
    "                    AllTimeDictSess[sess_unit] = counts\n",
    "                    AllCellDictSess_HD[sess_unit] = nr_tot_act_biaised_HD \n",
    "                    AllTimeDictSess_HD[sess_unit] = counts_HD\n",
    "\n",
    "            sess+=1\n",
    "\n",
    "    print(f'{len(AllCellDict.keys())} unique cells found so far')  \n",
    "    \n",
    "#########################################################################################\n",
    "                # Average activity map across sessions for each cell #\n",
    "##########################################################################################\n",
    "\n",
    "SumActDict={}\n",
    "for uniquecell in AllCellDict.keys():\n",
    "    sums = {}\n",
    "    for subdict in AllCellDict[uniquecell].values():\n",
    "        for key, value in subdict.items():\n",
    "            sums[key] = sums.get(key, 0) + value\n",
    "    SumActDict[uniquecell] = {key: sums[key] for key in sums}\n",
    "\n",
    "SumTimeDict={}\n",
    "for uniquecell in AllTimeDict.keys():\n",
    "    sums = {}\n",
    "    for subdict in AllTimeDict[uniquecell].values():\n",
    "        for key, value in subdict.items():\n",
    "            sums[key] = sums.get(key, 0) + value\n",
    "    SumTimeDict[uniquecell] = {key: sums[key] for key in sums}\n",
    "\n",
    "\n",
    "\n",
    "SumActDict_HD={}\n",
    "for outer_key, inner_dict in AllCellDict_HD.items():\n",
    "    first_array = next(iter(inner_dict.values()))\n",
    "    sum_array = np.zeros_like(first_array)\n",
    "    for arr in inner_dict.values():\n",
    "        sum_array += arr\n",
    "    SumActDict_HD[outer_key] = sum_array\n",
    "\n",
    "\n",
    "SumTimeDict_HD={}\n",
    "for outer_key, inner_dict in AllTimeDict_HD.items():\n",
    "    first_array = next(iter(inner_dict.values()))\n",
    "    sum_array = np.zeros_like(first_array)\n",
    "    for arr in inner_dict.values():\n",
    "        sum_array += arr\n",
    "    SumTimeDict_HD[outer_key] = sum_array\n",
    "\n",
    "\n",
    "print(f'Total unique cell = {len(SumActDict.keys())}')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b4a39",
   "metadata": {},
   "source": [
    "# Place Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d2350",
   "metadata": {},
   "source": [
    "Plot place map of cell per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = sorted(AllCellDictSess.keys())\n",
    "AllCellDictSess = {key: AllCellDictSess[key]for rank, key in enumerate(sorted_keys)}\n",
    "\n",
    "nb_subplot=len(AllCellDictSess.keys())\n",
    "rows = int(np.ceil(np.sqrt(nb_subplot)))  # Rows: ceil(sqrt(X))\n",
    "cols = int(np.ceil(np.sqrt(nb_subplot)))  # Columns: floor(sqrt(X))\n",
    "\n",
    "# Create the figure with a 2x2 grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(int(nb_subplot/10), int(nb_subplot/10)))\n",
    "axs = axs.flatten()\n",
    "plt.tight_layout()\n",
    "\n",
    "sigma = 1  # Standard deviation for Gaussian kernel\n",
    "max_row = int(table_radius*2/square_size)+1\n",
    "max_col = int(table_radius*2/square_size)+1\n",
    "\n",
    "Smoothed_ActSess={}\n",
    "Smoothed_TimeSess={}\n",
    "Spatial_infoSess = {}\n",
    "\n",
    "for nsubplot, nr in enumerate(AllCellDictSess.keys()):\n",
    "\n",
    "    nr_tot_act = AllCellDictSess[nr]    \n",
    "    nr_tot_act_array = [[None for _ in range(max_col)] for _ in range(max_row)] # Initialize 2D array\n",
    "    for (row, col), value in nr_tot_act.items(): # Fill array\n",
    "        nr_tot_act_array[row][col] = value    \n",
    "    nr_tot_act_array = [[np.nan if v is None else v for v in row] for row in nr_tot_act_array] # Replace None with np.nan\n",
    "    nr_tot_act_array = np.array(nr_tot_act_array, dtype=float)\n",
    "    nr_tot_act_array=nr_tot_act_array.T\n",
    "\n",
    "    array_for_filter = np.nan_to_num(nr_tot_act_array, nan=0.0) # Replace nan with 0 for Gaussian filtering\n",
    "    act_smoothed_array = gaussian_filter(array_for_filter, sigma=sigma)    # Apply 2D Gaussian filter\n",
    "\n",
    "\n",
    "    nr_tot_time = AllTimeDictSess[nr]\n",
    "    nr_tot_time_array = [[None for _ in range(max_col)] for _ in range(max_row)] # Initialize 2D array\n",
    "    for (row, col), value in nr_tot_time.items():# Fill array\n",
    "        nr_tot_time_array[row][col] = value\n",
    "    nr_tot_time_array = [[np.nan if v is None else v for v in row] for row in nr_tot_time_array] # Replace None with np.nan\n",
    "    nr_tot_time_array = np.array(nr_tot_time_array, dtype=float)\n",
    "    nr_tot_time_array=nr_tot_time_array.T\n",
    "\n",
    "    array_for_filter = np.nan_to_num(nr_tot_time_array, nan=0.0) # Replace nan with 0 for Gaussian filtering\n",
    "    time_smoothed_array = gaussian_filter(array_for_filter, sigma=sigma)    # Apply 2D Gaussian filter\n",
    "    time_smoothed_array[time_smoothed_array < .5] = np.nan  # do not consider if spent less than 0.1 second in the square\n",
    "\n",
    "    # Ratio\n",
    "    act_smoothed_array = np.divide(act_smoothed_array, time_smoothed_array)\n",
    "\n",
    "    # --- Mask outside circle ---\n",
    "    rows, cols = act_smoothed_array.shape\n",
    "    center_row, center_col = rows // 2, cols // 2\n",
    "    radius = min(rows, cols) // 2   \n",
    "    Y, X = np.ogrid[:rows, :cols]# Create a circular mask\n",
    "    dist_from_center = np.sqrt((X - center_col)**2 + (Y - center_row)**2)\n",
    "    mask = dist_from_center < radius    \n",
    "    act_smoothed_array_mask = np.where(mask, act_smoothed_array, np.nan)# Apply mask: set values outside circle to NaN\n",
    "    Smoothed_ActSess[nr]= act_smoothed_array_mask\n",
    "\n",
    "    time_smoothed_array_mask = np.where(mask, time_smoothed_array, np.nan)# Apply mask: set values outside circle to NaN\n",
    "    Smoothed_TimeSess[nr]= time_smoothed_array_mask\n",
    "\n",
    "\n",
    "    # --- Spatial information ---\n",
    "    mean_act=np.nanmean(Smoothed_ActSess[nr])\n",
    "    sum_time=np.nansum(Smoothed_TimeSess[nr])\n",
    "    Spatial_infoSess[nr] = 0\n",
    "    for rows in np.arange(np.shape(Smoothed_ActSess[nr])[0]) : \n",
    "        for cols in np.arange(np.shape(Smoothed_ActSess[nr])[1]) :             \n",
    "            bin_act=Smoothed_ActSess[nr][rows,cols]\n",
    "            p_bin=Smoothed_TimeSess[nr][rows,cols]/sum_time \n",
    "            if ~ np.isnan(bin_act): \n",
    "                if bin_act!=0:             \n",
    "                    spatial_info_bin=p_bin*(bin_act/mean_act)*math.log2(bin_act/mean_act)\n",
    "                else:\n",
    "                    spatial_info_bin=0    \n",
    "                Spatial_infoSess[nr] += spatial_info_bin\n",
    "\n",
    "\n",
    "    # Mask outside circle \n",
    "    rows, cols = np.full((max_row, max_col),1).shape\n",
    "    center_row, center_col = rows // 2, cols // 2\n",
    "    radius = min(rows, cols) // 2   \n",
    "    Y, X = np.ogrid[:rows, :cols]# Create a circular mask\n",
    "    dist_from_center = np.sqrt((X - center_col)**2 + (Y - center_row)**2)\n",
    "    mask = dist_from_center < radius    \n",
    "    masked_array = np.where(mask, np.full((max_row, max_col),1), np.nan)# Apply mask: set values outside circle to NaN\n",
    "    \n",
    "    # Draw map\n",
    "    axs[nsubplot].imshow(masked_array, cmap='Greys_r', origin='upper')\n",
    "    axs[nsubplot].imshow(act_smoothed_array_mask, cmap='jet', origin='upper')\n",
    "    axs[nsubplot].set_aspect('equal')\n",
    "    axs[nsubplot].set_title(f'{nr} ({np.round(Spatial_infoSess[nr], 1)})', pad=0, loc='left', fontsize=15)\n",
    "    axs[nsubplot].set_title(f\"{nr.split('_')[0]}_{nr.split('_')[1]}\", pad=0, loc='left', fontsize=max(7,int(nb_subplot/20)))\n",
    "\n",
    "        \n",
    "    # Remove box (spines)\n",
    "    for spine in axs[nsubplot].spines.values():\n",
    "        spine.set_visible(False)\n",
    "    axs[nsubplot].set_xticks([])  # No x-axis ticks\n",
    "    axs[nsubplot].set_yticks([])  # No y-axis ticks\n",
    "\n",
    "# Adjust layout to avoid clipping\n",
    "fig.subplots_adjust(wspace=.1, hspace=.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3318f3d",
   "metadata": {},
   "source": [
    "Plot place map of all unique cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad4d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = sorted(SumActDict.keys())\n",
    "SumActDict = {key: SumActDict[key]for rank, key in enumerate(sorted_keys)}\n",
    "\n",
    "nb_subplot=len(SumActDict.keys())\n",
    "rows = int(np.ceil(np.sqrt(nb_subplot)))  # Rows: ceil(sqrt(X))\n",
    "cols = int(np.ceil(np.sqrt(nb_subplot)))  # Columns: floor(sqrt(X))\n",
    "\n",
    "# Create the figure with a 2x2 grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(int(nb_subplot/10), int(nb_subplot/10)))\n",
    "axs = axs.flatten()\n",
    "plt.tight_layout()\n",
    "\n",
    "sigma = 1  # Standard deviation for Gaussian kernel\n",
    "max_row = int(table_radius*2/square_size)+1\n",
    "max_col = int(table_radius*2/square_size)+1\n",
    "\n",
    "Smoothed_Act={}\n",
    "Smoothed_Time={}\n",
    "Spatial_info = {}\n",
    "\n",
    "for nsubplot, nr in enumerate(SumActDict.keys()):\n",
    "\n",
    "    nr_tot_act = SumActDict[nr]    \n",
    "    nr_tot_act_array = [[None for _ in range(max_col)] for _ in range(max_row)] # Initialize 2D array\n",
    "    for (row, col), value in nr_tot_act.items():# Fill array\n",
    "        nr_tot_act_array[row][col] = value    \n",
    "    nr_tot_act_array = [[np.nan if v is None else v for v in row] for row in nr_tot_act_array] # Replace None with np.nan\n",
    "    nr_tot_act_array = np.array(nr_tot_act_array, dtype=float)\n",
    "    nr_tot_act_array=nr_tot_act_array.T\n",
    "\n",
    "    array_for_filter = np.nan_to_num(nr_tot_act_array, nan=0.0) # Replace nan with 0 for Gaussian filtering\n",
    "    act_smoothed_array = gaussian_filter(array_for_filter, sigma=sigma)    # Apply 2D Gaussian filter\n",
    "\n",
    "\n",
    "    nr_tot_time = SumTimeDict[nr]\n",
    "    nr_tot_time_array = [[None for _ in range(max_col)] for _ in range(max_row)] # Initialize 2D array\n",
    "    for (row, col), value in nr_tot_time.items():# Fill array\n",
    "        nr_tot_time_array[row][col] = value\n",
    "    nr_tot_time_array = [[np.nan if v is None else v for v in row] for row in nr_tot_time_array] # Replace None with np.nan\n",
    "    nr_tot_time_array = np.array(nr_tot_time_array, dtype=float)\n",
    "    nr_tot_time_array=nr_tot_time_array.T\n",
    "\n",
    "    array_for_filter = np.nan_to_num(nr_tot_time_array, nan=0.0) # Replace nan with 0 for Gaussian filtering\n",
    "    time_smoothed_array = gaussian_filter(array_for_filter, sigma=sigma)    # Apply 2D Gaussian filter\n",
    "    time_smoothed_array[time_smoothed_array < .5] = np.nan  # do not consider if spent less than 0.1 second in the square\n",
    "\n",
    "    # Ratio\n",
    "    act_smoothed_array = np.divide(act_smoothed_array, time_smoothed_array)\n",
    "\n",
    "    # --- Mask outside circle ---\n",
    "    rows, cols = act_smoothed_array.shape\n",
    "    center_row, center_col = rows // 2, cols // 2\n",
    "    radius = min(rows, cols) // 2   \n",
    "    Y, X = np.ogrid[:rows, :cols]# Create a circular mask\n",
    "    dist_from_center = np.sqrt((X - center_col)**2 + (Y - center_row)**2)\n",
    "    mask = dist_from_center < radius    \n",
    "    act_smoothed_array_mask = np.where(mask, act_smoothed_array, np.nan)# Apply mask: set values outside circle to NaN\n",
    "    Smoothed_Act[nr]= act_smoothed_array_mask\n",
    "\n",
    "    time_smoothed_array_mask = np.where(mask, time_smoothed_array, np.nan)# Apply mask: set values outside circle to NaN\n",
    "    Smoothed_Time[nr]= time_smoothed_array_mask\n",
    "\n",
    "\n",
    "    # --- Spatial information ---\n",
    "    mean_act=np.nanmean(Smoothed_Act[nr])\n",
    "    sum_time=np.nansum(Smoothed_Time[nr])\n",
    "    Spatial_info[nr] = 0\n",
    "    for rows in np.arange(np.shape(Smoothed_Act[nr])[0]) : \n",
    "        for cols in np.arange(np.shape(Smoothed_Act[nr])[1]) :             \n",
    "            bin_act=Smoothed_Act[nr][rows,cols]\n",
    "            p_bin=Smoothed_Time[nr][rows,cols]/sum_time \n",
    "            if ~ np.isnan(bin_act): \n",
    "                if bin_act!=0:             \n",
    "                    spatial_info_bin=p_bin*(bin_act/mean_act)*math.log2(bin_act/mean_act)\n",
    "                else:\n",
    "                    spatial_info_bin=0    \n",
    "                Spatial_info[nr] += spatial_info_bin\n",
    "\n",
    "\n",
    "    # Mask outside circle \n",
    "    rows, cols = np.full((max_row, max_col),1).shape\n",
    "    center_row, center_col = rows // 2, cols // 2\n",
    "    radius = min(rows, cols) // 2   \n",
    "    Y, X = np.ogrid[:rows, :cols]# Create a circular mask\n",
    "    dist_from_center = np.sqrt((X - center_col)**2 + (Y - center_row)**2)\n",
    "    mask = dist_from_center < radius    \n",
    "    masked_array = np.where(mask, np.full((max_row, max_col),1), np.nan)# Apply mask: set values outside circle to NaN\n",
    "    \n",
    "    # Draw map\n",
    "    axs[nsubplot].imshow(masked_array, cmap='Greys_r', origin='upper')\n",
    "    axs[nsubplot].imshow(act_smoothed_array_mask, cmap='jet', origin='upper')\n",
    "    axs[nsubplot].set_aspect('equal')\n",
    "    axs[nsubplot].set_title(f'{nr} ({np.round(Spatial_info[nr], 1)})', pad=0, loc='left', fontsize=max(7,int(nb_subplot/10)))\n",
    "\n",
    "        \n",
    "    # Remove box (spines)\n",
    "    for spine in axs[nsubplot].spines.values():\n",
    "        spine.set_visible(False)\n",
    "    axs[nsubplot].set_xticks([])  # No x-axis ticks\n",
    "    axs[nsubplot].set_yticks([])  # No y-axis ticks\n",
    "\n",
    "# Adjust layout to avoid clipping\n",
    "fig.subplots_adjust(wspace=.1, hspace=.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5519b84e",
   "metadata": {},
   "source": [
    "# Head Direction Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b39ac1",
   "metadata": {},
   "source": [
    "Plot Head direction of cells per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976df61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = sorted(AllCellDictSess_HD.keys())\n",
    "AllCellDictSess_HD = {key: AllCellDictSess_HD[key]for rank, key in enumerate(sorted_keys)}\n",
    "\n",
    "nb_subplot=len(AllCellDictSess_HD.keys())\n",
    "rows = int(np.ceil(np.sqrt(nb_subplot)))  # Rows: ceil(sqrt(X))\n",
    "cols = int(np.ceil(np.sqrt(nb_subplot)))  # Columns: floor(sqrt(X))\n",
    "\n",
    "# Create the figure with a 2x2 grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(int(nb_subplot/10), int(nb_subplot/10)),  subplot_kw={'projection': 'polar'})\n",
    "axs = axs.flatten()\n",
    "plt.tight_layout()\n",
    "\n",
    "bin_size = 10\n",
    "bin_edges = np.arange(0, 361, bin_size)\n",
    "bin_centers = np.radians(bin_edges[:-1] + bin_size/2)\n",
    "\n",
    "HeadDirection_infoSess={}\n",
    "for nsubplot, nr in enumerate(AllCellDictSess_HD.keys()):\n",
    "\n",
    "    ax = axs[nsubplot]\n",
    "    ax.set_theta_zero_location(\"E\")  # 0° at East\n",
    "    ax.set_theta_direction(-1)        # Clockwise\n",
    "    ax.set_title(f\"{nr.split('_')[0]}_{nr.split('_')[1]}\", pad=0, loc='left', fontsize=max(7,int(nb_subplot/10)))\n",
    "    HeadDirection_infoSess[nr] = np.divide(AllCellDictSess_HD[nr], AllTimeDictSess_HD[nr])\n",
    "    \n",
    "    \n",
    "    #ax.bar(bin_centers, HeadDirection_infoSess[nr], width=np.radians(bin_size), alpha=0.6, color= 'k')   \n",
    "    \n",
    "    theta = bin_centers\n",
    "    theta = np.append(theta, theta[0])   \n",
    "    r = HeadDirection_infoSess[nr]\n",
    "    r_smooth = gaussian_filter1d(r, sigma=1, mode='wrap')  # wrap for circular smoothing\n",
    "    r_smooth = np.append(r_smooth, r_smooth[0])\n",
    "    pref_idx = np.argmax(r_smooth)    # Find preferred angle\n",
    "    pref_angle = theta[pref_idx]    \n",
    "    color = plt.cm.hsv(pref_angle / (2*np.pi))# pref_angle ranges 0 to 2*pi -> normalize to 0-1 for colormap\n",
    "    ax.plot(theta, r_smooth, color=color)\n",
    "    \n",
    "    #r = np.append(r, r[0])\n",
    "    #ax.plot(theta, r, linestyle='-', alpha=0.7, color= 'r')\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Adjust layout to avoid clipping\n",
    "fig.subplots_adjust(wspace=.1, hspace=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cec6d",
   "metadata": {},
   "source": [
    "Plot Head direction of all unique cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = sorted(SumActDict_HD.keys())\n",
    "SumActDict_HD = {key: SumActDict_HD[key]for rank, key in enumerate(sorted_keys)}\n",
    "\n",
    "nb_subplot=len(SumActDict_HD.keys())\n",
    "rows = int(np.ceil(np.sqrt(nb_subplot)))  # Rows: ceil(sqrt(X))\n",
    "cols = int(np.ceil(np.sqrt(nb_subplot)))  # Columns: floor(sqrt(X))\n",
    "\n",
    "# Create the figure with a 2x2 grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(int(nb_subplot/10), int(nb_subplot/10)),  subplot_kw={'projection': 'polar'})\n",
    "axs = axs.flatten()\n",
    "plt.tight_layout()\n",
    "\n",
    "bin_size = 10\n",
    "bin_edges = np.arange(0, 361, bin_size)\n",
    "bin_centers = np.radians(bin_edges[:-1] + bin_size/2)\n",
    "\n",
    "HeadDirection_info={}\n",
    "for nsubplot, nr in enumerate(SumActDict_HD.keys()):\n",
    "\n",
    "    ax = axs[nsubplot]\n",
    "    ax.set_theta_zero_location(\"E\")  # 0° at East\n",
    "    ax.set_theta_direction(-1)        # Clockwise\n",
    "    ax.set_title(f'{nr}', pad=0, loc='left', fontsize=max(7,int(nb_subplot/10)))\n",
    "    HeadDirection_info[nr] = np.divide(SumActDict_HD[nr], SumTimeDict_HD[nr])\n",
    "    \n",
    "    #ax.bar(bin_centers, HeadDirection_info[nr], width=np.radians(bin_size), alpha=0.6, color= 'k')   \n",
    "    \n",
    "    theta = bin_centers\n",
    "    theta = np.append(theta, theta[0])  \n",
    "    r = HeadDirection_info[nr]\n",
    "    r_smooth = gaussian_filter1d(r, sigma=1, mode='wrap')  # wrap for circular smoothing\n",
    "    r_smooth = np.append(r_smooth, r_smooth[0])\n",
    "    pref_idx = np.argmax(r_smooth)    # Find preferred angle\n",
    "    pref_angle = theta[pref_idx]    \n",
    "    color = plt.cm.hsv(pref_angle / (2*np.pi))# pref_angle ranges 0 to 2*pi -> normalize to 0-1 for colormap\n",
    "    ax.plot(theta, r_smooth, color=color)\n",
    "    \n",
    "    #r = np.append(r, r[0])\n",
    "    #ax.plot(theta, r, linestyle='-', alpha=0.7, color= 'r')\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.subplots_adjust(wspace=.1, hspace=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973405f",
   "metadata": {},
   "source": [
    "# Run permutation tests on circularly shifted calcium traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShuffledSpatial_info={}\n",
    "ShuffledHeadDirection_info={}\n",
    "for nr in SumActDict.keys():\n",
    "    ShuffledSpatial_info[nr]={}\n",
    "    ShuffledHeadDirection_info[nr]={}\n",
    "\n",
    "nb_tot_permutation=1000\n",
    "\n",
    "for permutation_nb in range(nb_tot_permutation):\n",
    "\n",
    "    AllCellDict_Sh={}\n",
    "    AllTimeDict_Sh={}\n",
    "    AllCellDictSess_Sh={}\n",
    "    AllTimeDictSess_Sh={}\n",
    "\n",
    "    AllCellDict_HD_Sh={}\n",
    "    AllTimeDict_HD_Sh={}\n",
    "    AllCellDictSess_HD_Sh={}\n",
    "    AllTimeDictSess_HD_Sh={}\n",
    "\n",
    "    for dpath in Path(dir).glob('**/mappingsAB.pkl'):\n",
    "\n",
    "        mappfile = open(dpath.parents[0]/ f'mappingsAB.pkl', 'rb')\n",
    "        mapping = pickle.load(mappfile)\n",
    "        mapping_sess = mapping['session']   \n",
    "        mice = dpath.parents[1].parts[-1]\n",
    "        NeuronType = dpath.parents[2].parts[-1]\n",
    "\n",
    "        nb_minian_total=0\n",
    "\n",
    "        minian_folders = [f for f in dpath.parents[0].rglob('minian') if f.is_dir()]\n",
    "\n",
    "        for sess, minianpath in enumerate(minian_folders): # for each minian folders found in this mouse \n",
    "\n",
    "            if any(p in all_expe_types for p in minianpath.parts): # have to be to the expe_types\n",
    "\n",
    "                session_type=minianpath.parents[2].name\n",
    "                session_date=minianpath.parents[3].name\n",
    "                session_time=minianpath.parents[1].name\n",
    "                session=session_time\n",
    "                session_path=minianpath.parents[1] \n",
    "\n",
    "                # Minian data\n",
    "                \n",
    "                minian_ds = open_minian(minianpath)\n",
    "                Co = minian_ds['C']  # calcium traces\n",
    "                tsmini= pd.read_csv(list(session_path.glob('*V4_Miniscope/timeStamps.csv'))[0])['Time Stamp (ms)']\n",
    "                minian_freq=round(1/np.mean(np.diff(np.array(tsmini)/1000)))\n",
    "                try: \n",
    "                    TodropFile = minianpath / f'TodropFileAB.json'\n",
    "                    with open(TodropFile, 'r') as f:\n",
    "                        unit_to_drop = json.load(f)\n",
    "                except:\n",
    "                    TodropFile = minianpath.parent / f'TodropFileAB.json'\n",
    "                    with open(TodropFile, 'r') as f:\n",
    "                        unit_to_drop = json.load(f)\n",
    "\n",
    "\n",
    "            # DeepLabCut data\n",
    "\n",
    "            dlcfile=''\n",
    "            dlcpath=Path(f'{Path(session_path)}/My_First_WebCam/')\n",
    "            for file in os.listdir(dlcpath):\n",
    "                if file.endswith(('.h5')):\n",
    "                    dlcfile=file\n",
    "                    break\n",
    "            dlc_path = os.path.join(dlcpath, dlcfile)\n",
    "            df = pd.read_hdf(dlc_path)\n",
    "            directory = os.path.dirname(dlc_path)\n",
    "            timestamps_path = Path(directory,'timeStamps.csv')\n",
    "            if timestamps_path.exists():\n",
    "                timestamps = pd.read_csv(timestamps_path)\n",
    "                tswebcam = np.array(timestamps['Time Stamp (ms)'])\n",
    "                frame_rate = round(1/(np.mean(np.diff(timestamps.iloc[:,1]))/1000))  # fps\n",
    "            else:\n",
    "                frame_rate = 16  # fps /!\\ CHANGE ACCORDING TO YOUR DATA\n",
    "            \n",
    "            \n",
    "            # Head\n",
    "            df_head= df.copy()\n",
    "            df_head.iloc[:, 0] = df_head.apply(lambda row: row.iloc[0] if row.iloc[2] > 0.5 else np.nan, axis=1)\n",
    "            df_head.iloc[:, 1] = df_head.apply(lambda row: row.iloc[1] if row.iloc[2] > 0.5 else np.nan, axis=1)\n",
    "            X = df_head.iloc[:, 0]\n",
    "            Y = df_head.iloc[:, 1]        \n",
    "            individual_xO= np.array(X.values) \n",
    "            individual_yO = np.array(Y.values)\n",
    "            for i, x in enumerate(individual_xO):# Define when the mouse is on the cheeseboard (start)\n",
    "                y = individual_yO[i]\n",
    "                if calculate_relative_distance(x, y, table_center_x, table_center_y) >= table_radius:\n",
    "                    individual_xO[i] = np.nan\n",
    "                    individual_yO[i] = np.nan\n",
    "            individual_xOO = remove_short_sequences(individual_xO, max_len=3)\n",
    "            individual_yOO = remove_short_sequences(individual_yO, max_len=3)\n",
    "            x_start = find_long_non_nan_sequences(individual_xOO)[0][0] # first value of the first long non nan sequence\n",
    "            y_start = find_long_non_nan_sequences(individual_yOO)[0][0] # first value of the first long non nan sequence\n",
    "            start_frame = np.where(individual_xOO == x_start)[0][0].item()\n",
    "            individual_xOO[:start_frame]=np.nan # remove any path before the real start\n",
    "            individual_yOO[:start_frame]=np.nan # remove any path before the real start\n",
    "            individual_x1, individual_y1 = replace_high_speed_points_with_nan(individual_xOO, individual_yOO, speed_threshold=10)\n",
    "            last_frame = len(individual_x1)\n",
    "            individual_x2, individual_y2 = interpolate_2d_path(individual_x1[start_frame:last_frame], individual_y1[start_frame:last_frame], kind='nearest')\n",
    "            individual_x3, individual_y3 = limit_speed(individual_x2, individual_y2, max_speed=20)\n",
    "            individual_x = np.concatenate((individual_x1[:start_frame], individual_x3))\n",
    "            individual_y = np.concatenate((individual_y1[:start_frame], individual_y3))\n",
    "\n",
    "\n",
    "            # Tail \n",
    "            df_tail= df.copy()\n",
    "            df_tail.iloc[:, 3] = df_tail.apply(lambda row: row.iloc[3] if row.iloc[5] > 0.5 else np.nan, axis=1)\n",
    "            df_tail.iloc[:, 4] = df_tail.apply(lambda row: row.iloc[4] if row.iloc[5] > 0.5 else np.nan, axis=1)\n",
    "            Xt = df_tail.iloc[:, 3]\n",
    "            Yt = df_tail.iloc[:, 4]        \n",
    "            tail_xO= np.array(Xt.values) \n",
    "            tail_yO = np.array(Yt.values)\n",
    "            for i, x in enumerate(tail_xO):# Define when the mouse is on the cheeseboard (start)\n",
    "                y = tail_yO[i]\n",
    "                if calculate_relative_distance(x, y, table_center_x, table_center_y) >= table_radius:\n",
    "                    tail_xO[i] = np.nan\n",
    "                    tail_yO[i] = np.nan\n",
    "            tail_xOO = remove_short_sequences(tail_xO, max_len=3)\n",
    "            tail_yOO = remove_short_sequences(tail_yO, max_len=3)\n",
    "            tail_xOO[:start_frame]=np.nan # remove any path before the real start\n",
    "            tail_yOO[:start_frame]=np.nan # remove any path before the real start\n",
    "            tail_x1, tail_y1 = replace_high_speed_points_with_nan(tail_xOO, tail_yOO, speed_threshold=10)\n",
    "            tail_x2, tail_y2 = interpolate_2d_path(tail_x1[start_frame:last_frame], tail_y1[start_frame:last_frame], kind='nearest')\n",
    "            tail_x3, tail_y3 = limit_speed(tail_x2, tail_y2, max_speed=20)\n",
    "            tail_x = np.concatenate((tail_x1[:start_frame], tail_x3))\n",
    "            tail_y = np.concatenate((tail_y1[:start_frame], tail_y3))\n",
    "            \n",
    "\n",
    "            # Keep only crossregistered cells\n",
    "            C_sel=Co.drop_sel(unit_id=unit_to_drop)\n",
    "            indexMappList=mapping_sess[session]\n",
    "            kept_uniq_unit_List=[]\n",
    "            for unit in C_sel['unit_id'].values:\n",
    "                indexMapp = np.where(indexMappList == unit)[0]\n",
    "                kept_uniq_unit_List.append(str(indexMapp))\n",
    "            nb_unit=len(C_sel['unit_id'].values)\n",
    "            if nb_unit == 0:\n",
    "                continue\n",
    "\n",
    "\n",
    "            # If the miniscope recording started after the webcam recording, cut the webcam data\n",
    "            if tsmini.iloc[0] > tswebcam[start_frame]:\n",
    "                Newstart_frame = np.where(tswebcam >= tsmini.iloc[0].item())[0][1].item()\n",
    "                start_frame = Newstart_frame \n",
    "            # If the miniscope recording is shorter than the webcam recording, cut the webcam data\n",
    "            if tsmini.iloc[-1] < tswebcam[last_frame-1]:\n",
    "                Newlast_frame = np.where(tswebcam <= tsmini.iloc[-1].item())[0][-1].item()\n",
    "                last_frame = Newlast_frame\n",
    "            \n",
    "\n",
    "            # Align data\n",
    "            x = individual_x[start_frame:last_frame]\n",
    "            y = individual_y[start_frame:last_frame]   \n",
    "            xt = tail_x[start_frame:last_frame]\n",
    "            yt = tail_y[start_frame:last_frame]\n",
    "            angles_deg = np.array([direction(x1, y1, x2, y2) for x1, y1, x2, y2 in zip(xt, yt, x, y)])\n",
    "                            \n",
    "            Carray = C_sel.to_numpy().T\n",
    "\n",
    "            # Randomly shift the calcium trace to break temporal correlation with behavior\n",
    "            offset = random.randint(20*minian_freq, len(Carray)) # Random shift between 20s and the length of the session\n",
    "            Carray = np.roll(Carray, shift=offset, axis=0) \n",
    "\n",
    "            Calcium = pd.DataFrame(Carray.T, index=C_sel['unit_id'].values.tolist())            \n",
    "\n",
    "\n",
    "            # Keep only crossregistered cells\n",
    "            n = int(np.floor(2 * table_radius / square_size))\n",
    "            for unit in range(nb_unit): \n",
    "                indexMapp = np.where(mapping_sess[session] == Calcium.index[unit])[0]\n",
    "                if len(indexMapp)>0 : # The neuron needs to be in the cross-registration\n",
    "                    unique_unit = f'{mice}{str(indexMapp[0])}'\n",
    "                    sess_unit = f'{mice}{str(indexMapp[0])}_s{sess}_{unit}'\n",
    "                    Carray_unit = Carray[:,unit]\n",
    "                    nr_tot_act_biaised = defaultdict(int) # Neuron activity per square\n",
    "                    counts = defaultdict(int) # Count visits per square\n",
    "                    \n",
    "                    nr_tot_act_biaised_HD = np.zeros(len(bin_centers))\n",
    "                    counts_HD = np.zeros(len(bin_centers))\n",
    "\n",
    "                    for idx, (px, py, angle) in enumerate(zip(x, y, angles_deg)):\n",
    "                        if np.sqrt((px - table_center_x)**2 + (py - table_center_y)**2) > table_radius:\n",
    "                            continue  # skip points outside circle\n",
    "                        ix = int(np.floor((px - (table_center_x - n/2 * square_size)) / square_size))\n",
    "                        iy = int(np.floor((py - (table_center_y - n/2 * square_size)) / square_size))\n",
    "                        closest_point = find_closest_index_sorted(tsmini, tswebcam[start_frame+idx])\n",
    "                        nr_tot_act_biaised[(ix, iy)] += Carray_unit[closest_point]\n",
    "                        counts[(ix, iy)] += 1/frame_rate \n",
    "\n",
    "                        bin_idx = int(angle // bin_size)\n",
    "                        nr_tot_act_biaised_HD[bin_idx] += Carray_unit[closest_point]\n",
    "                        counts_HD[bin_idx] += 1\n",
    "                        \n",
    "\n",
    "                    if unique_unit in AllCellDict_Sh:\n",
    "                        AllCellDict_Sh[unique_unit][session] = nr_tot_act_biaised \n",
    "                        AllTimeDict_Sh[unique_unit][session] = counts\n",
    "\n",
    "                        AllCellDict_HD_Sh[unique_unit][session] = nr_tot_act_biaised_HD \n",
    "                        AllTimeDict_HD_Sh[unique_unit][session] = counts_HD\n",
    "\n",
    "                    else :\n",
    "                        AllCellDict_Sh[unique_unit] = {}\n",
    "                        AllCellDict_Sh[unique_unit][session] = nr_tot_act_biaised \n",
    "                        AllTimeDict_Sh[unique_unit] = {}\n",
    "                        AllTimeDict_Sh[unique_unit][session] = counts\n",
    "                        \n",
    "                        AllCellDict_HD_Sh[unique_unit] = {}\n",
    "                        AllCellDict_HD_Sh[unique_unit][session] = nr_tot_act_biaised_HD \n",
    "                        AllTimeDict_HD_Sh[unique_unit] = {}\n",
    "                        AllTimeDict_HD_Sh[unique_unit][session] = counts_HD\n",
    "\n",
    "                    AllCellDictSess_Sh[sess_unit] = nr_tot_act_biaised \n",
    "                    AllTimeDictSess_Sh[sess_unit] = counts\n",
    "                    AllCellDictSess_HD_Sh[sess_unit] = nr_tot_act_biaised_HD \n",
    "                    AllTimeDictSess_HD_Sh[sess_unit] = counts_HD\n",
    "\n",
    "            nb_minian_total+=1\n",
    "\n",
    "    #########################################################################################\n",
    "                    # Average activity map across sessions for each cell #\n",
    "    ##########################################################################################\n",
    "\n",
    "    SumActDict_Sh={}\n",
    "    for uniquecell in AllCellDict_Sh.keys():\n",
    "        sums = {}\n",
    "        for subdict in AllCellDict_Sh[uniquecell].values():\n",
    "            for key, value in subdict.items():\n",
    "                sums[key] = sums.get(key, 0) + value\n",
    "        SumActDict_Sh[uniquecell] = {key: sums[key] for key in sums}\n",
    "\n",
    "    SumTimeDict_Sh={}\n",
    "    for uniquecell in AllTimeDict_Sh.keys():\n",
    "        sums = {}\n",
    "        for subdict in AllTimeDict_Sh[uniquecell].values():\n",
    "            for key, value in subdict.items():\n",
    "                sums[key] = sums.get(key, 0) + value\n",
    "        SumTimeDict_Sh[uniquecell] = {key: sums[key] for key in sums}\n",
    "\n",
    "    \n",
    "    SumActDict_HD_Sh={}\n",
    "    for outer_key, inner_dict in AllCellDict_HD_Sh.items():\n",
    "        first_array = next(iter(inner_dict.values()))\n",
    "        sum_array = np.zeros_like(first_array)\n",
    "        for arr in inner_dict.values():\n",
    "            sum_array += arr\n",
    "        SumActDict_HD_Sh[outer_key] = sum_array\n",
    "\n",
    "\n",
    "    SumTimeDict_HD_Sh={}\n",
    "    for outer_key, inner_dict in AllTimeDict_HD_Sh.items():\n",
    "        first_array = next(iter(inner_dict.values()))\n",
    "        sum_array = np.zeros_like(first_array)\n",
    "        for arr in inner_dict.values():\n",
    "            sum_array += arr\n",
    "        SumTimeDict_HD_Sh[outer_key] = sum_array\n",
    "\n",
    "\n",
    "    sorted_keys = sorted(SumActDict_Sh.keys())\n",
    "    SumActDict_Sh = {key: SumActDict_Sh[key]for rank, key in enumerate(sorted_keys)}\n",
    "\n",
    "    sigma = 1  # Standard deviation for Gaussian kernel\n",
    "    max_row = int(table_radius*2/square_size)+1\n",
    "    max_col = int(table_radius*2/square_size)+1\n",
    "\n",
    "    Smoothed_Act_Sh={}\n",
    "    Smoothed_Time_Sh={}\n",
    "    \n",
    "    for nsubplot, nr in enumerate(SumActDict_Sh.keys()):\n",
    "\n",
    "        nr_tot_act = SumActDict_Sh[nr]    \n",
    "        nr_tot_act_array = [[None for _ in range(max_col)] for _ in range(max_row)] # Initialize 2D array\n",
    "        for (row, col), value in nr_tot_act.items():# Fill array\n",
    "            nr_tot_act_array[row][col] = value    \n",
    "        nr_tot_act_array = [[np.nan if v is None else v for v in row] for row in nr_tot_act_array] # Replace None with np.nan\n",
    "        nr_tot_act_array = np.array(nr_tot_act_array, dtype=float)\n",
    "        nr_tot_act_array=nr_tot_act_array.T\n",
    "\n",
    "        array_for_filter = np.nan_to_num(nr_tot_act_array, nan=0.0) # Replace nan with 0 for Gaussian filtering\n",
    "        act_smoothed_array = gaussian_filter(array_for_filter, sigma=sigma)    # Apply 2D Gaussian filter\n",
    "\n",
    "\n",
    "        nr_tot_time = SumTimeDict_Sh[nr]\n",
    "        nr_tot_time_array = [[None for _ in range(max_col)] for _ in range(max_row)] # Initialize 2D array\n",
    "        for (row, col), value in nr_tot_time.items():# Fill array\n",
    "            nr_tot_time_array[row][col] = value\n",
    "        nr_tot_time_array = [[np.nan if v is None else v for v in row] for row in nr_tot_time_array] # Replace None with np.nan\n",
    "        nr_tot_time_array = np.array(nr_tot_time_array, dtype=float)\n",
    "        nr_tot_time_array=nr_tot_time_array.T\n",
    "\n",
    "        array_for_filter = np.nan_to_num(nr_tot_time_array, nan=0.0) # Replace nan with 0 for Gaussian filtering\n",
    "        time_smoothed_array = gaussian_filter(array_for_filter, sigma=sigma)    # Apply 2D Gaussian filter\n",
    "        time_smoothed_array[time_smoothed_array < .5] = np.nan  # do not consider if spent less than 0.1 second in the square\n",
    "\n",
    "        # Ratio\n",
    "        act_smoothed_array = np.divide(act_smoothed_array, time_smoothed_array)\n",
    "\n",
    "        # --- Mask outside circle ---\n",
    "        rows, cols = act_smoothed_array.shape\n",
    "        center_row, center_col = rows // 2, cols // 2\n",
    "        radius = min(rows, cols) // 2   \n",
    "        Y, X = np.ogrid[:rows, :cols]# Create a circular mask\n",
    "        dist_from_center = np.sqrt((X - center_col)**2 + (Y - center_row)**2)\n",
    "        mask = dist_from_center < radius    \n",
    "        act_smoothed_array_mask = np.where(mask, act_smoothed_array, np.nan)# Apply mask: set values outside circle to NaN\n",
    "        Smoothed_Act_Sh[nr]= act_smoothed_array_mask\n",
    "\n",
    "        time_smoothed_array_mask = np.where(mask, time_smoothed_array, np.nan)# Apply mask: set values outside circle to NaN\n",
    "        Smoothed_Time_Sh[nr]= time_smoothed_array_mask\n",
    "\n",
    "\n",
    "        # --- Spatial information ---\n",
    "        mean_act=np.nanmean(Smoothed_Act_Sh[nr])\n",
    "        sum_time=np.nansum(Smoothed_Time_Sh[nr])\n",
    "\n",
    "        ShuffledSpatial_info[nr][permutation_nb] = 0\n",
    "\n",
    "        for rows in np.arange(np.shape(Smoothed_Act_Sh[nr])[0]) : \n",
    "            for cols in np.arange(np.shape(Smoothed_Act_Sh[nr])[1]) :             \n",
    "                bin_act=Smoothed_Act_Sh[nr][rows,cols]\n",
    "                p_bin=Smoothed_Time_Sh[nr][rows,cols]/sum_time \n",
    "                if ~ np.isnan(bin_act): \n",
    "                    if bin_act!=0:             \n",
    "                        spatial_info_bin=p_bin*(bin_act/mean_act)*math.log2(bin_act/mean_act)\n",
    "                    else:\n",
    "                        spatial_info_bin=0 \n",
    "\n",
    "                    ShuffledSpatial_info[nr][permutation_nb] += spatial_info_bin\n",
    "        \n",
    "        ShuffledHeadDirection_info[nr][permutation_nb] = np.divide(SumActDict_HD_Sh[nr], SumTimeDict_HD_Sh[nr])    \n",
    "\n",
    "    print(f'Permutation n° {permutation_nb} done')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac732865",
   "metadata": {},
   "source": [
    "Results of permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51793c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = sorted(ShuffledSpatial_info.keys())\n",
    "ShuffledSpatial_info = {key: ShuffledSpatial_info[key] for rank, key in enumerate(sorted_keys)}\n",
    "nb_tot_permutation = 1000\n",
    "counts_PC = 0\n",
    "counts_HD = 0\n",
    "counts_PCHD = 0\n",
    "for nr in ShuffledSpatial_info.keys():\n",
    "    count_higher=0\n",
    "    for permutation_nb in range(nb_tot_permutation):\n",
    "        if ShuffledSpatial_info[nr][permutation_nb]>=Spatial_info[nr]:\n",
    "            count_higher+=1\n",
    "    p_value=(count_higher+1)/(nb_tot_permutation+1) # +1 for the observed value, +1 for the total number of shuffling\n",
    "    \n",
    "    \n",
    "    observed_psth = HeadDirection_info[nr]\n",
    "    shuffled_psths = arr = list(ShuffledHeadDirection_info[nr].values())\n",
    "    alpha = 0.01\n",
    "    lower_bound = np.percentile(shuffled_psths, alpha / 2 * 100, axis=0)\n",
    "    upper_bound = np.percentile(shuffled_psths, (1 - alpha / 2) * 100, axis=0)\n",
    "    significant_bins = (observed_psth < lower_bound) | (observed_psth > upper_bound)\n",
    "\n",
    "    if significant_bins.any() and p_value>0.05 :\n",
    "        print(f'{nr} is a head direction cell !')\n",
    "        counts_HD+=1\n",
    "    elif p_value<0.05 and not significant_bins.any() :\n",
    "        print(f'{nr} is a place cell !')\n",
    "        counts_PC+=1\n",
    "    elif p_value<0.05 and significant_bins.any():\n",
    "        print(f'{nr} is a place cell and a head direction cell !')\n",
    "        counts_PC+=1\n",
    "        counts_HD+=1\n",
    "        counts_PCHD+=1\n",
    "\n",
    "print(f'{counts_PC}/{len(Spatial_info.keys())} are place cells')\n",
    "print(f'{counts_HD}/{len(HeadDirection_info.keys())} are head direction cells')\n",
    "print(f'{counts_PCHD}/{len(HeadDirection_info.keys())} are both place cells and head direction cells')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_subplot=len(HeadDirection_info.keys())\n",
    "rows = int(np.ceil(np.sqrt(nb_subplot)))  # Rows: ceil(sqrt(X))\n",
    "cols = int(np.ceil(np.sqrt(nb_subplot)))  # Columns: floor(sqrt(X))\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(8, 8))\n",
    "axs = axs.flatten()\n",
    "plt.tight_layout()\n",
    "\n",
    "for nsubplot, nr in enumerate(HeadDirection_info.keys()):\n",
    "\n",
    "    # Compute the observed PSTH\n",
    "    observed_psth = HeadDirection_info[nr]\n",
    "    shuffled_psths = arr = list(ShuffledHeadDirection_info[nr].values())\n",
    "\n",
    "    # Compute significance (e.g., p < 0.05)\n",
    "    alpha = 0.05\n",
    "    lower_bound = np.percentile(shuffled_psths, alpha / 2 * 100, axis=0)\n",
    "    upper_bound = np.percentile(shuffled_psths, (1 - alpha / 2) * 100, axis=0)\n",
    "    significant_bins05 = (observed_psth < lower_bound) | (observed_psth > upper_bound)\n",
    "\n",
    "    alpha = 0.01\n",
    "    lower_bound = np.percentile(shuffled_psths, alpha / 2 * 100, axis=0)\n",
    "    upper_bound = np.percentile(shuffled_psths, (1 - alpha / 2) * 100, axis=0)\n",
    "    significant_bins01 = (observed_psth < lower_bound) | (observed_psth > upper_bound)\n",
    "\n",
    "    alpha = 0.001\n",
    "    lower_bound = np.percentile(shuffled_psths, alpha / 2 * 100, axis=0)\n",
    "    upper_bound = np.percentile(shuffled_psths, (1 - alpha / 2) * 100, axis=0)\n",
    "    significant_bins001 = (observed_psth < lower_bound) | (observed_psth > upper_bound)\n",
    "\n",
    "    ax = axs[nsubplot]\n",
    "    time_bins = np.linspace(0, 360, int(360//bin_size))\n",
    "    ax.bar(time_bins, observed_psth, color='blue', width=bin_size, alpha=0.5)\n",
    "    ax.bar(time_bins, shuffled_psths[0], color='gray', width=bin_size, alpha=0.5)\n",
    "\n",
    "    ax.scatter(\n",
    "        time_bins[significant_bins05],\n",
    "        observed_psth[significant_bins05],\n",
    "        color='green',\n",
    "        label='p<0.05',\n",
    "    )\n",
    "    ax.scatter(\n",
    "        time_bins[significant_bins01],\n",
    "        observed_psth[significant_bins01],\n",
    "        color='blue',\n",
    "        label='p<0.01',\n",
    "    )\n",
    "    ax.scatter(\n",
    "        time_bins[significant_bins001],\n",
    "        observed_psth[significant_bins001],\n",
    "        color='red',\n",
    "        label='p<0.001',\n",
    "    )\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f'{nr}', pad=0, loc='left', fontsize=7)\n",
    "\n",
    "fig.subplots_adjust(wspace=.1, hspace=.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79621bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_subplot=len(HeadDirection_info.keys())\n",
    "rows = int(np.ceil(np.sqrt(nb_subplot)))  # Rows: ceil(sqrt(X))\n",
    "cols = int(np.ceil(np.sqrt(nb_subplot)))  # Columns: floor(sqrt(X))\n",
    "\n",
    "# Create the figure with a 2x2 grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(8, 8),  subplot_kw={'projection': 'polar'})\n",
    "axs = axs.flatten()\n",
    "plt.tight_layout()\n",
    "\n",
    "bin_size = 10\n",
    "bin_edges = np.arange(0, 361, bin_size)\n",
    "bin_centers = np.radians(bin_edges[:-1] + bin_size/2)\n",
    "\n",
    "for nsubplot, nr in enumerate(HeadDirection_info.keys()):\n",
    "\n",
    "    ax = axs[nsubplot]\n",
    "    ax.set_theta_zero_location(\"E\")  # 0° at East\n",
    "    ax.set_theta_direction(-1)        # Clockwise\n",
    "    ax.set_title(f'{nr}', pad=0, loc='left', fontsize=7)\n",
    "    \n",
    "    theta = bin_centers\n",
    "    theta = np.append(theta, theta[0])  \n",
    "    r_smooth = gaussian_filter1d(HeadDirection_info[nr], sigma=1, mode='wrap')  # wrap for circular smoothing\n",
    "    r_smooth = np.append(r_smooth, r_smooth[0])\n",
    "    pref_idx = np.argmax(r_smooth)    # Find preferred angle\n",
    "    pref_angle = theta[pref_idx]    \n",
    "    color = plt.cm.hsv(pref_angle / (2*np.pi))# pref_angle ranges 0 to 2*pi -> normalize to 0-1 for colormap\n",
    "    ax.plot(theta, r_smooth, color=color)\n",
    "\n",
    "    r_shuffled = np.mean(list(ShuffledHeadDirection_info[nr].values()),0)\n",
    "    r_smooth = gaussian_filter1d(r_shuffled, sigma=1, mode='wrap')  # wrap for circular smoothing\n",
    "    r_smooth = np.append(r_smooth, r_smooth[0])\n",
    "    ax.plot(theta, r_smooth, color='grey')\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.subplots_adjust(wspace=.1, hspace=.1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f084e",
   "metadata": {},
   "source": [
    "Save Spatial & Head Direction info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dpath.parent, f\"ShuffledSpatialInfo_{mice}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(ShuffledSpatial_info, f)\n",
    "with open(os.path.join(dpath.parent, f\"SpatialInfo_{mice}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(Spatial_info, f)\n",
    "\n",
    "with open(os.path.join(dpath.parent, f\"ShuffledHeadDirectionInfo_{mice}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(ShuffledHeadDirection_info, f)\n",
    "with open(os.path.join(dpath.parent, f\"HeadDirectionInfo_{mice}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(HeadDirection_info, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e25b47",
   "metadata": {},
   "source": [
    "Load Spatial & Head Direction info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath= '//10.69.168.1/crnldata/forgetting/Aurelie/MiniscopeOE_data/L2_3_mice/RL/PlaceCells_experiment/'\n",
    "with open(f'{dpath}/ShuffledSpatialInfo_RL.pkl', \"rb\") as f:\n",
    "    ShuffledSpatial_info = pickle.load(f)\n",
    "with open(f'{dpath}/SpatialInfo_RL.pkl', \"rb\") as f:\n",
    "    Spatial_info = pickle.load(f)\n",
    "with open(f'{dpath}/ShuffledHeadDirectionInfo_RL.pkl', \"rb\") as f:\n",
    "    ShuffledHeadDirection_info = pickle.load(f)\n",
    "with open(f'{dpath}/HeadDirectionInfo_RL.pkl', \"rb\") as f:\n",
    "    HeadDirection_info = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
